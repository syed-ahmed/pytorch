// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#define __STDC_FORMAT_MACROS

#include <ATen/CUDAType.h>

// @generated by aten/src/ATen/gen.py

#include <THC/THC.h>
#include <THC/THCTensor.hpp>
#include <THCUNN/THCUNN.h>
#undef THNN_
#undef THCIndexTensor_
#include <c10/core/TensorImpl.h>
#include <ATen/CUDAGenerator.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NativeFunctions.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/Half.h>
#include <c10/core/TensorImpl.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>
#include <ATen/DeviceGuard.h>
#include <ATen/cuda/ATenCUDAGeneral.h>
#include <ATen/cuda/CUDADevice.h>
#include <ATen/cuda/CUDATypeDefault.h>

namespace at {

CUDAType::CUDAType()
  : CUDATypeDefault(CUDATensorId(), /*is_variable=*/false, /*is_undefined=*/false) {}

Backend CUDAType::backend() const {
  return Backend::CUDA;
}

const char * CUDAType::toString() const {
  return "CUDAType";
}

TypeID CUDAType::ID() const {
  return TypeID::CUDA;
}

/* example
Tensor * CUDAType::add(Tensor & a, Tensor & b) {
  std::cout << "add Tensor with backend CUDA\n";
  return &a;
}
*/

Tensor & CUDAType::_th_set_(Tensor & self, Storage source) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_storage(source,"source",2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Bool));
            THCudaBoolTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_storage(source,"source",2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Byte));
            THCudaByteTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto source_ = checked_storage(source,"source",2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Char));
            THCudaCharTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto source_ = checked_storage(source,"source",2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Double));
            THCudaDoubleTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto source_ = checked_storage(source,"source",2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Float));
            THCudaTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto source_ = checked_storage(source,"source",2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Int));
            THCudaIntTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_storage(source,"source",2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Long));
            THCudaLongTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto source_ = checked_storage(source,"source",2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Short));
            THCudaShortTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto source_ = checked_storage(source,"source",2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Half));
            THCudaHalfTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_set_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_set_(Tensor & self, Storage source, int64_t storage_offset, IntArrayRef size, IntArrayRef stride) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_storage(source,"source",2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Bool));
            THCudaBoolTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            self_->maybe_zero_dim(size.size() == 0);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_storage(source,"source",2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Byte));
            THCudaByteTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            self_->maybe_zero_dim(size.size() == 0);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto source_ = checked_storage(source,"source",2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Char));
            THCudaCharTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            self_->maybe_zero_dim(size.size() == 0);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto source_ = checked_storage(source,"source",2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Double));
            THCudaDoubleTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            self_->maybe_zero_dim(size.size() == 0);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto source_ = checked_storage(source,"source",2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Float));
            THCudaTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            self_->maybe_zero_dim(size.size() == 0);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto source_ = checked_storage(source,"source",2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Int));
            THCudaIntTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            self_->maybe_zero_dim(size.size() == 0);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_storage(source,"source",2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Long));
            THCudaLongTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            self_->maybe_zero_dim(size.size() == 0);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto source_ = checked_storage(source,"source",2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Short));
            THCudaShortTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            self_->maybe_zero_dim(size.size() == 0);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto source_ = checked_storage(source,"source",2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Half));
            THCudaHalfTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            self_->maybe_zero_dim(size.size() == 0);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_set_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_set_(Tensor & self, const Tensor & source) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_tensor_unwrap(source,"source",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_set(globalContext().getTHCState(), self_, source_);
            self_->maybe_zero_dim(source_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_tensor_unwrap(source,"source",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_set(globalContext().getTHCState(), self_, source_);
            self_->maybe_zero_dim(source_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto source_ = checked_tensor_unwrap(source,"source",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_set(globalContext().getTHCState(), self_, source_);
            self_->maybe_zero_dim(source_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto source_ = checked_tensor_unwrap(source,"source",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_set(globalContext().getTHCState(), self_, source_);
            self_->maybe_zero_dim(source_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto source_ = checked_tensor_unwrap(source,"source",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_set(globalContext().getTHCState(), self_, source_);
            self_->maybe_zero_dim(source_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto source_ = checked_tensor_unwrap(source,"source",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_set(globalContext().getTHCState(), self_, source_);
            self_->maybe_zero_dim(source_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_set(globalContext().getTHCState(), self_, source_);
            self_->maybe_zero_dim(source_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto source_ = checked_tensor_unwrap(source,"source",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_set(globalContext().getTHCState(), self_, source_);
            self_->maybe_zero_dim(source_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto source_ = checked_tensor_unwrap(source,"source",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_set(globalContext().getTHCState(), self_, source_);
            self_->maybe_zero_dim(source_->dim() == 0);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_set_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_set_(Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_setStorage(globalContext().getTHCState(), self_, NULL, 0, {0}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_setStorage(globalContext().getTHCState(), self_, NULL, 0, {0}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_setStorage(globalContext().getTHCState(), self_, NULL, 0, {0}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_setStorage(globalContext().getTHCState(), self_, NULL, 0, {0}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_setStorage(globalContext().getTHCState(), self_, NULL, 0, {0}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_setStorage(globalContext().getTHCState(), self_, NULL, 0, {0}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_setStorage(globalContext().getTHCState(), self_, NULL, 0, {0}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_setStorage(globalContext().getTHCState(), self_, NULL, 0, {0}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_setStorage(globalContext().getTHCState(), self_, NULL, 0, {0}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_set_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_fill_(Tensor & self, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto value_ = value.toBool();
            THCudaBoolTensor_fill(globalContext().getTHCState(), self_, value_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toByte();
            THCudaByteTensor_fill(globalContext().getTHCState(), self_, value_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto value_ = value.toChar();
            THCudaCharTensor_fill(globalContext().getTHCState(), self_, value_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto value_ = value.toDouble();
            THCudaDoubleTensor_fill(globalContext().getTHCState(), self_, value_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto value_ = value.toFloat();
            THCudaTensor_fill(globalContext().getTHCState(), self_, value_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto value_ = value.toInt();
            THCudaIntTensor_fill(globalContext().getTHCState(), self_, value_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toLong();
            THCudaLongTensor_fill(globalContext().getTHCState(), self_, value_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto value_ = value.toShort();
            THCudaShortTensor_fill(globalContext().getTHCState(), self_, value_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto value_ = value.toHalf();
            THCudaHalfTensor_fill(globalContext().getTHCState(), self_, value_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_fill_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_fill_(Tensor & self, const Tensor & value) const {
    // DeviceGuard omitted
    if (value.dim() == 0) {
        return static_cast<const TypeExtendedInterface*>(this)->_th_fill_(self, value.item());
    }
    AT_ERROR("_th_fill_ only supports a 0-dimensional value tensor, but got tensor "
        "with ", value.dim(), " dimension(s).");
}
bool CUDAType::_th_is_set_to(const Tensor & self, const Tensor & tensor) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto tensor_ = checked_tensor_unwrap(tensor,"tensor",2, false, Backend::CUDA, ScalarType::Bool);
            return THCudaBoolTensor_isSetTo(globalContext().getTHCState(), self_, tensor_);
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto tensor_ = checked_tensor_unwrap(tensor,"tensor",2, false, Backend::CUDA, ScalarType::Byte);
            return THCudaByteTensor_isSetTo(globalContext().getTHCState(), self_, tensor_);
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto tensor_ = checked_tensor_unwrap(tensor,"tensor",2, false, Backend::CUDA, ScalarType::Char);
            return THCudaCharTensor_isSetTo(globalContext().getTHCState(), self_, tensor_);
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto tensor_ = checked_tensor_unwrap(tensor,"tensor",2, false, Backend::CUDA, ScalarType::Double);
            return THCudaDoubleTensor_isSetTo(globalContext().getTHCState(), self_, tensor_);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto tensor_ = checked_tensor_unwrap(tensor,"tensor",2, false, Backend::CUDA, ScalarType::Float);
            return THCudaTensor_isSetTo(globalContext().getTHCState(), self_, tensor_);
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto tensor_ = checked_tensor_unwrap(tensor,"tensor",2, false, Backend::CUDA, ScalarType::Int);
            return THCudaIntTensor_isSetTo(globalContext().getTHCState(), self_, tensor_);
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto tensor_ = checked_tensor_unwrap(tensor,"tensor",2, false, Backend::CUDA, ScalarType::Long);
            return THCudaLongTensor_isSetTo(globalContext().getTHCState(), self_, tensor_);
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto tensor_ = checked_tensor_unwrap(tensor,"tensor",2, false, Backend::CUDA, ScalarType::Short);
            return THCudaShortTensor_isSetTo(globalContext().getTHCState(), self_, tensor_);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto tensor_ = checked_tensor_unwrap(tensor,"tensor",2, false, Backend::CUDA, ScalarType::Half);
            return THCudaHalfTensor_isSetTo(globalContext().getTHCState(), self_, tensor_);
            break;
        }
        default:
            AT_ERROR("_th_is_set_to not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_masked_fill_(Tensor & self, const Tensor & mask, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toByte();
            THCudaByteTensor_maskedFill(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toChar();
            THCudaCharTensor_maskedFill(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toDouble();
            THCudaDoubleTensor_maskedFill(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toFloat();
            THCudaTensor_maskedFill(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toInt();
            THCudaIntTensor_maskedFill(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toLong();
            THCudaLongTensor_maskedFill(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toShort();
            THCudaShortTensor_maskedFill(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toHalf();
            THCudaHalfTensor_maskedFill(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_masked_fill_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_masked_fill_(Tensor & self, const Tensor & mask, const Tensor & value) const {
    // DeviceGuard omitted
    if (value.dim() == 0) {
        return static_cast<const TypeExtendedInterface*>(this)->_th_masked_fill_(self, mask, value.item());
    }
    AT_ERROR("_th_masked_fill_ only supports a 0-dimensional value tensor, but got tensor "
        "with ", value.dim(), " dimension(s).");
}
Tensor & CUDAType::_th_masked_fill_bool_(Tensor & self, const Tensor & mask, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            auto value_ = value.toByte();
            THCudaByteTensor_maskedFillBool(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            auto value_ = value.toChar();
            THCudaCharTensor_maskedFillBool(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            auto value_ = value.toDouble();
            THCudaDoubleTensor_maskedFillBool(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            auto value_ = value.toFloat();
            THCudaTensor_maskedFillBool(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            auto value_ = value.toInt();
            THCudaIntTensor_maskedFillBool(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            auto value_ = value.toLong();
            THCudaLongTensor_maskedFillBool(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            auto value_ = value.toShort();
            THCudaShortTensor_maskedFillBool(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            auto value_ = value.toHalf();
            THCudaHalfTensor_maskedFillBool(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_masked_fill_bool_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_masked_fill_bool_(Tensor & self, const Tensor & mask, const Tensor & value) const {
    // DeviceGuard omitted
    if (value.dim() == 0) {
        return static_cast<const TypeExtendedInterface*>(this)->_th_masked_fill_bool_(self, mask, value.item());
    }
    AT_ERROR("_th_masked_fill_bool_ only supports a 0-dimensional value tensor, but got tensor "
        "with ", value.dim(), " dimension(s).");
}
Tensor & CUDAType::s__th_masked_scatter_(Tensor & self, const Tensor & mask, const Tensor & source) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_maskedCopy(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_maskedCopy(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_maskedCopy(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_maskedCopy(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_maskedCopy(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_maskedCopy(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_maskedCopy(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_maskedCopy(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_masked_scatter_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_masked_scatter_bool_(Tensor & self, const Tensor & mask, const Tensor & source) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_maskedCopyBool(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_maskedCopyBool(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_maskedCopyBool(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_maskedCopyBool(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_maskedCopyBool(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_maskedCopyBool(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_maskedCopyBool(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_maskedCopyBool(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_masked_scatter_bool_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_masked_select_out(Tensor & result, const Tensor & self, const Tensor & mask) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaCharTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaDoubleTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaIntTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaLongTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaShortTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaHalfTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_masked_select_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_masked_select(const Tensor & self, const Tensor & mask) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaCharTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaDoubleTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaIntTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaLongTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaShortTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaHalfTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_masked_select not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_masked_select_bool_out(Tensor & result, const Tensor & self, const Tensor & mask) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaByteTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaCharTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaDoubleTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaIntTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaLongTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaShortTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaHalfTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_masked_select_bool_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_masked_select_bool(const Tensor & self, const Tensor & mask) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaByteTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaCharTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaDoubleTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaIntTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaLongTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaShortTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaHalfTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_masked_select_bool not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_nonzero_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_nonzero_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_nonzero(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_nonzero not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_clone(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaBoolTensor_newClone(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaByteTensor_newClone(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaCharTensor_newClone(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaDoubleTensor_newClone(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaTensor_newClone(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaIntTensor_newClone(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaLongTensor_newClone(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaShortTensor_newClone(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaHalfTensor_newClone(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        default:
            AT_ERROR("_th_clone not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_view(const Tensor & self, IntArrayRef size) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaBoolTensor_newView(globalContext().getTHCState(), self_, size))->maybe_zero_dim(size.size() == 0)));
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaByteTensor_newView(globalContext().getTHCState(), self_, size))->maybe_zero_dim(size.size() == 0)));
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaCharTensor_newView(globalContext().getTHCState(), self_, size))->maybe_zero_dim(size.size() == 0)));
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaDoubleTensor_newView(globalContext().getTHCState(), self_, size))->maybe_zero_dim(size.size() == 0)));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaTensor_newView(globalContext().getTHCState(), self_, size))->maybe_zero_dim(size.size() == 0)));
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaIntTensor_newView(globalContext().getTHCState(), self_, size))->maybe_zero_dim(size.size() == 0)));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaLongTensor_newView(globalContext().getTHCState(), self_, size))->maybe_zero_dim(size.size() == 0)));
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaShortTensor_newView(globalContext().getTHCState(), self_, size))->maybe_zero_dim(size.size() == 0)));
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaHalfTensor_newView(globalContext().getTHCState(), self_, size))->maybe_zero_dim(size.size() == 0)));
            break;
        }
        default:
            AT_ERROR("_th_view not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_resize_as_(Tensor & self, const Tensor & the_template) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto the_template_ = checked_tensor_unwrap(the_template,"the_template",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_resizeAs(globalContext().getTHCState(), self_, the_template_);
            self_->maybe_zero_dim(the_template_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto the_template_ = checked_tensor_unwrap(the_template,"the_template",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_resizeAs(globalContext().getTHCState(), self_, the_template_);
            self_->maybe_zero_dim(the_template_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto the_template_ = checked_tensor_unwrap(the_template,"the_template",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_resizeAs(globalContext().getTHCState(), self_, the_template_);
            self_->maybe_zero_dim(the_template_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto the_template_ = checked_tensor_unwrap(the_template,"the_template",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_resizeAs(globalContext().getTHCState(), self_, the_template_);
            self_->maybe_zero_dim(the_template_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto the_template_ = checked_tensor_unwrap(the_template,"the_template",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_resizeAs(globalContext().getTHCState(), self_, the_template_);
            self_->maybe_zero_dim(the_template_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto the_template_ = checked_tensor_unwrap(the_template,"the_template",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_resizeAs(globalContext().getTHCState(), self_, the_template_);
            self_->maybe_zero_dim(the_template_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto the_template_ = checked_tensor_unwrap(the_template,"the_template",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_resizeAs(globalContext().getTHCState(), self_, the_template_);
            self_->maybe_zero_dim(the_template_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto the_template_ = checked_tensor_unwrap(the_template,"the_template",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_resizeAs(globalContext().getTHCState(), self_, the_template_);
            self_->maybe_zero_dim(the_template_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto the_template_ = checked_tensor_unwrap(the_template,"the_template",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_resizeAs(globalContext().getTHCState(), self_, the_template_);
            self_->maybe_zero_dim(the_template_->dim() == 0);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_resize_as_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_index_select_out(Tensor & result, const Tensor & self, int64_t dim, const Tensor & index) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaByteTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaCharTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaDoubleTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaIntTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaShortTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaHalfTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_index_select_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_index_select(const Tensor & self, int64_t dim, const Tensor & index) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaByteTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaCharTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaDoubleTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaIntTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaShortTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaHalfTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_index_select not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_index_copy_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_indexCopy(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_indexCopy(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_indexCopy(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_indexCopy(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_indexCopy(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_indexCopy(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_indexCopy(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_indexCopy(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_index_copy_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_take_out(Tensor & result, const Tensor & self, const Tensor & index) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            THCudaByteTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            THCudaCharTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            THCudaDoubleTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            THCudaTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            THCudaIntTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            THCudaShortTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            THCudaHalfTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_take_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_take(const Tensor & self, const Tensor & index) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            THCudaByteTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            THCudaCharTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            THCudaDoubleTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            THCudaTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            THCudaIntTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            THCudaShortTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            THCudaHalfTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_take not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_put_(Tensor & self, const Tensor & index, const Tensor & source, bool accumulate) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_put(globalContext().getTHCState(), self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_put(globalContext().getTHCState(), self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_put(globalContext().getTHCState(), self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_put(globalContext().getTHCState(), self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_put(globalContext().getTHCState(), self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_put(globalContext().getTHCState(), self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_put(globalContext().getTHCState(), self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_put(globalContext().getTHCState(), self_, index_, source_, accumulate);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_put_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_index_add_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_indexAdd(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_indexAdd(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_indexAdd(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_indexAdd(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_indexAdd(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_indexAdd(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_indexAdd(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_indexAdd(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_index_add_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_index_fill_(Tensor & self, int64_t dim, const Tensor & index, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toByte();
            THCudaByteTensor_indexFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toChar();
            THCudaCharTensor_indexFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toDouble();
            THCudaDoubleTensor_indexFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toFloat();
            THCudaTensor_indexFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toInt();
            THCudaIntTensor_indexFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toLong();
            THCudaLongTensor_indexFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toShort();
            THCudaShortTensor_indexFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toHalf();
            THCudaHalfTensor_indexFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_index_fill_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_index_fill_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & value) const {
    // DeviceGuard omitted
    if (value.dim() == 0) {
        return static_cast<const TypeExtendedInterface*>(this)->_th_index_fill_(self, dim, index, value.item());
    }
    AT_ERROR("_th_index_fill_ only supports a 0-dimensional value tensor, but got tensor "
        "with ", value.dim(), " dimension(s).");
}
Tensor & CUDAType::_th_unfold_out(Tensor & result, const Tensor & self, int64_t dimension, int64_t size, int64_t step) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaBoolTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaByteTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaCharTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaDoubleTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaIntTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaLongTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaShortTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaHalfTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_unfold_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_unfold(const Tensor & self, int64_t dimension, int64_t size, int64_t step) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaBoolTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaByteTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaCharTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaDoubleTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaIntTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaLongTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaShortTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaHalfTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_unfold not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_scatter_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) const {
    // DeviceGuard omitted
    if (src.dim() == 0) {
        return static_cast<const TypeExtendedInterface*>(this)->_th_scatter_(self, dim, index, src.item());
    }
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_scatter(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_scatter(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_scatter(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_scatter(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_scatter(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_scatter(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_scatter(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_scatter(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_scatter_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_scatter_(Tensor & self, int64_t dim, const Tensor & index, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toByte();
            THCudaByteTensor_scatterFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toChar();
            THCudaCharTensor_scatterFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toDouble();
            THCudaDoubleTensor_scatterFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toFloat();
            THCudaTensor_scatterFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toInt();
            THCudaIntTensor_scatterFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toLong();
            THCudaLongTensor_scatterFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toShort();
            THCudaShortTensor_scatterFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toHalf();
            THCudaHalfTensor_scatterFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_scatter_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_scatter_add_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_scatterAdd(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_scatterAdd(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_scatterAdd(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_scatterAdd(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_scatterAdd(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_scatterAdd(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_scatterAdd(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_scatterAdd(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_scatter_add_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_gather_out(Tensor & result, const Tensor & self, int64_t dim, const Tensor & index) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaByteTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaCharTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaDoubleTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaIntTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaShortTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaHalfTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gather_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_gather(const Tensor & self, int64_t dim, const Tensor & index) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaByteTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaCharTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaDoubleTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaIntTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaShortTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
            THCudaHalfTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gather not supported on CUDAType for ", dispatch_scalar_type);
    }
}
bool CUDAType::_th_equal(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            return THCudaByteTensor_equal(globalContext().getTHCState(), self_, other_);
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            return THCudaCharTensor_equal(globalContext().getTHCState(), self_, other_);
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            return THCudaDoubleTensor_equal(globalContext().getTHCState(), self_, other_);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            return THCudaTensor_equal(globalContext().getTHCState(), self_, other_);
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            return THCudaIntTensor_equal(globalContext().getTHCState(), self_, other_);
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            return THCudaLongTensor_equal(globalContext().getTHCState(), self_, other_);
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            return THCudaShortTensor_equal(globalContext().getTHCState(), self_, other_);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            return THCudaHalfTensor_equal(globalContext().getTHCState(), self_, other_);
            break;
        }
        default:
            AT_ERROR("_th_equal not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_and_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_and_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_and(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_and not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_and_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_and_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_and(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_and not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_iand_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_bitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_bitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_bitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_bitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_bitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_bitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_bitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_bitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_iand_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_iand_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cbitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cbitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cbitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cbitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cbitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cbitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cbitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cbitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_iand_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_or_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_or_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_or(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_or not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_or_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_or_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_or(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_or not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_ior_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_bitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_bitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_bitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_bitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_bitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_bitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_bitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_bitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ior_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_ior_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cbitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cbitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cbitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cbitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cbitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cbitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cbitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cbitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ior_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_xor_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_xor_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_xor(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_xor not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_xor_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_xor_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_xor(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_xor not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_ixor_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_bitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_bitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_bitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_bitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_bitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_bitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_bitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_bitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ixor_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_ixor_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cbitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cbitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cbitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cbitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cbitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cbitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cbitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cbitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ixor_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_lshift_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lshift_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_lshift(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lshift not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_lshift_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lshift_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_lshift(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lshift not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_ilshift_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_lshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_lshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_lshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_lshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_lshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_lshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_lshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_lshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ilshift_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_ilshift_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_clshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_clshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_clshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_clshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_clshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_clshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_clshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_clshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ilshift_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_rshift_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_rshift_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_rshift(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_rshift not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_rshift_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_rshift_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_rshift(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_rshift not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_irshift_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_rshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_rshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_rshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_rshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_rshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_rshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_rshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_rshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_irshift_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_irshift_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_crshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_crshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_crshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_crshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_crshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_crshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_crshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_crshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_irshift_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_lt_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lt_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_lt(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lt not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_lt_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lt_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_lt(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lt not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_lt_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_ltValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_ltValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_ltValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_ltValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_ltValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_ltValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_ltValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_ltValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_ltValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_lt_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_lt_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_ltTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_ltTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_ltTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_ltTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_ltTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_ltTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_ltTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_ltTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_ltTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_lt_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_gt_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gt_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_gt(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gt not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_gt_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gt_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_gt(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gt not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_gt_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_gtValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_gtValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_gtValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_gtValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_gtValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_gtValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_gtValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_gtValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_gtValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_gt_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_gt_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_gtTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_gtTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_gtTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_gtTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_gtTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_gtTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_gtTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_gtTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_gtTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_gt_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_le_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_le_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_le(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_le not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_le_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_le_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_le(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_le not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_le_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_leValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_leValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_leValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_leValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_leValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_leValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_leValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_leValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_leValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_le_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_le_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_leTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_leTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_leTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_leTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_leTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_leTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_leTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_leTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_leTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_le_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_ge_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ge_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_ge(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ge not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_ge_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ge_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_ge(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ge not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_ge_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_geValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_geValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_geValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_geValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_geValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_geValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_geValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_geValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_geValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ge_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_ge_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_geTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_geTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_geTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_geTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_geTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_geTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_geTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_geTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_geTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ge_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_eq_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_eq_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_eq(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_eq not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_eq_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_eq_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_eq(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_eq not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_eq_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_eqValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_eqValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_eqValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_eqValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_eqValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_eqValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_eqValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_eqValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_eqValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_eq_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_eq_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_eqTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_eqTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_eqTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_eqTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_eqTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_eqTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_eqTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_eqTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_eqTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_eq_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_ne_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ne_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_ne(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ne not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_ne_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ne_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_ne(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ne not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_ne_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_neValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_neValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_neValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_neValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_neValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_neValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_neValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_neValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_neValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ne_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_ne_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_neTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_neTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_neTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_neTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_neTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_neTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_neTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_neTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_neTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ne_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_min_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_min_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_min(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_min not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_min(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            return at::scalar_tensor(convert<uint8_t>(THCudaByteTensor_minall(globalContext().getTHCState(), self_)), options(ScalarType::Byte));
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            return at::scalar_tensor(convert<int8_t>(THCudaCharTensor_minall(globalContext().getTHCState(), self_)), options(ScalarType::Char));
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            return at::scalar_tensor(convert<double>(THCudaDoubleTensor_minall(globalContext().getTHCState(), self_)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THCudaTensor_minall(globalContext().getTHCState(), self_)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            return at::scalar_tensor(convert<int>(THCudaIntTensor_minall(globalContext().getTHCState(), self_)), options(ScalarType::Int));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            return at::scalar_tensor(convert<int64_t>(THCudaLongTensor_minall(globalContext().getTHCState(), self_)), options(ScalarType::Long));
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            return at::scalar_tensor(convert<int16_t>(THCudaShortTensor_minall(globalContext().getTHCState(), self_)), options(ScalarType::Short));
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            return at::scalar_tensor(convert<Half>(THCudaHalfTensor_minall(globalContext().getTHCState(), self_)), options(ScalarType::Half));
            break;
        }
        default:
            AT_ERROR("_th_min not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::_th_min_out(Tensor & min, Tensor & min_indices, const Tensor & self, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto min_ = checked_tensor_unwrap(min,"min",0, false, Backend::CUDA, ScalarType::Byte);
            auto min_indices_ = checked_tensor_unwrap(min_indices,"min_indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Char: {
            auto min_ = checked_tensor_unwrap(min,"min",0, false, Backend::CUDA, ScalarType::Char);
            auto min_indices_ = checked_tensor_unwrap(min_indices,"min_indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Double: {
            auto min_ = checked_tensor_unwrap(min,"min",0, false, Backend::CUDA, ScalarType::Double);
            auto min_indices_ = checked_tensor_unwrap(min_indices,"min_indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Float: {
            auto min_ = checked_tensor_unwrap(min,"min",0, false, Backend::CUDA, ScalarType::Float);
            auto min_indices_ = checked_tensor_unwrap(min_indices,"min_indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Int: {
            auto min_ = checked_tensor_unwrap(min,"min",0, false, Backend::CUDA, ScalarType::Int);
            auto min_indices_ = checked_tensor_unwrap(min_indices,"min_indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Long: {
            auto min_ = checked_tensor_unwrap(min,"min",0, false, Backend::CUDA, ScalarType::Long);
            auto min_indices_ = checked_tensor_unwrap(min_indices,"min_indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Short: {
            auto min_ = checked_tensor_unwrap(min,"min",0, false, Backend::CUDA, ScalarType::Short);
            auto min_indices_ = checked_tensor_unwrap(min_indices,"min_indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Half: {
            auto min_ = checked_tensor_unwrap(min,"min",0, false, Backend::CUDA, ScalarType::Half);
            auto min_indices_ = checked_tensor_unwrap(min_indices,"min_indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        default:
            AT_ERROR("_th_min_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_th_min(const Tensor & self, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Char: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Double: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Float: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Int: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Long: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Short: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Half: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        default:
            AT_ERROR("_th_min not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_max_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_max_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_max(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_max not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_max(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            return at::scalar_tensor(convert<uint8_t>(THCudaByteTensor_maxall(globalContext().getTHCState(), self_)), options(ScalarType::Byte));
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            return at::scalar_tensor(convert<int8_t>(THCudaCharTensor_maxall(globalContext().getTHCState(), self_)), options(ScalarType::Char));
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            return at::scalar_tensor(convert<double>(THCudaDoubleTensor_maxall(globalContext().getTHCState(), self_)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THCudaTensor_maxall(globalContext().getTHCState(), self_)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            return at::scalar_tensor(convert<int>(THCudaIntTensor_maxall(globalContext().getTHCState(), self_)), options(ScalarType::Int));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            return at::scalar_tensor(convert<int64_t>(THCudaLongTensor_maxall(globalContext().getTHCState(), self_)), options(ScalarType::Long));
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            return at::scalar_tensor(convert<int16_t>(THCudaShortTensor_maxall(globalContext().getTHCState(), self_)), options(ScalarType::Short));
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            return at::scalar_tensor(convert<Half>(THCudaHalfTensor_maxall(globalContext().getTHCState(), self_)), options(ScalarType::Half));
            break;
        }
        default:
            AT_ERROR("_th_max not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::_th_max_out(Tensor & max, Tensor & max_indices, const Tensor & self, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto max_ = checked_tensor_unwrap(max,"max",0, false, Backend::CUDA, ScalarType::Byte);
            auto max_indices_ = checked_tensor_unwrap(max_indices,"max_indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Char: {
            auto max_ = checked_tensor_unwrap(max,"max",0, false, Backend::CUDA, ScalarType::Char);
            auto max_indices_ = checked_tensor_unwrap(max_indices,"max_indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Double: {
            auto max_ = checked_tensor_unwrap(max,"max",0, false, Backend::CUDA, ScalarType::Double);
            auto max_indices_ = checked_tensor_unwrap(max_indices,"max_indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Float: {
            auto max_ = checked_tensor_unwrap(max,"max",0, false, Backend::CUDA, ScalarType::Float);
            auto max_indices_ = checked_tensor_unwrap(max_indices,"max_indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Int: {
            auto max_ = checked_tensor_unwrap(max,"max",0, false, Backend::CUDA, ScalarType::Int);
            auto max_indices_ = checked_tensor_unwrap(max_indices,"max_indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Long: {
            auto max_ = checked_tensor_unwrap(max,"max",0, false, Backend::CUDA, ScalarType::Long);
            auto max_indices_ = checked_tensor_unwrap(max_indices,"max_indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Short: {
            auto max_ = checked_tensor_unwrap(max,"max",0, false, Backend::CUDA, ScalarType::Short);
            auto max_indices_ = checked_tensor_unwrap(max_indices,"max_indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Half: {
            auto max_ = checked_tensor_unwrap(max,"max",0, false, Backend::CUDA, ScalarType::Half);
            auto max_indices_ = checked_tensor_unwrap(max_indices,"max_indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        default:
            AT_ERROR("_th_max_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_th_max(const Tensor & self, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Char: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Double: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Float: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Int: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Long: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Short: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Half: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        default:
            AT_ERROR("_th_max not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::_th_mode_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Byte);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Char: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Char);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Double: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Double);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Float: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Int: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Int);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Long);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Short: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Short);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Half: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Half);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_mode_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_th_mode(const Tensor & self, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Char: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Double: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Float: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Int: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Short: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Half: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_mode not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::_th_sort_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool descending) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Byte);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Char: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Char);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Double: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Double);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Float: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Int: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Int);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Long);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Short: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Short);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Half: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Half);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_sort_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_th_sort(const Tensor & self, int64_t dim, bool descending) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Char: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Double: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Float: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Int: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Short: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Half: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_sort not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::_th_topk_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Byte);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Char: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Char);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Double: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Double);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Float: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Int: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Int);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Long);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Short: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Short);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Half: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Half);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_topk_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_th_topk(const Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Char: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Double: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Float: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Int: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Short: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Half: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_topk not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_abs_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_abs_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_abs(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_abs not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_sigmoid_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_sigmoid(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_sigmoid(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_sigmoid(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sigmoid_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_sigmoid(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_sigmoid(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_sigmoid(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_sigmoid(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sigmoid not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_log_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_log(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_log(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_log(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_log_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_log(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_log(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_log(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_log(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_log not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_log10_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_log10(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_log10(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_log10(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_log10_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_log10(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_log10(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_log10(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_log10(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_log10 not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_log1p_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_log1p(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_log1p(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_log1p(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_log1p_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_log1p(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_log1p(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_log1p(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_log1p(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_log1p not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_log2_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_log2(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_log2(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_log2(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_log2_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_log2(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_log2(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_log2(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_log2(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_log2 not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_lgamma_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_lgamma(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_lgamma(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_lgamma(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lgamma_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_lgamma(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_lgamma(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_lgamma(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_lgamma(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lgamma not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_lgamma_(Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_lgamma(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_lgamma(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_lgamma(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_lgamma_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_digamma_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_digamma(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_digamma(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_digamma(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_digamma_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_digamma(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_digamma(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_digamma(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_digamma(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_digamma not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_digamma_(Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_digamma(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_digamma(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_digamma(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_digamma_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_polygamma_out(Tensor & result, int64_t n, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_polygamma(globalContext().getTHCState(), result_, n, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_polygamma(globalContext().getTHCState(), result_, n, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_polygamma(globalContext().getTHCState(), result_, n, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_polygamma_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_polygamma(int64_t n, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_polygamma(globalContext().getTHCState(), result_, n, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_polygamma(globalContext().getTHCState(), result_, n, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_polygamma(globalContext().getTHCState(), result_, n, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_polygamma not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_polygamma_(Tensor & self, int64_t n) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_polygamma(globalContext().getTHCState(), self_, n, self_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_polygamma(globalContext().getTHCState(), self_, n, self_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_polygamma(globalContext().getTHCState(), self_, n, self_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_polygamma_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_exp_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_exp(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_exp(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_exp(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_exp_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_exp(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_exp(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_exp(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_exp(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_exp not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_expm1_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_expm1(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_expm1(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_expm1(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_expm1_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_expm1(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_expm1(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_expm1(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_expm1(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_expm1 not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_cos_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cos_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_cos(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cos not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_acos_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_acos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_acos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_acos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_acos_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_acos(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_acos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_acos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_acos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_acos not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_cosh_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cosh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cosh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cosh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cosh_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_cosh(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cosh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cosh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cosh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cosh not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_sin_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_sin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_sin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_sin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sin_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_sin(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_sin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_sin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_sin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sin not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_asin_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_asin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_asin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_asin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_asin_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_asin(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_asin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_asin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_asin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_asin not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_sinh_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_sinh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_sinh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_sinh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sinh_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_sinh(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_sinh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_sinh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_sinh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sinh not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_tan_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_tan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_tan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_tan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_tan_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_tan(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_tan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_tan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_tan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_tan not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_atan_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_atan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_atan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_atan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_atan_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_atan(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_atan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_atan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_atan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_atan not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_tanh_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_tanh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_tanh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_tanh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_tanh_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_tanh(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_tanh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_tanh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_tanh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_tanh not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_erf_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_erf(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_erf(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_erf(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_erf_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_erf(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_erf(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_erf(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_erf(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_erf not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_erfc_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_erfc(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_erfc(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_erfc(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_erfc_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_erfc(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_erfc(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_erfc(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_erfc(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_erfc not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_erfinv_(Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_erfinv(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_erfinv(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_erfinv(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_erfinv_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_erfinv_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_erfinv(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_erfinv(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_erfinv(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_erfinv_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_erfinv(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_erfinv(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_erfinv(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_erfinv(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_erfinv not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_sqrt_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_sqrt(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_sqrt(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_sqrt(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sqrt_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_sqrt(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_sqrt(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_sqrt(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_sqrt(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sqrt not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_rsqrt_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_rsqrt(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_rsqrt(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_rsqrt(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_rsqrt_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_rsqrt(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_rsqrt(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_rsqrt(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_rsqrt(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_rsqrt not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_ceil_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_ceil(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_ceil(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_ceil(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ceil_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_ceil(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_ceil(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_ceil(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_ceil(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ceil not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_floor_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_floor(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_floor(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_floor(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_floor_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_floor(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_floor(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_floor(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_floor(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_floor not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_round_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_round(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_round(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_round(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_round_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_round(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_round(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_round(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_round(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_round not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_trunc_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_trunc(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_trunc(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_trunc(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_trunc_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_trunc(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_trunc(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_trunc(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_trunc(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_trunc not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_frac_(Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_frac(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_frac(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_frac(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_frac_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_frac_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_frac(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_frac(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_frac(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_frac_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_frac(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_frac(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_frac(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_frac(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_frac not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_var_out(Tensor & result, const Tensor & self, int64_t dim, bool unbiased, bool keepdim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_var(globalContext().getTHCState(), result_, self_, dim, (unbiased) ? 0 : 1, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_var(globalContext().getTHCState(), result_, self_, dim, (unbiased) ? 0 : 1, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_var(globalContext().getTHCState(), result_, self_, dim, (unbiased) ? 0 : 1, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        default:
            AT_ERROR("_th_var_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_var(const Tensor & self, int64_t dim, bool unbiased, bool keepdim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_var(globalContext().getTHCState(), result_, self_, dim, (unbiased) ? 0 : 1, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_var(globalContext().getTHCState(), result_, self_, dim, (unbiased) ? 0 : 1, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_var(globalContext().getTHCState(), result_, self_, dim, (unbiased) ? 0 : 1, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        default:
            AT_ERROR("_th_var not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_var(const Tensor & self, bool unbiased) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            return at::scalar_tensor(convert<double>(THCudaDoubleTensor_varall(globalContext().getTHCState(), self_, (unbiased) ? 0 : 1)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THCudaTensor_varall(globalContext().getTHCState(), self_, (unbiased) ? 0 : 1)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            return at::scalar_tensor(convert<Half>(THCudaHalfTensor_varall(globalContext().getTHCState(), self_, (unbiased) ? 0 : 1)), options(ScalarType::Half));
            break;
        }
        default:
            AT_ERROR("_th_var not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_std_out(Tensor & result, const Tensor & self, int64_t dim, bool unbiased, bool keepdim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_std(globalContext().getTHCState(), result_, self_, dim, (unbiased) ? 0 : 1, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_std(globalContext().getTHCState(), result_, self_, dim, (unbiased) ? 0 : 1, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_std(globalContext().getTHCState(), result_, self_, dim, (unbiased) ? 0 : 1, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        default:
            AT_ERROR("_th_std_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_std(const Tensor & self, int64_t dim, bool unbiased, bool keepdim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_std(globalContext().getTHCState(), result_, self_, dim, (unbiased) ? 0 : 1, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_std(globalContext().getTHCState(), result_, self_, dim, (unbiased) ? 0 : 1, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_std(globalContext().getTHCState(), result_, self_, dim, (unbiased) ? 0 : 1, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        default:
            AT_ERROR("_th_std not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_std(const Tensor & self, bool unbiased) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            return at::scalar_tensor(convert<double>(THCudaDoubleTensor_stdall(globalContext().getTHCState(), self_, (unbiased) ? 0 : 1)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THCudaTensor_stdall(globalContext().getTHCState(), self_, (unbiased) ? 0 : 1)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            return at::scalar_tensor(convert<Half>(THCudaHalfTensor_stdall(globalContext().getTHCState(), self_, (unbiased) ? 0 : 1)), options(ScalarType::Half));
            break;
        }
        default:
            AT_ERROR("_th_std not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_renorm_out(Tensor & result, const Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto p_ = p.toDouble();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toDouble();
            THCudaDoubleTensor_renorm(globalContext().getTHCState(), result_, self_, p_, dim, maxnorm_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto p_ = p.toFloat();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toFloat();
            THCudaTensor_renorm(globalContext().getTHCState(), result_, self_, p_, dim, maxnorm_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto p_ = p.toHalf();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toHalf();
            THCudaHalfTensor_renorm(globalContext().getTHCState(), result_, self_, p_, dim, maxnorm_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_renorm_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_renorm(const Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto p_ = p.toDouble();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toDouble();
            THCudaDoubleTensor_renorm(globalContext().getTHCState(), result_, self_, p_, dim, maxnorm_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto p_ = p.toFloat();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toFloat();
            THCudaTensor_renorm(globalContext().getTHCState(), result_, self_, p_, dim, maxnorm_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto p_ = p.toHalf();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toHalf();
            THCudaHalfTensor_renorm(globalContext().getTHCState(), result_, self_, p_, dim, maxnorm_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_renorm not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_renorm_(Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto p_ = p.toDouble();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toDouble();
            THCudaDoubleTensor_renorm(globalContext().getTHCState(), self_, self_, p_, dim, maxnorm_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto p_ = p.toFloat();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toFloat();
            THCudaTensor_renorm(globalContext().getTHCState(), self_, self_, p_, dim, maxnorm_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto p_ = p.toHalf();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toHalf();
            THCudaHalfTensor_renorm(globalContext().getTHCState(), self_, self_, p_, dim, maxnorm_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_renorm_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_dist(const Tensor & self, const Tensor & other, Scalar p) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            auto p_ = p.toDouble();
            return at::scalar_tensor(convert<double>(THCudaDoubleTensor_dist(globalContext().getTHCState(), self_, other_, p_)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            auto p_ = p.toFloat();
            return at::scalar_tensor(convert<float>(THCudaTensor_dist(globalContext().getTHCState(), self_, other_, p_)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            auto p_ = p.toHalf();
            return at::scalar_tensor(convert<Half>(THCudaHalfTensor_dist(globalContext().getTHCState(), self_, other_, p_)), options(ScalarType::Half));
            break;
        }
        default:
            AT_ERROR("_th_dist not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_reciprocal_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cinv(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cinv(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cinv(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_reciprocal_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_reciprocal(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cinv(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cinv(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cinv(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_reciprocal not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_reciprocal_(Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cinv(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cinv(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cinv(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_reciprocal_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_neg_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_neg(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_neg(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_neg(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_neg(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_neg(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_neg(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_neg(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_neg(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_neg_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_neg(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_neg(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_neg(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_neg(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_neg(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_neg(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_neg(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_neg(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_neg(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_neg not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_neg_(Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_neg(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_neg(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_neg(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_neg(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_neg(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_neg(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_neg(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_neg(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_neg_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_atan2_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_atan2(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_atan2(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_atan2(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_atan2_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_atan2(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_atan2(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_atan2(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_atan2(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_atan2 not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_atan2_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_atan2(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_atan2(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_atan2(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_atan2_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_pow_out(Tensor & result, const Tensor & self, Scalar exponent) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto exponent_ = exponent.toByte();
            THCudaByteTensor_pow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto exponent_ = exponent.toChar();
            THCudaCharTensor_pow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto exponent_ = exponent.toDouble();
            THCudaDoubleTensor_pow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto exponent_ = exponent.toFloat();
            THCudaTensor_pow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto exponent_ = exponent.toInt();
            THCudaIntTensor_pow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto exponent_ = exponent.toLong();
            THCudaLongTensor_pow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto exponent_ = exponent.toShort();
            THCudaShortTensor_pow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto exponent_ = exponent.toHalf();
            THCudaHalfTensor_pow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_pow_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_pow(const Tensor & self, Scalar exponent) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto exponent_ = exponent.toByte();
            THCudaByteTensor_pow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto exponent_ = exponent.toChar();
            THCudaCharTensor_pow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto exponent_ = exponent.toDouble();
            THCudaDoubleTensor_pow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto exponent_ = exponent.toFloat();
            THCudaTensor_pow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto exponent_ = exponent.toInt();
            THCudaIntTensor_pow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto exponent_ = exponent.toLong();
            THCudaLongTensor_pow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto exponent_ = exponent.toShort();
            THCudaShortTensor_pow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto exponent_ = exponent.toHalf();
            THCudaHalfTensor_pow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_pow not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_pow_out(Tensor & result, const Tensor & self, const Tensor & exponent) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_pow_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_pow(const Tensor & self, const Tensor & exponent) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_pow not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_pow_out(Tensor & result, Scalar self, const Tensor & exponent) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(result);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = self.toByte();
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_tpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = self.toChar();
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_tpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = self.toDouble();
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_tpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = self.toFloat();
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_tpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = self.toInt();
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_tpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = self.toLong();
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_tpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = self.toShort();
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_tpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = self.toHalf();
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_tpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(exponent_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_pow_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_pow(Scalar self, const Tensor & exponent) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(exponent);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = self.toByte();
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_tpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = self.toChar();
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_tpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = self.toDouble();
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_tpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = self.toFloat();
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_tpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = self.toInt();
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_tpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = self.toLong();
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_tpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = self.toShort();
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_tpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = self.toHalf();
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_tpow(globalContext().getTHCState(), result_, self_, exponent_);
            result_->maybe_zero_dim(exponent_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_pow not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_pow_(Tensor & self, Scalar exponent) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto exponent_ = exponent.toByte();
            THCudaByteTensor_pow(globalContext().getTHCState(), self_, self_, exponent_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto exponent_ = exponent.toChar();
            THCudaCharTensor_pow(globalContext().getTHCState(), self_, self_, exponent_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto exponent_ = exponent.toDouble();
            THCudaDoubleTensor_pow(globalContext().getTHCState(), self_, self_, exponent_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto exponent_ = exponent.toFloat();
            THCudaTensor_pow(globalContext().getTHCState(), self_, self_, exponent_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto exponent_ = exponent.toInt();
            THCudaIntTensor_pow(globalContext().getTHCState(), self_, self_, exponent_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto exponent_ = exponent.toLong();
            THCudaLongTensor_pow(globalContext().getTHCState(), self_, self_, exponent_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto exponent_ = exponent.toShort();
            THCudaShortTensor_pow(globalContext().getTHCState(), self_, self_, exponent_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto exponent_ = exponent.toHalf();
            THCudaHalfTensor_pow(globalContext().getTHCState(), self_, self_, exponent_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_pow_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_pow_(Tensor & self, const Tensor & exponent) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",3, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cpow(globalContext().getTHCState(), self_, self_, exponent_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",3, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cpow(globalContext().getTHCState(), self_, self_, exponent_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cpow(globalContext().getTHCState(), self_, self_, exponent_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cpow(globalContext().getTHCState(), self_, self_, exponent_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",3, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cpow(globalContext().getTHCState(), self_, self_, exponent_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",3, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cpow(globalContext().getTHCState(), self_, self_, exponent_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",3, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cpow(globalContext().getTHCState(), self_, self_, exponent_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cpow(globalContext().getTHCState(), self_, self_, exponent_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_pow_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_zero_(Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_zero(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_zero(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_zero(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_zero(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_zero(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_zero(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_zero(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_zero(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_zero(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_zero_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_cumsum_out(Tensor & result, const Tensor & self, int64_t dim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cumsum_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_cumsum(const Tensor & self, int64_t dim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cumsum not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_cumprod_out(Tensor & result, const Tensor & self, int64_t dim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cumprod_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_cumprod(const Tensor & self, int64_t dim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cumprod not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_sign_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_sign(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_sign(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_sign(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_sign(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_sign(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_sign(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_sign(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_sign(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sign_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_sign(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_sign(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_sign(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_sign(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_sign(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_sign(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_sign(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_sign(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_sign(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sign not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_sign_(Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_sign(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_sign(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_sign(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_sign(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_sign(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_sign(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_sign(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_sign(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_sign_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_trace(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            return at::scalar_tensor(convert<uint8_t>(THCudaByteTensor_trace(globalContext().getTHCState(), self_)), options(ScalarType::Byte));
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            return at::scalar_tensor(convert<int8_t>(THCudaCharTensor_trace(globalContext().getTHCState(), self_)), options(ScalarType::Char));
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            return at::scalar_tensor(convert<double>(THCudaDoubleTensor_trace(globalContext().getTHCState(), self_)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THCudaTensor_trace(globalContext().getTHCState(), self_)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            return at::scalar_tensor(convert<int>(THCudaIntTensor_trace(globalContext().getTHCState(), self_)), options(ScalarType::Int));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            return at::scalar_tensor(convert<int64_t>(THCudaLongTensor_trace(globalContext().getTHCState(), self_)), options(ScalarType::Long));
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            return at::scalar_tensor(convert<int16_t>(THCudaShortTensor_trace(globalContext().getTHCState(), self_)), options(ScalarType::Short));
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            return at::scalar_tensor(convert<Half>(THCudaHalfTensor_trace(globalContext().getTHCState(), self_)), options(ScalarType::Half));
            break;
        }
        default:
            AT_ERROR("_th_trace not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_fmod_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_fmod_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_fmod(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_fmod not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_fmod_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_fmod_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_fmod(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_fmod not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_fmod_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_fmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_fmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_fmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_fmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_fmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_fmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_fmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_fmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_fmod_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_fmod_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cfmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cfmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cfmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cfmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cfmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cfmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cfmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cfmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_fmod_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_remainder_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_remainder_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_remainder(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_remainder not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_remainder_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_remainder_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_remainder(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_remainder not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_remainder_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_remainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_remainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_remainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_remainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_remainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_remainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_remainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_remainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_remainder_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_remainder_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cremainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cremainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cremainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cremainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cremainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cremainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cremainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cremainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_remainder_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_clamp_out(Tensor & result, const Tensor & self, Scalar min, Scalar max) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto min_ = min.toByte();
            auto max_ = max.toByte();
            THCudaByteTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto min_ = min.toChar();
            auto max_ = max.toChar();
            THCudaCharTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto min_ = min.toDouble();
            auto max_ = max.toDouble();
            THCudaDoubleTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto min_ = min.toFloat();
            auto max_ = max.toFloat();
            THCudaTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto min_ = min.toInt();
            auto max_ = max.toInt();
            THCudaIntTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto min_ = min.toLong();
            auto max_ = max.toLong();
            THCudaLongTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto min_ = min.toShort();
            auto max_ = max.toShort();
            THCudaShortTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto min_ = min.toHalf();
            auto max_ = max.toHalf();
            THCudaHalfTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_clamp_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_clamp(const Tensor & self, Scalar min, Scalar max) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto min_ = min.toByte();
            auto max_ = max.toByte();
            THCudaByteTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto min_ = min.toChar();
            auto max_ = max.toChar();
            THCudaCharTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto min_ = min.toDouble();
            auto max_ = max.toDouble();
            THCudaDoubleTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto min_ = min.toFloat();
            auto max_ = max.toFloat();
            THCudaTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto min_ = min.toInt();
            auto max_ = max.toInt();
            THCudaIntTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto min_ = min.toLong();
            auto max_ = max.toLong();
            THCudaLongTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto min_ = min.toShort();
            auto max_ = max.toShort();
            THCudaShortTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto min_ = min.toHalf();
            auto max_ = max.toHalf();
            THCudaHalfTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_clamp not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_clamp_min_out(Tensor & result, const Tensor & self, Scalar min) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto min_ = min.toByte();
            THCudaByteTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto min_ = min.toChar();
            THCudaCharTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto min_ = min.toDouble();
            THCudaDoubleTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto min_ = min.toFloat();
            THCudaTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto min_ = min.toInt();
            THCudaIntTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto min_ = min.toLong();
            THCudaLongTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto min_ = min.toShort();
            THCudaShortTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto min_ = min.toHalf();
            THCudaHalfTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_clamp_min_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_clamp_min(const Tensor & self, Scalar min) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto min_ = min.toByte();
            THCudaByteTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto min_ = min.toChar();
            THCudaCharTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto min_ = min.toDouble();
            THCudaDoubleTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto min_ = min.toFloat();
            THCudaTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto min_ = min.toInt();
            THCudaIntTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto min_ = min.toLong();
            THCudaLongTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto min_ = min.toShort();
            THCudaShortTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto min_ = min.toHalf();
            THCudaHalfTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_clamp_min not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_clamp_max_out(Tensor & result, const Tensor & self, Scalar max) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto max_ = max.toByte();
            THCudaByteTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto max_ = max.toChar();
            THCudaCharTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto max_ = max.toDouble();
            THCudaDoubleTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto max_ = max.toFloat();
            THCudaTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto max_ = max.toInt();
            THCudaIntTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto max_ = max.toLong();
            THCudaLongTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto max_ = max.toShort();
            THCudaShortTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto max_ = max.toHalf();
            THCudaHalfTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_clamp_max_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_clamp_max(const Tensor & self, Scalar max) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto max_ = max.toByte();
            THCudaByteTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto max_ = max.toChar();
            THCudaCharTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto max_ = max.toDouble();
            THCudaDoubleTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto max_ = max.toFloat();
            THCudaTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto max_ = max.toInt();
            THCudaIntTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto max_ = max.toLong();
            THCudaLongTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto max_ = max.toShort();
            THCudaShortTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto max_ = max.toHalf();
            THCudaHalfTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_clamp_max not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_dot(const Tensor & self, const Tensor & tensor) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto tensor_ = checked_tensor_unwrap(tensor,"tensor",2, false, Backend::CUDA, ScalarType::Double);
            return at::scalar_tensor(convert<double>(THCudaDoubleTensor_dot(globalContext().getTHCState(), self_, tensor_)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto tensor_ = checked_tensor_unwrap(tensor,"tensor",2, false, Backend::CUDA, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THCudaTensor_dot(globalContext().getTHCState(), self_, tensor_)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto tensor_ = checked_tensor_unwrap(tensor,"tensor",2, false, Backend::CUDA, ScalarType::Half);
            return at::scalar_tensor(convert<Half>(THCudaHalfTensor_dot(globalContext().getTHCState(), self_, tensor_)), options(ScalarType::Half));
            break;
        }
        default:
            AT_ERROR("_th_dot not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_cross_kernel_out(Tensor & result, const Tensor & self, const Tensor & other, int64_t dim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cross_kernel_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_cross_kernel(const Tensor & self, const Tensor & other, int64_t dim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cross_kernel not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_diag_out(Tensor & result, const Tensor & self, int64_t diagonal) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaByteTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaCharTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaDoubleTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaIntTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaLongTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaShortTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaHalfTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_diag_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_diag(const Tensor & self, int64_t diagonal) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaByteTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaCharTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaDoubleTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaIntTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaLongTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaShortTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaHalfTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_diag not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_addmm_out(Tensor & result, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Byte);
            auto alpha_ = alpha.toByte();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CUDA, ScalarType::Byte);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat1_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Char);
            auto alpha_ = alpha.toChar();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CUDA, ScalarType::Char);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat1_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto alpha_ = alpha.toDouble();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CUDA, ScalarType::Double);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat1_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto alpha_ = alpha.toFloat();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CUDA, ScalarType::Float);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat1_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Int);
            auto alpha_ = alpha.toInt();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CUDA, ScalarType::Int);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat1_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Long);
            auto alpha_ = alpha.toLong();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CUDA, ScalarType::Long);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat1_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Short);
            auto alpha_ = alpha.toShort();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CUDA, ScalarType::Short);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat1_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto alpha_ = alpha.toHalf();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CUDA, ScalarType::Half);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat1_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addmm_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toByte();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Byte);
            auto alpha_ = alpha.toByte();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CUDA, ScalarType::Byte);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat1_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toChar();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Char);
            auto alpha_ = alpha.toChar();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CUDA, ScalarType::Char);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat1_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toDouble();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto alpha_ = alpha.toDouble();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CUDA, ScalarType::Double);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat1_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toFloat();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto alpha_ = alpha.toFloat();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CUDA, ScalarType::Float);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat1_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toInt();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Int);
            auto alpha_ = alpha.toInt();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CUDA, ScalarType::Int);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat1_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toLong();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Long);
            auto alpha_ = alpha.toLong();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CUDA, ScalarType::Long);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat1_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toShort();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Short);
            auto alpha_ = alpha.toShort();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CUDA, ScalarType::Short);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat1_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toHalf();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto alpha_ = alpha.toHalf();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CUDA, ScalarType::Half);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat1_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addmm not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",5, false, Backend::CUDA, ScalarType::Byte);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",6, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addmm(globalContext().getTHCState(), self_, beta_, self_, alpha_, mat1_, mat2_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",5, false, Backend::CUDA, ScalarType::Char);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",6, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addmm(globalContext().getTHCState(), self_, beta_, self_, alpha_, mat1_, mat2_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",5, false, Backend::CUDA, ScalarType::Double);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",6, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addmm(globalContext().getTHCState(), self_, beta_, self_, alpha_, mat1_, mat2_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",5, false, Backend::CUDA, ScalarType::Float);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",6, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addmm(globalContext().getTHCState(), self_, beta_, self_, alpha_, mat1_, mat2_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",5, false, Backend::CUDA, ScalarType::Int);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",6, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addmm(globalContext().getTHCState(), self_, beta_, self_, alpha_, mat1_, mat2_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",5, false, Backend::CUDA, ScalarType::Long);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",6, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addmm(globalContext().getTHCState(), self_, beta_, self_, alpha_, mat1_, mat2_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",5, false, Backend::CUDA, ScalarType::Short);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",6, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addmm(globalContext().getTHCState(), self_, beta_, self_, alpha_, mat1_, mat2_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto alpha_ = alpha.toHalf();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",5, false, Backend::CUDA, ScalarType::Half);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",6, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addmm(globalContext().getTHCState(), self_, beta_, self_, alpha_, mat1_, mat2_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_addmm_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_addmv_out(Tensor & result, const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Byte);
            auto alpha_ = alpha.toByte();
            auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CUDA, ScalarType::Byte);
            auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addmv(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Char);
            auto alpha_ = alpha.toChar();
            auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CUDA, ScalarType::Char);
            auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addmv(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto alpha_ = alpha.toDouble();
            auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CUDA, ScalarType::Double);
            auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addmv(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto alpha_ = alpha.toFloat();
            auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CUDA, ScalarType::Float);
            auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addmv(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Int);
            auto alpha_ = alpha.toInt();
            auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CUDA, ScalarType::Int);
            auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addmv(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Long);
            auto alpha_ = alpha.toLong();
            auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CUDA, ScalarType::Long);
            auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addmv(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Short);
            auto alpha_ = alpha.toShort();
            auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CUDA, ScalarType::Short);
            auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addmv(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto alpha_ = alpha.toHalf();
            auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CUDA, ScalarType::Half);
            auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addmv(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addmv_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_addmv(const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toByte();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Byte);
            auto alpha_ = alpha.toByte();
            auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CUDA, ScalarType::Byte);
            auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addmv(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toChar();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Char);
            auto alpha_ = alpha.toChar();
            auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CUDA, ScalarType::Char);
            auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addmv(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toDouble();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto alpha_ = alpha.toDouble();
            auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CUDA, ScalarType::Double);
            auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addmv(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toFloat();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto alpha_ = alpha.toFloat();
            auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CUDA, ScalarType::Float);
            auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addmv(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toInt();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Int);
            auto alpha_ = alpha.toInt();
            auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CUDA, ScalarType::Int);
            auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addmv(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toLong();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Long);
            auto alpha_ = alpha.toLong();
            auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CUDA, ScalarType::Long);
            auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addmv(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toShort();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Short);
            auto alpha_ = alpha.toShort();
            auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CUDA, ScalarType::Short);
            auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addmv(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toHalf();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto alpha_ = alpha.toHalf();
            auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CUDA, ScalarType::Half);
            auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addmv(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addmv not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_addmv_(Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            auto mat_ = checked_tensor_unwrap(mat,"mat",5, false, Backend::CUDA, ScalarType::Byte);
            auto vec_ = checked_tensor_unwrap(vec,"vec",6, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addmv(globalContext().getTHCState(), self_, beta_, self_, alpha_, mat_, vec_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            auto mat_ = checked_tensor_unwrap(mat,"mat",5, false, Backend::CUDA, ScalarType::Char);
            auto vec_ = checked_tensor_unwrap(vec,"vec",6, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addmv(globalContext().getTHCState(), self_, beta_, self_, alpha_, mat_, vec_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            auto mat_ = checked_tensor_unwrap(mat,"mat",5, false, Backend::CUDA, ScalarType::Double);
            auto vec_ = checked_tensor_unwrap(vec,"vec",6, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addmv(globalContext().getTHCState(), self_, beta_, self_, alpha_, mat_, vec_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            auto mat_ = checked_tensor_unwrap(mat,"mat",5, false, Backend::CUDA, ScalarType::Float);
            auto vec_ = checked_tensor_unwrap(vec,"vec",6, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addmv(globalContext().getTHCState(), self_, beta_, self_, alpha_, mat_, vec_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            auto mat_ = checked_tensor_unwrap(mat,"mat",5, false, Backend::CUDA, ScalarType::Int);
            auto vec_ = checked_tensor_unwrap(vec,"vec",6, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addmv(globalContext().getTHCState(), self_, beta_, self_, alpha_, mat_, vec_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            auto mat_ = checked_tensor_unwrap(mat,"mat",5, false, Backend::CUDA, ScalarType::Long);
            auto vec_ = checked_tensor_unwrap(vec,"vec",6, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addmv(globalContext().getTHCState(), self_, beta_, self_, alpha_, mat_, vec_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            auto mat_ = checked_tensor_unwrap(mat,"mat",5, false, Backend::CUDA, ScalarType::Short);
            auto vec_ = checked_tensor_unwrap(vec,"vec",6, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addmv(globalContext().getTHCState(), self_, beta_, self_, alpha_, mat_, vec_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto alpha_ = alpha.toHalf();
            auto mat_ = checked_tensor_unwrap(mat,"mat",5, false, Backend::CUDA, ScalarType::Half);
            auto vec_ = checked_tensor_unwrap(vec,"vec",6, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addmv(globalContext().getTHCState(), self_, beta_, self_, alpha_, mat_, vec_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_addmv_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_addr_out(Tensor & result, const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Byte);
            auto alpha_ = alpha.toByte();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CUDA, ScalarType::Byte);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addr(globalContext().getTHCState(), result_, beta_, self_, alpha_, vec1_, vec2_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Char);
            auto alpha_ = alpha.toChar();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CUDA, ScalarType::Char);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addr(globalContext().getTHCState(), result_, beta_, self_, alpha_, vec1_, vec2_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto alpha_ = alpha.toDouble();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CUDA, ScalarType::Double);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addr(globalContext().getTHCState(), result_, beta_, self_, alpha_, vec1_, vec2_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto alpha_ = alpha.toFloat();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CUDA, ScalarType::Float);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addr(globalContext().getTHCState(), result_, beta_, self_, alpha_, vec1_, vec2_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Int);
            auto alpha_ = alpha.toInt();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CUDA, ScalarType::Int);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addr(globalContext().getTHCState(), result_, beta_, self_, alpha_, vec1_, vec2_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Long);
            auto alpha_ = alpha.toLong();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CUDA, ScalarType::Long);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addr(globalContext().getTHCState(), result_, beta_, self_, alpha_, vec1_, vec2_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Short);
            auto alpha_ = alpha.toShort();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CUDA, ScalarType::Short);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addr(globalContext().getTHCState(), result_, beta_, self_, alpha_, vec1_, vec2_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto alpha_ = alpha.toHalf();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CUDA, ScalarType::Half);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addr(globalContext().getTHCState(), result_, beta_, self_, alpha_, vec1_, vec2_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addr_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_addr(const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toByte();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Byte);
            auto alpha_ = alpha.toByte();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CUDA, ScalarType::Byte);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addr(globalContext().getTHCState(), result_, beta_, self_, alpha_, vec1_, vec2_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toChar();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Char);
            auto alpha_ = alpha.toChar();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CUDA, ScalarType::Char);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addr(globalContext().getTHCState(), result_, beta_, self_, alpha_, vec1_, vec2_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toDouble();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto alpha_ = alpha.toDouble();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CUDA, ScalarType::Double);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addr(globalContext().getTHCState(), result_, beta_, self_, alpha_, vec1_, vec2_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toFloat();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto alpha_ = alpha.toFloat();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CUDA, ScalarType::Float);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addr(globalContext().getTHCState(), result_, beta_, self_, alpha_, vec1_, vec2_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toInt();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Int);
            auto alpha_ = alpha.toInt();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CUDA, ScalarType::Int);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addr(globalContext().getTHCState(), result_, beta_, self_, alpha_, vec1_, vec2_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toLong();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Long);
            auto alpha_ = alpha.toLong();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CUDA, ScalarType::Long);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addr(globalContext().getTHCState(), result_, beta_, self_, alpha_, vec1_, vec2_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toShort();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Short);
            auto alpha_ = alpha.toShort();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CUDA, ScalarType::Short);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addr(globalContext().getTHCState(), result_, beta_, self_, alpha_, vec1_, vec2_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toHalf();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto alpha_ = alpha.toHalf();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CUDA, ScalarType::Half);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addr(globalContext().getTHCState(), result_, beta_, self_, alpha_, vec1_, vec2_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addr not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_addr_(Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",5, false, Backend::CUDA, ScalarType::Byte);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",6, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addr(globalContext().getTHCState(), self_, beta_, self_, alpha_, vec1_, vec2_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",5, false, Backend::CUDA, ScalarType::Char);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",6, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addr(globalContext().getTHCState(), self_, beta_, self_, alpha_, vec1_, vec2_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",5, false, Backend::CUDA, ScalarType::Double);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",6, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addr(globalContext().getTHCState(), self_, beta_, self_, alpha_, vec1_, vec2_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",5, false, Backend::CUDA, ScalarType::Float);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",6, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addr(globalContext().getTHCState(), self_, beta_, self_, alpha_, vec1_, vec2_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",5, false, Backend::CUDA, ScalarType::Int);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",6, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addr(globalContext().getTHCState(), self_, beta_, self_, alpha_, vec1_, vec2_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",5, false, Backend::CUDA, ScalarType::Long);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",6, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addr(globalContext().getTHCState(), self_, beta_, self_, alpha_, vec1_, vec2_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",5, false, Backend::CUDA, ScalarType::Short);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",6, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addr(globalContext().getTHCState(), self_, beta_, self_, alpha_, vec1_, vec2_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto alpha_ = alpha.toHalf();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",5, false, Backend::CUDA, ScalarType::Half);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",6, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addr(globalContext().getTHCState(), self_, beta_, self_, alpha_, vec1_, vec2_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_addr_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_ger_out(Tensor & result, const Tensor & self, const Tensor & vec2) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addr(globalContext().getTHCState(), result_, uint8_t(0), result_, uint8_t(1), self_, vec2_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addr(globalContext().getTHCState(), result_, int8_t(0), result_, int8_t(1), self_, vec2_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addr(globalContext().getTHCState(), result_, double(0), result_, double(1), self_, vec2_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addr(globalContext().getTHCState(), result_, float(0), result_, float(1), self_, vec2_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addr(globalContext().getTHCState(), result_, int(0), result_, int(1), self_, vec2_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addr(globalContext().getTHCState(), result_, int64_t(0), result_, int64_t(1), self_, vec2_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addr(globalContext().getTHCState(), result_, int16_t(0), result_, int16_t(1), self_, vec2_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addr(globalContext().getTHCState(), result_, Half(0), result_, Half(1), self_, vec2_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ger_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_ger(const Tensor & self, const Tensor & vec2) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addr(globalContext().getTHCState(), result_, uint8_t(0), result_, uint8_t(1), self_, vec2_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addr(globalContext().getTHCState(), result_, int8_t(0), result_, int8_t(1), self_, vec2_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addr(globalContext().getTHCState(), result_, double(0), result_, double(1), self_, vec2_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addr(globalContext().getTHCState(), result_, float(0), result_, float(1), self_, vec2_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addr(globalContext().getTHCState(), result_, int(0), result_, int(1), self_, vec2_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addr(globalContext().getTHCState(), result_, int64_t(0), result_, int64_t(1), self_, vec2_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addr(globalContext().getTHCState(), result_, int16_t(0), result_, int16_t(1), self_, vec2_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addr(globalContext().getTHCState(), result_, Half(0), result_, Half(1), self_, vec2_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ger not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_mv_out(Tensor & result, const Tensor & self, const Tensor & vec) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addmv(globalContext().getTHCState(), result_, uint8_t(0), result_, uint8_t(1), self_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addmv(globalContext().getTHCState(), result_, int8_t(0), result_, int8_t(1), self_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addmv(globalContext().getTHCState(), result_, double(0), result_, double(1), self_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addmv(globalContext().getTHCState(), result_, float(0), result_, float(1), self_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addmv(globalContext().getTHCState(), result_, int(0), result_, int(1), self_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addmv(globalContext().getTHCState(), result_, int64_t(0), result_, int64_t(1), self_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addmv(globalContext().getTHCState(), result_, int16_t(0), result_, int16_t(1), self_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addmv(globalContext().getTHCState(), result_, Half(0), result_, Half(1), self_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_mv_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_mv(const Tensor & self, const Tensor & vec) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addmv(globalContext().getTHCState(), result_, uint8_t(0), result_, uint8_t(1), self_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addmv(globalContext().getTHCState(), result_, int8_t(0), result_, int8_t(1), self_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addmv(globalContext().getTHCState(), result_, double(0), result_, double(1), self_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addmv(globalContext().getTHCState(), result_, float(0), result_, float(1), self_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addmv(globalContext().getTHCState(), result_, int(0), result_, int(1), self_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addmv(globalContext().getTHCState(), result_, int64_t(0), result_, int64_t(1), self_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addmv(globalContext().getTHCState(), result_, int16_t(0), result_, int16_t(1), self_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addmv(globalContext().getTHCState(), result_, Half(0), result_, Half(1), self_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_mv not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_mm_out(Tensor & result, const Tensor & self, const Tensor & mat2) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addmm(globalContext().getTHCState(), result_, uint8_t(0), result_, uint8_t(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addmm(globalContext().getTHCState(), result_, int8_t(0), result_, int8_t(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addmm(globalContext().getTHCState(), result_, double(0), result_, double(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addmm(globalContext().getTHCState(), result_, float(0), result_, float(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addmm(globalContext().getTHCState(), result_, int(0), result_, int(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addmm(globalContext().getTHCState(), result_, int64_t(0), result_, int64_t(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addmm(globalContext().getTHCState(), result_, int16_t(0), result_, int16_t(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addmm(globalContext().getTHCState(), result_, Half(0), result_, Half(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_mm_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_mm(const Tensor & self, const Tensor & mat2) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addmm(globalContext().getTHCState(), result_, uint8_t(0), result_, uint8_t(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addmm(globalContext().getTHCState(), result_, int8_t(0), result_, int8_t(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addmm(globalContext().getTHCState(), result_, double(0), result_, double(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addmm(globalContext().getTHCState(), result_, float(0), result_, float(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addmm(globalContext().getTHCState(), result_, int(0), result_, int(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addmm(globalContext().getTHCState(), result_, int64_t(0), result_, int64_t(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addmm(globalContext().getTHCState(), result_, int16_t(0), result_, int16_t(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addmm(globalContext().getTHCState(), result_, Half(0), result_, Half(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_mm not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_bmm_out(Tensor & result, const Tensor & self, const Tensor & mat2) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_baddbmm(globalContext().getTHCState(), result_, uint8_t(0), result_, uint8_t(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_baddbmm(globalContext().getTHCState(), result_, int8_t(0), result_, int8_t(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_baddbmm(globalContext().getTHCState(), result_, double(0), result_, double(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_baddbmm(globalContext().getTHCState(), result_, float(0), result_, float(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_baddbmm(globalContext().getTHCState(), result_, int(0), result_, int(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_baddbmm(globalContext().getTHCState(), result_, int64_t(0), result_, int64_t(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_baddbmm(globalContext().getTHCState(), result_, int16_t(0), result_, int16_t(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_baddbmm(globalContext().getTHCState(), result_, Half(0), result_, Half(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_bmm_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_bmm(const Tensor & self, const Tensor & mat2) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_baddbmm(globalContext().getTHCState(), result_, uint8_t(0), result_, uint8_t(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_baddbmm(globalContext().getTHCState(), result_, int8_t(0), result_, int8_t(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_baddbmm(globalContext().getTHCState(), result_, double(0), result_, double(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_baddbmm(globalContext().getTHCState(), result_, float(0), result_, float(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_baddbmm(globalContext().getTHCState(), result_, int(0), result_, int(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_baddbmm(globalContext().getTHCState(), result_, int64_t(0), result_, int64_t(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_baddbmm(globalContext().getTHCState(), result_, int16_t(0), result_, int16_t(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_baddbmm(globalContext().getTHCState(), result_, Half(0), result_, Half(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_bmm not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_addbmm_out(Tensor & result, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Byte);
            auto alpha_ = alpha.toByte();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Byte);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Char);
            auto alpha_ = alpha.toChar();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Char);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto alpha_ = alpha.toDouble();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Double);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto alpha_ = alpha.toFloat();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Float);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Int);
            auto alpha_ = alpha.toInt();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Int);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Long);
            auto alpha_ = alpha.toLong();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Long);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Short);
            auto alpha_ = alpha.toShort();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Short);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto alpha_ = alpha.toHalf();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Half);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addbmm_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_addbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toByte();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Byte);
            auto alpha_ = alpha.toByte();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Byte);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toChar();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Char);
            auto alpha_ = alpha.toChar();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Char);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toDouble();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto alpha_ = alpha.toDouble();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Double);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toFloat();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto alpha_ = alpha.toFloat();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Float);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toInt();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Int);
            auto alpha_ = alpha.toInt();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Int);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toLong();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Long);
            auto alpha_ = alpha.toLong();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Long);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toShort();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Short);
            auto alpha_ = alpha.toShort();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Short);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toHalf();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto alpha_ = alpha.toHalf();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Half);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addbmm not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_addbmm_(Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",5, false, Backend::CUDA, ScalarType::Byte);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",6, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addbmm(globalContext().getTHCState(), self_, beta_, self_, alpha_, batch1_, batch2_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",5, false, Backend::CUDA, ScalarType::Char);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",6, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addbmm(globalContext().getTHCState(), self_, beta_, self_, alpha_, batch1_, batch2_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",5, false, Backend::CUDA, ScalarType::Double);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",6, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addbmm(globalContext().getTHCState(), self_, beta_, self_, alpha_, batch1_, batch2_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",5, false, Backend::CUDA, ScalarType::Float);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",6, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addbmm(globalContext().getTHCState(), self_, beta_, self_, alpha_, batch1_, batch2_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",5, false, Backend::CUDA, ScalarType::Int);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",6, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addbmm(globalContext().getTHCState(), self_, beta_, self_, alpha_, batch1_, batch2_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",5, false, Backend::CUDA, ScalarType::Long);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",6, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addbmm(globalContext().getTHCState(), self_, beta_, self_, alpha_, batch1_, batch2_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",5, false, Backend::CUDA, ScalarType::Short);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",6, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addbmm(globalContext().getTHCState(), self_, beta_, self_, alpha_, batch1_, batch2_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto alpha_ = alpha.toHalf();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",5, false, Backend::CUDA, ScalarType::Half);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",6, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addbmm(globalContext().getTHCState(), self_, beta_, self_, alpha_, batch1_, batch2_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_addbmm_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_baddbmm_out(Tensor & result, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Byte);
            auto alpha_ = alpha.toByte();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Byte);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_baddbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Char);
            auto alpha_ = alpha.toChar();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Char);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_baddbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto alpha_ = alpha.toDouble();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Double);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_baddbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto alpha_ = alpha.toFloat();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Float);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_baddbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Int);
            auto alpha_ = alpha.toInt();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Int);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_baddbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Long);
            auto alpha_ = alpha.toLong();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Long);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_baddbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Short);
            auto alpha_ = alpha.toShort();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Short);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_baddbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto alpha_ = alpha.toHalf();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Half);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_baddbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_baddbmm_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_baddbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toByte();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Byte);
            auto alpha_ = alpha.toByte();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Byte);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_baddbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toChar();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Char);
            auto alpha_ = alpha.toChar();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Char);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_baddbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toDouble();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto alpha_ = alpha.toDouble();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Double);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_baddbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toFloat();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto alpha_ = alpha.toFloat();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Float);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_baddbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toInt();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Int);
            auto alpha_ = alpha.toInt();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Int);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_baddbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toLong();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Long);
            auto alpha_ = alpha.toLong();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Long);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_baddbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toShort();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Short);
            auto alpha_ = alpha.toShort();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Short);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_baddbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toHalf();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto alpha_ = alpha.toHalf();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Half);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_baddbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_baddbmm not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_addcmul_out(Tensor & result, const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toByte();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Byte);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addcmul(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto value_ = value.toChar();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Char);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addcmul(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto value_ = value.toDouble();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Double);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addcmul(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto value_ = value.toFloat();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Float);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addcmul(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto value_ = value.toInt();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Int);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addcmul(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toLong();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Long);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addcmul(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto value_ = value.toShort();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Short);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addcmul(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto value_ = value.toHalf();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Half);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addcmul(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addcmul_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_addcmul(const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toByte();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Byte);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addcmul(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto value_ = value.toChar();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Char);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addcmul(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto value_ = value.toDouble();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Double);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addcmul(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto value_ = value.toFloat();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Float);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addcmul(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto value_ = value.toInt();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Int);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addcmul(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toLong();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Long);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addcmul(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto value_ = value.toShort();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Short);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addcmul(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto value_ = value.toHalf();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Half);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addcmul(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addcmul not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_addcmul_(Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toByte();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CUDA, ScalarType::Byte);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addcmul(globalContext().getTHCState(), self_, self_, value_, tensor1_, tensor2_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto value_ = value.toChar();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CUDA, ScalarType::Char);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addcmul(globalContext().getTHCState(), self_, self_, value_, tensor1_, tensor2_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto value_ = value.toDouble();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CUDA, ScalarType::Double);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addcmul(globalContext().getTHCState(), self_, self_, value_, tensor1_, tensor2_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto value_ = value.toFloat();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CUDA, ScalarType::Float);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addcmul(globalContext().getTHCState(), self_, self_, value_, tensor1_, tensor2_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto value_ = value.toInt();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CUDA, ScalarType::Int);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addcmul(globalContext().getTHCState(), self_, self_, value_, tensor1_, tensor2_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toLong();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CUDA, ScalarType::Long);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addcmul(globalContext().getTHCState(), self_, self_, value_, tensor1_, tensor2_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto value_ = value.toShort();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CUDA, ScalarType::Short);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addcmul(globalContext().getTHCState(), self_, self_, value_, tensor1_, tensor2_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto value_ = value.toHalf();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CUDA, ScalarType::Half);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addcmul(globalContext().getTHCState(), self_, self_, value_, tensor1_, tensor2_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_addcmul_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_addcdiv_out(Tensor & result, const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toByte();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Byte);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addcdiv(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto value_ = value.toChar();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Char);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addcdiv(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto value_ = value.toDouble();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Double);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addcdiv(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto value_ = value.toFloat();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Float);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addcdiv(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto value_ = value.toInt();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Int);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addcdiv(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toLong();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Long);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addcdiv(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto value_ = value.toShort();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Short);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addcdiv(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto value_ = value.toHalf();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Half);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addcdiv(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addcdiv_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s__th_addcdiv(const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toByte();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Byte);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addcdiv(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto value_ = value.toChar();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Char);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addcdiv(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto value_ = value.toDouble();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Double);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addcdiv(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto value_ = value.toFloat();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Float);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addcdiv(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto value_ = value.toInt();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Int);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addcdiv(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toLong();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Long);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addcdiv(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto value_ = value.toShort();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Short);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addcdiv(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto value_ = value.toHalf();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Half);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addcdiv(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addcdiv not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s__th_addcdiv_(Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toByte();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CUDA, ScalarType::Byte);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addcdiv(globalContext().getTHCState(), self_, self_, value_, tensor1_, tensor2_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto value_ = value.toChar();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CUDA, ScalarType::Char);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addcdiv(globalContext().getTHCState(), self_, self_, value_, tensor1_, tensor2_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto value_ = value.toDouble();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CUDA, ScalarType::Double);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addcdiv(globalContext().getTHCState(), self_, self_, value_, tensor1_, tensor2_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto value_ = value.toFloat();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CUDA, ScalarType::Float);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addcdiv(globalContext().getTHCState(), self_, self_, value_, tensor1_, tensor2_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto value_ = value.toInt();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CUDA, ScalarType::Int);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addcdiv(globalContext().getTHCState(), self_, self_, value_, tensor1_, tensor2_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toLong();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CUDA, ScalarType::Long);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addcdiv(globalContext().getTHCState(), self_, self_, value_, tensor1_, tensor2_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto value_ = value.toShort();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CUDA, ScalarType::Short);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addcdiv(globalContext().getTHCState(), self_, self_, value_, tensor1_, tensor2_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto value_ = value.toHalf();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CUDA, ScalarType::Half);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addcdiv(globalContext().getTHCState(), self_, self_, value_, tensor1_, tensor2_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_addcdiv_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::_th_gels_out(Tensor & res1, Tensor & res2, const Tensor & self, const Tensor & A) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = checked_tensor_unwrap(res1,"res1",0, false, Backend::CUDA, ScalarType::Double);
            auto res2_ = checked_tensor_unwrap(res2,"res2",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto A_ = checked_tensor_unwrap(A,"A",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_gels(globalContext().getTHCState(), res1_, res2_, self_, A_);
            bool maybe_scalar = self_->dim() == 0 && A_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = checked_tensor_unwrap(res1,"res1",0, false, Backend::CUDA, ScalarType::Float);
            auto res2_ = checked_tensor_unwrap(res2,"res2",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto A_ = checked_tensor_unwrap(A,"A",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_gels(globalContext().getTHCState(), res1_, res2_, self_, A_);
            bool maybe_scalar = self_->dim() == 0 && A_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_gels_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_th_gels(const Tensor & self, const Tensor & A) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto A_ = checked_tensor_unwrap(A,"A",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_gels(globalContext().getTHCState(), res1_, res2_, self_, A_);
            bool maybe_scalar = self_->dim() == 0 && A_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto A_ = checked_tensor_unwrap(A,"A",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_gels(globalContext().getTHCState(), res1_, res2_, self_, A_);
            bool maybe_scalar = self_->dim() == 0 && A_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_gels not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::_th_symeig_out(Tensor & res1, Tensor & res2, const Tensor & self, bool eigenvectors, bool upper) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = checked_tensor_unwrap(res1,"res1",0, false, Backend::CUDA, ScalarType::Double);
            auto res2_ = checked_tensor_unwrap(res2,"res2",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_syev(globalContext().getTHCState(), res1_, res2_, self_, (eigenvectors) ? "V" : "N", (upper) ? "U" : "L");
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = checked_tensor_unwrap(res1,"res1",0, false, Backend::CUDA, ScalarType::Float);
            auto res2_ = checked_tensor_unwrap(res2,"res2",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_syev(globalContext().getTHCState(), res1_, res2_, self_, (eigenvectors) ? "V" : "N", (upper) ? "U" : "L");
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_symeig_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_th_symeig(const Tensor & self, bool eigenvectors, bool upper) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_syev(globalContext().getTHCState(), res1_, res2_, self_, (eigenvectors) ? "V" : "N", (upper) ? "U" : "L");
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_syev(globalContext().getTHCState(), res1_, res2_, self_, (eigenvectors) ? "V" : "N", (upper) ? "U" : "L");
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_symeig not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::_th_eig_out(Tensor & res1, Tensor & res2, const Tensor & self, bool eigenvectors) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = checked_tensor_unwrap(res1,"res1",0, false, Backend::CUDA, ScalarType::Double);
            auto res2_ = checked_tensor_unwrap(res2,"res2",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_geev(globalContext().getTHCState(), res1_, res2_, self_, (eigenvectors) ? "V" : "N");
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = checked_tensor_unwrap(res1,"res1",0, false, Backend::CUDA, ScalarType::Float);
            auto res2_ = checked_tensor_unwrap(res2,"res2",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_geev(globalContext().getTHCState(), res1_, res2_, self_, (eigenvectors) ? "V" : "N");
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_eig_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_th_eig(const Tensor & self, bool eigenvectors) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_geev(globalContext().getTHCState(), res1_, res2_, self_, (eigenvectors) ? "V" : "N");
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_geev(globalContext().getTHCState(), res1_, res2_, self_, (eigenvectors) ? "V" : "N");
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_eig not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAType::_th_svd_out(Tensor & res1, Tensor & res2, Tensor & res3, const Tensor & self, bool some, bool compute_uv) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = checked_tensor_unwrap(res1,"res1",0, false, Backend::CUDA, ScalarType::Double);
            auto res2_ = checked_tensor_unwrap(res2,"res2",0, false, Backend::CUDA, ScalarType::Double);
            auto res3_ = checked_tensor_unwrap(res3,"res3",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_gesdd(globalContext().getTHCState(), res1_, res2_, res3_, self_, (some) ? "S" : "A", (compute_uv) ? "S" : "N");
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            res3_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(res1, res2, res3);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = checked_tensor_unwrap(res1,"res1",0, false, Backend::CUDA, ScalarType::Float);
            auto res2_ = checked_tensor_unwrap(res2,"res2",0, false, Backend::CUDA, ScalarType::Float);
            auto res3_ = checked_tensor_unwrap(res3,"res3",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_gesdd(globalContext().getTHCState(), res1_, res2_, res3_, self_, (some) ? "S" : "A", (compute_uv) ? "S" : "N");
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            res3_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(res1, res2, res3);
            break;
        }
        default:
            AT_ERROR("_th_svd_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::_th_svd(const Tensor & self, bool some, bool compute_uv) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto res3_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto res3 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res3_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_gesdd(globalContext().getTHCState(), res1_, res2_, res3_, self_, (some) ? "S" : "A", (compute_uv) ? "S" : "N");
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            res3_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(res1, res2, res3);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto res3_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto res3 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res3_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_gesdd(globalContext().getTHCState(), res1_, res2_, res3_, self_, (some) ? "S" : "A", (compute_uv) ? "S" : "N");
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            res3_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(res1, res2, res3);
            break;
        }
        default:
            AT_ERROR("_th_svd not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_getri_single_out(Tensor & output, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_getri(globalContext().getTHCState(), output_, self_);
            output_->maybe_zero_dim(self_->dim() == 0);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_getri(globalContext().getTHCState(), output_, self_);
            output_->maybe_zero_dim(self_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_getri_single_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_getri_single(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_getri(globalContext().getTHCState(), output_, self_);
            output_->maybe_zero_dim(self_->dim() == 0);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_getri(globalContext().getTHCState(), output_, self_);
            output_->maybe_zero_dim(self_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_getri_single not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_potri_out(Tensor & output, const Tensor & self, bool upper) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_potri(globalContext().getTHCState(), output_, self_, (upper) ? "U" : "L");
            output_->maybe_zero_dim(self_->dim() == 0);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_potri(globalContext().getTHCState(), output_, self_, (upper) ? "U" : "L");
            output_->maybe_zero_dim(self_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_potri_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_potri(const Tensor & self, bool upper) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_potri(globalContext().getTHCState(), output_, self_, (upper) ? "U" : "L");
            output_->maybe_zero_dim(self_->dim() == 0);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_potri(globalContext().getTHCState(), output_, self_, (upper) ? "U" : "L");
            output_->maybe_zero_dim(self_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_potri not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::_th_qr_out(Tensor & res1, Tensor & res2, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = checked_tensor_unwrap(res1,"res1",0, false, Backend::CUDA, ScalarType::Double);
            auto res2_ = checked_tensor_unwrap(res2,"res2",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_qr(globalContext().getTHCState(), res1_, res2_, self_);
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = checked_tensor_unwrap(res1,"res1",0, false, Backend::CUDA, ScalarType::Float);
            auto res2_ = checked_tensor_unwrap(res2,"res2",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_qr(globalContext().getTHCState(), res1_, res2_, self_);
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_qr_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_th_qr(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_qr(globalContext().getTHCState(), res1_, res2_, self_);
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_qr(globalContext().getTHCState(), res1_, res2_, self_);
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_qr not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::_th_geqrf_out(Tensor & res1, Tensor & res2, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = checked_tensor_unwrap(res1,"res1",0, false, Backend::CUDA, ScalarType::Double);
            auto res2_ = checked_tensor_unwrap(res2,"res2",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_geqrf(globalContext().getTHCState(), res1_, res2_, self_);
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = checked_tensor_unwrap(res1,"res1",0, false, Backend::CUDA, ScalarType::Float);
            auto res2_ = checked_tensor_unwrap(res2,"res2",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_geqrf(globalContext().getTHCState(), res1_, res2_, self_);
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_geqrf_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_th_geqrf(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_geqrf(globalContext().getTHCState(), res1_, res2_, self_);
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_geqrf(globalContext().getTHCState(), res1_, res2_, self_);
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_geqrf not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_btrisolve_out(Tensor & result, const Tensor & self, const Tensor & LU_data, const Tensor & LU_pivots) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto LU_data_ = checked_tensor_unwrap(LU_data,"LU_data",2, false, Backend::CUDA, ScalarType::Double);
            auto LU_pivots_ = checked_tensor_unwrap(LU_pivots,"LU_pivots",3, false, Backend::CUDA, ScalarType::Int);
            THCudaDoubleTensor_btrisolve(globalContext().getTHCState(), result_, self_, LU_data_, LU_pivots_);
            result_->maybe_zero_dim(self_->dim() == 0 && LU_data_->dim() == 0 && LU_pivots_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto LU_data_ = checked_tensor_unwrap(LU_data,"LU_data",2, false, Backend::CUDA, ScalarType::Float);
            auto LU_pivots_ = checked_tensor_unwrap(LU_pivots,"LU_pivots",3, false, Backend::CUDA, ScalarType::Int);
            THCudaTensor_btrisolve(globalContext().getTHCState(), result_, self_, LU_data_, LU_pivots_);
            result_->maybe_zero_dim(self_->dim() == 0 && LU_data_->dim() == 0 && LU_pivots_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto LU_data_ = checked_tensor_unwrap(LU_data,"LU_data",2, false, Backend::CUDA, ScalarType::Half);
            auto LU_pivots_ = checked_tensor_unwrap(LU_pivots,"LU_pivots",3, false, Backend::CUDA, ScalarType::Int);
            THCudaHalfTensor_btrisolve(globalContext().getTHCState(), result_, self_, LU_data_, LU_pivots_);
            result_->maybe_zero_dim(self_->dim() == 0 && LU_data_->dim() == 0 && LU_pivots_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_btrisolve_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_btrisolve(const Tensor & self, const Tensor & LU_data, const Tensor & LU_pivots) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto LU_data_ = checked_tensor_unwrap(LU_data,"LU_data",2, false, Backend::CUDA, ScalarType::Double);
            auto LU_pivots_ = checked_tensor_unwrap(LU_pivots,"LU_pivots",3, false, Backend::CUDA, ScalarType::Int);
            THCudaDoubleTensor_btrisolve(globalContext().getTHCState(), result_, self_, LU_data_, LU_pivots_);
            result_->maybe_zero_dim(self_->dim() == 0 && LU_data_->dim() == 0 && LU_pivots_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto LU_data_ = checked_tensor_unwrap(LU_data,"LU_data",2, false, Backend::CUDA, ScalarType::Float);
            auto LU_pivots_ = checked_tensor_unwrap(LU_pivots,"LU_pivots",3, false, Backend::CUDA, ScalarType::Int);
            THCudaTensor_btrisolve(globalContext().getTHCState(), result_, self_, LU_data_, LU_pivots_);
            result_->maybe_zero_dim(self_->dim() == 0 && LU_data_->dim() == 0 && LU_pivots_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto LU_data_ = checked_tensor_unwrap(LU_data,"LU_data",2, false, Backend::CUDA, ScalarType::Half);
            auto LU_pivots_ = checked_tensor_unwrap(LU_pivots,"LU_pivots",3, false, Backend::CUDA, ScalarType::Int);
            THCudaHalfTensor_btrisolve(globalContext().getTHCState(), result_, self_, LU_data_, LU_pivots_);
            result_->maybe_zero_dim(self_->dim() == 0 && LU_data_->dim() == 0 && LU_pivots_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_btrisolve not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_random_(Tensor & self, int64_t from, int64_t to, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaBoolTensor_clampedRandom(globalContext().getTHCState(), self_, from, to);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaByteTensor_clampedRandom(globalContext().getTHCState(), self_, from, to);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaCharTensor_clampedRandom(globalContext().getTHCState(), self_, from, to);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaDoubleTensor_clampedRandom(globalContext().getTHCState(), self_, from, to);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaTensor_clampedRandom(globalContext().getTHCState(), self_, from, to);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaIntTensor_clampedRandom(globalContext().getTHCState(), self_, from, to);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaLongTensor_clampedRandom(globalContext().getTHCState(), self_, from, to);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaShortTensor_clampedRandom(globalContext().getTHCState(), self_, from, to);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaHalfTensor_clampedRandom(globalContext().getTHCState(), self_, from, to);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_random_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_random_(Tensor & self, int64_t to, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaBoolTensor_cappedRandom(globalContext().getTHCState(), self_, to);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaByteTensor_cappedRandom(globalContext().getTHCState(), self_, to);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaCharTensor_cappedRandom(globalContext().getTHCState(), self_, to);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaDoubleTensor_cappedRandom(globalContext().getTHCState(), self_, to);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaTensor_cappedRandom(globalContext().getTHCState(), self_, to);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaIntTensor_cappedRandom(globalContext().getTHCState(), self_, to);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaLongTensor_cappedRandom(globalContext().getTHCState(), self_, to);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaShortTensor_cappedRandom(globalContext().getTHCState(), self_, to);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaHalfTensor_cappedRandom(globalContext().getTHCState(), self_, to);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_random_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_random_(Tensor & self, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaBoolTensor_random(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaByteTensor_random(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaCharTensor_random(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaDoubleTensor_random(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaTensor_random(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaIntTensor_random(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaLongTensor_random(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaShortTensor_random(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaHalfTensor_random(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_random_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::_th_multinomial_alias_setup_out(Tensor & J, Tensor & q, const Tensor & probs) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(J);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto probs_ = checked_tensor_unwrap(probs,"probs",1, false, Backend::CUDA, ScalarType::Double);
            auto J_ = checked_tensor_unwrap(J,"J",1, false, Backend::CUDA, ScalarType::Long);
            auto q_ = checked_tensor_unwrap(q,"q",1, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_multinomialAliasSetup(globalContext().getTHCState(), probs_, J_, q_);
            bool maybe_scalar = probs_->dim() == 0;
            J_->maybe_zero_dim(maybe_scalar);
            q_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(J, q);
            break;
        }
        case ScalarType::Float: {
            auto probs_ = checked_tensor_unwrap(probs,"probs",1, false, Backend::CUDA, ScalarType::Float);
            auto J_ = checked_tensor_unwrap(J,"J",1, false, Backend::CUDA, ScalarType::Long);
            auto q_ = checked_tensor_unwrap(q,"q",1, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_multinomialAliasSetup(globalContext().getTHCState(), probs_, J_, q_);
            bool maybe_scalar = probs_->dim() == 0;
            J_->maybe_zero_dim(maybe_scalar);
            q_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(J, q);
            break;
        }
        case ScalarType::Half: {
            auto probs_ = checked_tensor_unwrap(probs,"probs",1, false, Backend::CUDA, ScalarType::Half);
            auto J_ = checked_tensor_unwrap(J,"J",1, false, Backend::CUDA, ScalarType::Long);
            auto q_ = checked_tensor_unwrap(q,"q",1, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_multinomialAliasSetup(globalContext().getTHCState(), probs_, J_, q_);
            bool maybe_scalar = probs_->dim() == 0;
            J_->maybe_zero_dim(maybe_scalar);
            q_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(J, q);
            break;
        }
        default:
            AT_ERROR("_th_multinomial_alias_setup_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_th_multinomial_alias_setup(const Tensor & probs) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(probs);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto probs_ = checked_tensor_unwrap(probs,"probs",1, false, Backend::CUDA, ScalarType::Double);
            auto J_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto J = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(J_));
            auto q_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto q = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(q_));
            THCudaDoubleTensor_multinomialAliasSetup(globalContext().getTHCState(), probs_, J_, q_);
            bool maybe_scalar = probs_->dim() == 0;
            J_->maybe_zero_dim(maybe_scalar);
            q_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(J, q);
            break;
        }
        case ScalarType::Float: {
            auto probs_ = checked_tensor_unwrap(probs,"probs",1, false, Backend::CUDA, ScalarType::Float);
            auto J_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto J = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(J_));
            auto q_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto q = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(q_));
            THCudaTensor_multinomialAliasSetup(globalContext().getTHCState(), probs_, J_, q_);
            bool maybe_scalar = probs_->dim() == 0;
            J_->maybe_zero_dim(maybe_scalar);
            q_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(J, q);
            break;
        }
        case ScalarType::Half: {
            auto probs_ = checked_tensor_unwrap(probs,"probs",1, false, Backend::CUDA, ScalarType::Half);
            auto J_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto J = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(J_));
            auto q_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto q = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(q_));
            THCudaHalfTensor_multinomialAliasSetup(globalContext().getTHCState(), probs_, J_, q_);
            bool maybe_scalar = probs_->dim() == 0;
            J_->maybe_zero_dim(maybe_scalar);
            q_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(J, q);
            break;
        }
        default:
            AT_ERROR("_th_multinomial_alias_setup not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_multinomial_alias_draw_out(Tensor & result, const Tensor & q, const Tensor & J, int64_t num_samples, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(result);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto q_ = checked_tensor_unwrap(q,"q",2, false, Backend::CUDA, ScalarType::Double);
            auto J_ = checked_tensor_unwrap(J,"J",3, false, Backend::CUDA, ScalarType::Long);
            THCudaDoubleTensor_multinomialAliasDraw(globalContext().getTHCState(), result_, q_, J_, num_samples);
            result_->maybe_zero_dim(q_->dim() == 0 && J_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto q_ = checked_tensor_unwrap(q,"q",2, false, Backend::CUDA, ScalarType::Float);
            auto J_ = checked_tensor_unwrap(J,"J",3, false, Backend::CUDA, ScalarType::Long);
            THCudaTensor_multinomialAliasDraw(globalContext().getTHCState(), result_, q_, J_, num_samples);
            result_->maybe_zero_dim(q_->dim() == 0 && J_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto q_ = checked_tensor_unwrap(q,"q",2, false, Backend::CUDA, ScalarType::Half);
            auto J_ = checked_tensor_unwrap(J,"J",3, false, Backend::CUDA, ScalarType::Long);
            THCudaHalfTensor_multinomialAliasDraw(globalContext().getTHCState(), result_, q_, J_, num_samples);
            result_->maybe_zero_dim(q_->dim() == 0 && J_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_multinomial_alias_draw_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_multinomial_alias_draw(const Tensor & q, const Tensor & J, int64_t num_samples, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(q);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto q_ = checked_tensor_unwrap(q,"q",2, false, Backend::CUDA, ScalarType::Double);
            auto J_ = checked_tensor_unwrap(J,"J",3, false, Backend::CUDA, ScalarType::Long);
            THCudaDoubleTensor_multinomialAliasDraw(globalContext().getTHCState(), result_, q_, J_, num_samples);
            result_->maybe_zero_dim(q_->dim() == 0 && J_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto q_ = checked_tensor_unwrap(q,"q",2, false, Backend::CUDA, ScalarType::Float);
            auto J_ = checked_tensor_unwrap(J,"J",3, false, Backend::CUDA, ScalarType::Long);
            THCudaTensor_multinomialAliasDraw(globalContext().getTHCState(), result_, q_, J_, num_samples);
            result_->maybe_zero_dim(q_->dim() == 0 && J_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto q_ = checked_tensor_unwrap(q,"q",2, false, Backend::CUDA, ScalarType::Half);
            auto J_ = checked_tensor_unwrap(J,"J",3, false, Backend::CUDA, ScalarType::Long);
            THCudaHalfTensor_multinomialAliasDraw(globalContext().getTHCState(), result_, q_, J_, num_samples);
            result_->maybe_zero_dim(q_->dim() == 0 && J_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_multinomial_alias_draw not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_multinomial_out(Tensor & result, const Tensor & self, int64_t num_samples, bool replacement, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_multinomial(globalContext().getTHCState(), result_, self_, num_samples, replacement);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_multinomial(globalContext().getTHCState(), result_, self_, num_samples, replacement);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_multinomial(globalContext().getTHCState(), result_, self_, num_samples, replacement);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_multinomial_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_multinomial(const Tensor & self, int64_t num_samples, bool replacement, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_multinomial(globalContext().getTHCState(), result_, self_, num_samples, replacement);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_multinomial(globalContext().getTHCState(), result_, self_, num_samples, replacement);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_multinomial(globalContext().getTHCState(), result_, self_, num_samples, replacement);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_multinomial not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_normal_out(Tensor & output, const Tensor & mean, double std, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(output);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CUDA, ScalarType::Double);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto mean_ = checked_tensor_unwrap(mean,"mean",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_normal_means(globalContext().getTHCState(), output_, mean_, std);
            output_->maybe_zero_dim(mean_->dim() == 0);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CUDA, ScalarType::Float);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto mean_ = checked_tensor_unwrap(mean,"mean",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_normal_means(globalContext().getTHCState(), output_, mean_, std);
            output_->maybe_zero_dim(mean_->dim() == 0);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CUDA, ScalarType::Half);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto mean_ = checked_tensor_unwrap(mean,"mean",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_normal_means(globalContext().getTHCState(), output_, mean_, std);
            output_->maybe_zero_dim(mean_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_normal_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_normal(const Tensor & mean, double std, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(mean);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto mean_ = checked_tensor_unwrap(mean,"mean",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_normal_means(globalContext().getTHCState(), output_, mean_, std);
            output_->maybe_zero_dim(mean_->dim() == 0);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto mean_ = checked_tensor_unwrap(mean,"mean",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_normal_means(globalContext().getTHCState(), output_, mean_, std);
            output_->maybe_zero_dim(mean_->dim() == 0);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto mean_ = checked_tensor_unwrap(mean,"mean",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_normal_means(globalContext().getTHCState(), output_, mean_, std);
            output_->maybe_zero_dim(mean_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_normal not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_normal_out(Tensor & output, double mean, const Tensor & std, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(output);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CUDA, ScalarType::Double);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto std_ = checked_tensor_unwrap(std,"std",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_normal_stddevs(globalContext().getTHCState(), output_, mean, std_);
            output_->maybe_zero_dim(std_->dim() == 0);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CUDA, ScalarType::Float);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto std_ = checked_tensor_unwrap(std,"std",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_normal_stddevs(globalContext().getTHCState(), output_, mean, std_);
            output_->maybe_zero_dim(std_->dim() == 0);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CUDA, ScalarType::Half);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto std_ = checked_tensor_unwrap(std,"std",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_normal_stddevs(globalContext().getTHCState(), output_, mean, std_);
            output_->maybe_zero_dim(std_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_normal_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_normal(double mean, const Tensor & std, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(std);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto std_ = checked_tensor_unwrap(std,"std",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_normal_stddevs(globalContext().getTHCState(), output_, mean, std_);
            output_->maybe_zero_dim(std_->dim() == 0);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto std_ = checked_tensor_unwrap(std,"std",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_normal_stddevs(globalContext().getTHCState(), output_, mean, std_);
            output_->maybe_zero_dim(std_->dim() == 0);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto std_ = checked_tensor_unwrap(std,"std",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_normal_stddevs(globalContext().getTHCState(), output_, mean, std_);
            output_->maybe_zero_dim(std_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_normal not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_normal_out(Tensor & output, const Tensor & mean, const Tensor & std, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(output);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CUDA, ScalarType::Double);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto mean_ = checked_tensor_unwrap(mean,"mean",2, false, Backend::CUDA, ScalarType::Double);
            auto std_ = checked_tensor_unwrap(std,"std",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_normal_means_stddevs(globalContext().getTHCState(), output_, mean_, std_);
            output_->maybe_zero_dim(mean_->dim() == 0 && std_->dim() == 0);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CUDA, ScalarType::Float);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto mean_ = checked_tensor_unwrap(mean,"mean",2, false, Backend::CUDA, ScalarType::Float);
            auto std_ = checked_tensor_unwrap(std,"std",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_normal_means_stddevs(globalContext().getTHCState(), output_, mean_, std_);
            output_->maybe_zero_dim(mean_->dim() == 0 && std_->dim() == 0);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CUDA, ScalarType::Half);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto mean_ = checked_tensor_unwrap(mean,"mean",2, false, Backend::CUDA, ScalarType::Half);
            auto std_ = checked_tensor_unwrap(std,"std",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_normal_means_stddevs(globalContext().getTHCState(), output_, mean_, std_);
            output_->maybe_zero_dim(mean_->dim() == 0 && std_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_normal_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_normal(const Tensor & mean, const Tensor & std, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(mean);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto mean_ = checked_tensor_unwrap(mean,"mean",2, false, Backend::CUDA, ScalarType::Double);
            auto std_ = checked_tensor_unwrap(std,"std",3, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_normal_means_stddevs(globalContext().getTHCState(), output_, mean_, std_);
            output_->maybe_zero_dim(mean_->dim() == 0 && std_->dim() == 0);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto mean_ = checked_tensor_unwrap(mean,"mean",2, false, Backend::CUDA, ScalarType::Float);
            auto std_ = checked_tensor_unwrap(std,"std",3, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_normal_means_stddevs(globalContext().getTHCState(), output_, mean_, std_);
            output_->maybe_zero_dim(mean_->dim() == 0 && std_->dim() == 0);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto mean_ = checked_tensor_unwrap(mean,"mean",2, false, Backend::CUDA, ScalarType::Half);
            auto std_ = checked_tensor_unwrap(std,"std",3, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_normal_means_stddevs(globalContext().getTHCState(), output_, mean_, std_);
            output_->maybe_zero_dim(mean_->dim() == 0 && std_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_normal not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_normal_(Tensor & self, double mean, double std, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaDoubleTensor_normal(globalContext().getTHCState(), self_, mean, std);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaTensor_normal(globalContext().getTHCState(), self_, mean, std);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaHalfTensor_normal(globalContext().getTHCState(), self_, mean, std);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_normal_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_cauchy_(Tensor & self, double median, double sigma, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaDoubleTensor_cauchy(globalContext().getTHCState(), self_, median, sigma);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaTensor_cauchy(globalContext().getTHCState(), self_, median, sigma);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaHalfTensor_cauchy(globalContext().getTHCState(), self_, median, sigma);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_cauchy_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_log_normal_(Tensor & self, double mean, double std, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaDoubleTensor_logNormal(globalContext().getTHCState(), self_, mean, std);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaTensor_logNormal(globalContext().getTHCState(), self_, mean, std);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaHalfTensor_logNormal(globalContext().getTHCState(), self_, mean, std);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_log_normal_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_exponential_(Tensor & self, double lambd, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaDoubleTensor_exponential(globalContext().getTHCState(), self_, lambd);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaTensor_exponential(globalContext().getTHCState(), self_, lambd);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaHalfTensor_exponential(globalContext().getTHCState(), self_, lambd);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_exponential_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_geometric_(Tensor & self, double p, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaByteTensor_geometric(globalContext().getTHCState(), self_, p);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaCharTensor_geometric(globalContext().getTHCState(), self_, p);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaDoubleTensor_geometric(globalContext().getTHCState(), self_, p);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaTensor_geometric(globalContext().getTHCState(), self_, p);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaIntTensor_geometric(globalContext().getTHCState(), self_, p);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaLongTensor_geometric(globalContext().getTHCState(), self_, p);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaShortTensor_geometric(globalContext().getTHCState(), self_, p);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THCudaHalfTensor_geometric(globalContext().getTHCState(), self_, p);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_geometric_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_alias(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Bool);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaBoolTensor_newWithTensor(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaByteTensor_newWithTensor(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaCharTensor_newWithTensor(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaDoubleTensor_newWithTensor(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaTensor_newWithTensor(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaIntTensor_newWithTensor(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaLongTensor_newWithTensor(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaShortTensor_newWithTensor(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaHalfTensor_newWithTensor(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        default:
            AT_ERROR("_th_alias not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_copy_ignoring_overlaps_(Tensor & self, const Tensor & src) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Byte);
            auto src_ = checked_tensor_unwrap(src,"src",2, false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_copyIgnoringOverlaps(globalContext().getTHCState(), self_, src_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Char);
            auto src_ = checked_tensor_unwrap(src,"src",2, false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_copyIgnoringOverlaps(globalContext().getTHCState(), self_, src_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto src_ = checked_tensor_unwrap(src,"src",2, false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_copyIgnoringOverlaps(globalContext().getTHCState(), self_, src_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto src_ = checked_tensor_unwrap(src,"src",2, false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_copyIgnoringOverlaps(globalContext().getTHCState(), self_, src_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Int);
            auto src_ = checked_tensor_unwrap(src,"src",2, false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_copyIgnoringOverlaps(globalContext().getTHCState(), self_, src_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src,"src",2, false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_copyIgnoringOverlaps(globalContext().getTHCState(), self_, src_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Short);
            auto src_ = checked_tensor_unwrap(src,"src",2, false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_copyIgnoringOverlaps(globalContext().getTHCState(), self_, src_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto src_ = checked_tensor_unwrap(src,"src",2, false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_copyIgnoringOverlaps(globalContext().getTHCState(), self_, src_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_copy_ignoring_overlaps_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_th_cat_out(Tensor & self, TensorList tensors, int64_t dim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self,"self",0, false, Backend::CUDA, ScalarType::Bool);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self,"self",0, false, Backend::CUDA, ScalarType::Byte);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self,"self",0, false, Backend::CUDA, ScalarType::Char);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",0, false, Backend::CUDA, ScalarType::Double);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",0, false, Backend::CUDA, ScalarType::Float);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Float);
            THCudaTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self,"self",0, false, Backend::CUDA, ScalarType::Int);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",0, false, Backend::CUDA, ScalarType::Long);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self,"self",0, false, Backend::CUDA, ScalarType::Short);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",0, false, Backend::CUDA, ScalarType::Half);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_cat_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_th_cat(TensorList tensors, int64_t dim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(tensors);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),CUDATensorId()).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Float);
            THCudaTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),CUDATensorId()).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),CUDATensorId()).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_cat not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_binary_cross_entropy_forward_out(Tensor & output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output,"output",4, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleBCECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",4, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaBCECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output,"output",4, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfBCECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_binary_cross_entropy_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_binary_cross_entropy_forward(const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleBCECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaBCECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfBCECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_binary_cross_entropy_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_binary_cross_entropy_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleBCECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaBCECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfBCECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_binary_cross_entropy_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_binary_cross_entropy_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleBCECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaBCECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfBCECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_binary_cross_entropy_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_l1_loss_forward_out(Tensor & output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleAbsCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaAbsCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfAbsCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_l1_loss_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_l1_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleAbsCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaAbsCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfAbsCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_l1_loss_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_l1_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleAbsCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaAbsCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfAbsCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_l1_loss_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_l1_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleAbsCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaAbsCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfAbsCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_l1_loss_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_mse_loss_forward_out(Tensor & output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleMSECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaMSECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfMSECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_mse_loss_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_mse_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleMSECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaMSECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfMSECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_mse_loss_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_mse_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleMSECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaMSECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfMSECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_mse_loss_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_mse_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleMSECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaMSECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfMSECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_mse_loss_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_multi_margin_loss_forward_out(Tensor & output, const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight,"weight",5, true, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleMultiMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0 || (reduction == Reduction::None && self_->dim() == 1));
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight,"weight",5, true, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaMultiMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0 || (reduction == Reduction::None && self_->dim() == 1));
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight,"weight",5, true, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfMultiMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0 || (reduction == Reduction::None && self_->dim() == 1));
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_multi_margin_loss_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_multi_margin_loss_forward(const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight,"weight",5, true, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleMultiMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0 || (reduction == Reduction::None && self_->dim() == 1));
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight,"weight",5, true, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaMultiMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0 || (reduction == Reduction::None && self_->dim() == 1));
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight,"weight",5, true, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfMultiMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0 || (reduction == Reduction::None && self_->dim() == 1));
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_multi_margin_loss_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_multi_margin_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight,"weight",6, true, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleMultiMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight,"weight",6, true, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaMultiMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight,"weight",6, true, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfMultiMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_multi_margin_loss_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_multi_margin_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight,"weight",6, true, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleMultiMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight,"weight",6, true, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaMultiMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight,"weight",6, true, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfMultiMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_multi_margin_loss_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::_thnn_multilabel_margin_loss_forward_out(Tensor & output, Tensor & is_target, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Double);
            auto is_target_ = checked_tensor_unwrap(is_target,"is_target",3, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleMultiLabelMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, is_target_, reduction);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            is_target_->maybe_zero_dim(target_->dim() == 0);
            return std::tuple<Tensor &, Tensor &>(output, is_target);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Float);
            auto is_target_ = checked_tensor_unwrap(is_target,"is_target",3, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaMultiLabelMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, is_target_, reduction);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            is_target_->maybe_zero_dim(target_->dim() == 0);
            return std::tuple<Tensor &, Tensor &>(output, is_target);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
            auto is_target_ = checked_tensor_unwrap(is_target,"is_target",3, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfMultiLabelMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, is_target_, reduction);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            is_target_->maybe_zero_dim(target_->dim() == 0);
            return std::tuple<Tensor &, Tensor &>(output, is_target);
            break;
        }
        default:
            AT_ERROR("_thnn_multilabel_margin_loss_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_thnn_multilabel_margin_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto is_target_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto is_target = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(is_target_));
            THNN_CudaDoubleMultiLabelMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, is_target_, reduction);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            is_target_->maybe_zero_dim(target_->dim() == 0);
            return std::tuple<Tensor, Tensor>(output, is_target);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto is_target_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto is_target = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(is_target_));
            THNN_CudaMultiLabelMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, is_target_, reduction);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            is_target_->maybe_zero_dim(target_->dim() == 0);
            return std::tuple<Tensor, Tensor>(output, is_target);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto is_target_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto is_target = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(is_target_));
            THNN_CudaHalfMultiLabelMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, is_target_, reduction);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            is_target_->maybe_zero_dim(target_->dim() == 0);
            return std::tuple<Tensor, Tensor>(output, is_target);
            break;
        }
        default:
            AT_ERROR("_thnn_multilabel_margin_loss_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_multilabel_margin_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction, const Tensor & is_target) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto is_target_ = checked_tensor_unwrap(is_target,"is_target",5, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleMultiLabelMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, is_target_, reduction);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto is_target_ = checked_tensor_unwrap(is_target,"is_target",5, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaMultiLabelMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, is_target_, reduction);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto is_target_ = checked_tensor_unwrap(is_target,"is_target",5, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfMultiLabelMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, is_target_, reduction);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_multilabel_margin_loss_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_multilabel_margin_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction, const Tensor & is_target) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto is_target_ = checked_tensor_unwrap(is_target,"is_target",5, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleMultiLabelMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, is_target_, reduction);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto is_target_ = checked_tensor_unwrap(is_target,"is_target",5, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaMultiLabelMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, is_target_, reduction);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto is_target_ = checked_tensor_unwrap(is_target,"is_target",5, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfMultiLabelMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, is_target_, reduction);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_multilabel_margin_loss_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::_thnn_nll_loss_forward_out(Tensor & output, Tensor & total_weight, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Double);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",5, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor &, Tensor &>(output, total_weight);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Float);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",5, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor &, Tensor &>(output, total_weight);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Half);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",5, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor &, Tensor &>(output, total_weight);
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_thnn_nll_loss_forward(const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto total_weight_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto total_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(total_weight_));
            THNN_CudaDoubleClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor, Tensor>(output, total_weight);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto total_weight_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto total_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(total_weight_));
            THNN_CudaClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor, Tensor>(output, total_weight);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto total_weight_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto total_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(total_weight_));
            THNN_CudaHalfClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor, Tensor>(output, total_weight);
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_nll_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Double);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",7, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Float);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",7, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Half);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",7, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_nll_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Double);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",7, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Float);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",7, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Half);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",7, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::_thnn_nll_loss2d_forward_out(Tensor & output, Tensor & total_weight, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Double);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",5, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSpatialClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor &, Tensor &>(output, total_weight);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Float);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",5, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSpatialClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor &, Tensor &>(output, total_weight);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Half);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",5, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSpatialClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor &, Tensor &>(output, total_weight);
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss2d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_thnn_nll_loss2d_forward(const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto total_weight_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto total_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(total_weight_));
            THNN_CudaDoubleSpatialClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor, Tensor>(output, total_weight);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto total_weight_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto total_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(total_weight_));
            THNN_CudaSpatialClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor, Tensor>(output, total_weight);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto total_weight_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto total_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(total_weight_));
            THNN_CudaHalfSpatialClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor, Tensor>(output, total_weight);
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss2d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_nll_loss2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Double);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",7, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSpatialClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Float);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",7, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSpatialClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Half);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",7, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSpatialClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss2d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_nll_loss2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Double);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",7, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleSpatialClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Float);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",7, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaSpatialClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Half);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",7, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfSpatialClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_smooth_l1_loss_forward_out(Tensor & output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSmoothL1Criterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSmoothL1Criterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSmoothL1Criterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_smooth_l1_loss_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_smooth_l1_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleSmoothL1Criterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaSmoothL1Criterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfSmoothL1Criterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_smooth_l1_loss_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_smooth_l1_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSmoothL1Criterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSmoothL1Criterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSmoothL1Criterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_smooth_l1_loss_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_smooth_l1_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleSmoothL1Criterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaSmoothL1Criterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfSmoothL1Criterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_smooth_l1_loss_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_soft_margin_loss_forward_out(Tensor & output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSoftMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSoftMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSoftMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_soft_margin_loss_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_soft_margin_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleSoftMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaSoftMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfSoftMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_soft_margin_loss_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_soft_margin_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSoftMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSoftMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSoftMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_soft_margin_loss_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_soft_margin_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleSoftMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaSoftMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfSoftMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_soft_margin_loss_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_elu_forward_out(Tensor & output, const Tensor & self, Scalar alpha, Scalar scale, Scalar input_scale) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",4, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleELU_updateOutput(globalContext().getTHCState(), self_, output_, alpha_, scale_, input_scale_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",4, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaELU_updateOutput(globalContext().getTHCState(), self_, output_, alpha_, scale_, input_scale_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",4, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfELU_updateOutput(globalContext().getTHCState(), self_, output_, alpha_, scale_, input_scale_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_elu_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_elu_forward(const Tensor & self, Scalar alpha, Scalar scale, Scalar input_scale) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleELU_updateOutput(globalContext().getTHCState(), self_, output_, alpha_, scale_, input_scale_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaELU_updateOutput(globalContext().getTHCState(), self_, output_, alpha_, scale_, input_scale_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfELU_updateOutput(globalContext().getTHCState(), self_, output_, alpha_, scale_, input_scale_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_elu_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_elu_backward_out(Tensor & grad_input, const Tensor & grad_output, Scalar alpha, Scalar scale, Scalar input_scale, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleELU_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_, alpha_, scale_, input_scale_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaELU_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_, alpha_, scale_, input_scale_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfELU_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_, alpha_, scale_, input_scale_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_elu_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_elu_backward(const Tensor & grad_output, Scalar alpha, Scalar scale, Scalar input_scale, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleELU_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_, alpha_, scale_, input_scale_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaELU_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_, alpha_, scale_, input_scale_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfELU_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_, alpha_, scale_, input_scale_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_elu_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_elu_forward_(Tensor & self, Scalar alpha, Scalar scale, Scalar input_scale) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            THNN_CudaDoubleELU_updateOutput(globalContext().getTHCState(), self_, self_, alpha_, scale_, input_scale_, true);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            THNN_CudaELU_updateOutput(globalContext().getTHCState(), self_, self_, alpha_, scale_, input_scale_, true);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            THNN_CudaHalfELU_updateOutput(globalContext().getTHCState(), self_, self_, alpha_, scale_, input_scale_, true);
            return self;
            break;
        }
        default:
            AT_ERROR("_thnn_elu_forward_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_glu_forward_out(Tensor & output, const Tensor & self, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleGatedLinear_updateOutput(globalContext().getTHCState(), self_, output_, dim);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaGatedLinear_updateOutput(globalContext().getTHCState(), self_, output_, dim);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfGatedLinear_updateOutput(globalContext().getTHCState(), self_, output_, dim);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_glu_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_glu_forward(const Tensor & self, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleGatedLinear_updateOutput(globalContext().getTHCState(), self_, output_, dim);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaGatedLinear_updateOutput(globalContext().getTHCState(), self_, output_, dim);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfGatedLinear_updateOutput(globalContext().getTHCState(), self_, output_, dim);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_glu_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_glu_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleGatedLinear_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, dim);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaGatedLinear_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, dim);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfGatedLinear_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, dim);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_glu_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_glu_backward(const Tensor & grad_output, const Tensor & self, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleGatedLinear_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, dim);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaGatedLinear_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, dim);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfGatedLinear_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, dim);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_glu_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_hardtanh_forward_out(Tensor & output, const Tensor & self, Scalar min_val, Scalar max_val) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleHardTanh_updateOutput(globalContext().getTHCState(), self_, output_, min_val_, max_val_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaHardTanh_updateOutput(globalContext().getTHCState(), self_, output_, min_val_, max_val_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfHardTanh_updateOutput(globalContext().getTHCState(), self_, output_, min_val_, max_val_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_hardtanh_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_hardtanh_forward(const Tensor & self, Scalar min_val, Scalar max_val) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleHardTanh_updateOutput(globalContext().getTHCState(), self_, output_, min_val_, max_val_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHardTanh_updateOutput(globalContext().getTHCState(), self_, output_, min_val_, max_val_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfHardTanh_updateOutput(globalContext().getTHCState(), self_, output_, min_val_, max_val_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_hardtanh_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_hardtanh_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar min_val, Scalar max_val) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleHardTanh_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, min_val_, max_val_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaHardTanh_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, min_val_, max_val_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfHardTanh_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, min_val_, max_val_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_hardtanh_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_hardtanh_backward(const Tensor & grad_output, const Tensor & self, Scalar min_val, Scalar max_val) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleHardTanh_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, min_val_, max_val_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHardTanh_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, min_val_, max_val_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfHardTanh_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, min_val_, max_val_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_hardtanh_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_hardtanh_forward_(Tensor & self, Scalar min_val, Scalar max_val) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            THNN_CudaDoubleHardTanh_updateOutput(globalContext().getTHCState(), self_, self_, min_val_, max_val_, true);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            THNN_CudaHardTanh_updateOutput(globalContext().getTHCState(), self_, self_, min_val_, max_val_, true);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            THNN_CudaHalfHardTanh_updateOutput(globalContext().getTHCState(), self_, self_, min_val_, max_val_, true);
            return self;
            break;
        }
        default:
            AT_ERROR("_thnn_hardtanh_forward_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_leaky_relu_forward_out(Tensor & output, const Tensor & self, Scalar negative_slope) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto negative_slope_ = negative_slope.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleLeakyReLU_updateOutput(globalContext().getTHCState(), self_, output_, negative_slope_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto negative_slope_ = negative_slope.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaLeakyReLU_updateOutput(globalContext().getTHCState(), self_, output_, negative_slope_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto negative_slope_ = negative_slope.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfLeakyReLU_updateOutput(globalContext().getTHCState(), self_, output_, negative_slope_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_leaky_relu_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_leaky_relu_forward(const Tensor & self, Scalar negative_slope) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto negative_slope_ = negative_slope.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleLeakyReLU_updateOutput(globalContext().getTHCState(), self_, output_, negative_slope_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto negative_slope_ = negative_slope.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaLeakyReLU_updateOutput(globalContext().getTHCState(), self_, output_, negative_slope_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto negative_slope_ = negative_slope.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfLeakyReLU_updateOutput(globalContext().getTHCState(), self_, output_, negative_slope_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_leaky_relu_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_leaky_relu_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar negative_slope) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto negative_slope_ = negative_slope.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleLeakyReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, negative_slope_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto negative_slope_ = negative_slope.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaLeakyReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, negative_slope_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto negative_slope_ = negative_slope.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfLeakyReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, negative_slope_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_leaky_relu_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_leaky_relu_backward(const Tensor & grad_output, const Tensor & self, Scalar negative_slope) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto negative_slope_ = negative_slope.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleLeakyReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, negative_slope_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto negative_slope_ = negative_slope.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaLeakyReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, negative_slope_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto negative_slope_ = negative_slope.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfLeakyReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, negative_slope_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_leaky_relu_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_leaky_relu_forward_(Tensor & self, Scalar negative_slope) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto negative_slope_ = negative_slope.toDouble();
            THNN_CudaDoubleLeakyReLU_updateOutput(globalContext().getTHCState(), self_, self_, negative_slope_, true);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto negative_slope_ = negative_slope.toDouble();
            THNN_CudaLeakyReLU_updateOutput(globalContext().getTHCState(), self_, self_, negative_slope_, true);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto negative_slope_ = negative_slope.toDouble();
            THNN_CudaHalfLeakyReLU_updateOutput(globalContext().getTHCState(), self_, self_, negative_slope_, true);
            return self;
            break;
        }
        default:
            AT_ERROR("_thnn_leaky_relu_forward_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::_thnn_log_sigmoid_forward_out(Tensor & output, Tensor & buffer, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output,"output",1, false, Backend::CUDA, ScalarType::Double);
            auto buffer_ = checked_tensor_unwrap(buffer,"buffer",1, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleLogSigmoid_updateOutput(globalContext().getTHCState(), self_, output_, buffer_);
            output_->maybe_zero_dim(false);
            buffer_->maybe_zero_dim(false);
            return std::tuple<Tensor &, Tensor &>(output, buffer);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",1, false, Backend::CUDA, ScalarType::Float);
            auto buffer_ = checked_tensor_unwrap(buffer,"buffer",1, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaLogSigmoid_updateOutput(globalContext().getTHCState(), self_, output_, buffer_);
            output_->maybe_zero_dim(false);
            buffer_->maybe_zero_dim(false);
            return std::tuple<Tensor &, Tensor &>(output, buffer);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output,"output",1, false, Backend::CUDA, ScalarType::Half);
            auto buffer_ = checked_tensor_unwrap(buffer,"buffer",1, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfLogSigmoid_updateOutput(globalContext().getTHCState(), self_, output_, buffer_);
            output_->maybe_zero_dim(false);
            buffer_->maybe_zero_dim(false);
            return std::tuple<Tensor &, Tensor &>(output, buffer);
            break;
        }
        default:
            AT_ERROR("_thnn_log_sigmoid_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_thnn_log_sigmoid_forward(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto buffer_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto buffer = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(buffer_));
            THNN_CudaDoubleLogSigmoid_updateOutput(globalContext().getTHCState(), self_, output_, buffer_);
            output_->maybe_zero_dim(false);
            buffer_->maybe_zero_dim(false);
            return std::tuple<Tensor, Tensor>(output, buffer);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto buffer_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto buffer = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(buffer_));
            THNN_CudaLogSigmoid_updateOutput(globalContext().getTHCState(), self_, output_, buffer_);
            output_->maybe_zero_dim(false);
            buffer_->maybe_zero_dim(false);
            return std::tuple<Tensor, Tensor>(output, buffer);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto buffer_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto buffer = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(buffer_));
            THNN_CudaHalfLogSigmoid_updateOutput(globalContext().getTHCState(), self_, output_, buffer_);
            output_->maybe_zero_dim(false);
            buffer_->maybe_zero_dim(false);
            return std::tuple<Tensor, Tensor>(output, buffer);
            break;
        }
        default:
            AT_ERROR("_thnn_log_sigmoid_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_log_sigmoid_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & buffer) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto buffer_ = checked_tensor_unwrap(buffer,"buffer",3, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleLogSigmoid_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, buffer_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto buffer_ = checked_tensor_unwrap(buffer,"buffer",3, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaLogSigmoid_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, buffer_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto buffer_ = checked_tensor_unwrap(buffer,"buffer",3, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfLogSigmoid_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, buffer_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_log_sigmoid_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_log_sigmoid_backward(const Tensor & grad_output, const Tensor & self, const Tensor & buffer) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto buffer_ = checked_tensor_unwrap(buffer,"buffer",3, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleLogSigmoid_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, buffer_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto buffer_ = checked_tensor_unwrap(buffer,"buffer",3, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaLogSigmoid_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, buffer_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto buffer_ = checked_tensor_unwrap(buffer,"buffer",3, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfLogSigmoid_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, buffer_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_log_sigmoid_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_rrelu_with_noise_forward_out(Tensor & output, const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto noise_ = checked_tensor_unwrap(noise,"noise",2, false, Backend::CUDA, ScalarType::Double);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleRReLU_updateOutput(globalContext().getTHCState(), self_, output_, noise_, lower_, upper_, training, false, NULL);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto noise_ = checked_tensor_unwrap(noise,"noise",2, false, Backend::CUDA, ScalarType::Float);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaRReLU_updateOutput(globalContext().getTHCState(), self_, output_, noise_, lower_, upper_, training, false, NULL);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto noise_ = checked_tensor_unwrap(noise,"noise",2, false, Backend::CUDA, ScalarType::Half);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfRReLU_updateOutput(globalContext().getTHCState(), self_, output_, noise_, lower_, upper_, training, false, NULL);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_rrelu_with_noise_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_rrelu_with_noise_forward(const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto noise_ = checked_tensor_unwrap(noise,"noise",2, false, Backend::CUDA, ScalarType::Double);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleRReLU_updateOutput(globalContext().getTHCState(), self_, output_, noise_, lower_, upper_, training, false, NULL);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto noise_ = checked_tensor_unwrap(noise,"noise",2, false, Backend::CUDA, ScalarType::Float);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaRReLU_updateOutput(globalContext().getTHCState(), self_, output_, noise_, lower_, upper_, training, false, NULL);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto noise_ = checked_tensor_unwrap(noise,"noise",2, false, Backend::CUDA, ScalarType::Half);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfRReLU_updateOutput(globalContext().getTHCState(), self_, output_, noise_, lower_, upper_, training, false, NULL);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_rrelu_with_noise_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_rrelu_with_noise_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto noise_ = checked_tensor_unwrap(noise,"noise",3, false, Backend::CUDA, ScalarType::Double);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",6, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleRReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, noise_, lower_, upper_, training, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto noise_ = checked_tensor_unwrap(noise,"noise",3, false, Backend::CUDA, ScalarType::Float);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",6, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaRReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, noise_, lower_, upper_, training, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto noise_ = checked_tensor_unwrap(noise,"noise",3, false, Backend::CUDA, ScalarType::Half);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",6, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfRReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, noise_, lower_, upper_, training, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_rrelu_with_noise_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_rrelu_with_noise_backward(const Tensor & grad_output, const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto noise_ = checked_tensor_unwrap(noise,"noise",3, false, Backend::CUDA, ScalarType::Double);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleRReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, noise_, lower_, upper_, training, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto noise_ = checked_tensor_unwrap(noise,"noise",3, false, Backend::CUDA, ScalarType::Float);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaRReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, noise_, lower_, upper_, training, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto noise_ = checked_tensor_unwrap(noise,"noise",3, false, Backend::CUDA, ScalarType::Half);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfRReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, noise_, lower_, upper_, training, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_rrelu_with_noise_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_rrelu_with_noise_forward_(Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto noise_ = checked_tensor_unwrap(noise,"noise",2, false, Backend::CUDA, ScalarType::Double);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THNN_CudaDoubleRReLU_updateOutput(globalContext().getTHCState(), self_, self_, noise_, lower_, upper_, training, true, NULL);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto noise_ = checked_tensor_unwrap(noise,"noise",2, false, Backend::CUDA, ScalarType::Float);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THNN_CudaRReLU_updateOutput(globalContext().getTHCState(), self_, self_, noise_, lower_, upper_, training, true, NULL);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto noise_ = checked_tensor_unwrap(noise,"noise",2, false, Backend::CUDA, ScalarType::Half);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            (void) generator_; //silence unused warning
            THNN_CudaHalfRReLU_updateOutput(globalContext().getTHCState(), self_, self_, noise_, lower_, upper_, training, true, NULL);
            return self;
            break;
        }
        default:
            AT_ERROR("_thnn_rrelu_with_noise_forward_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_softplus_forward_out(Tensor & output, const Tensor & self, Scalar beta, Scalar threshold) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSoftPlus_updateOutput(globalContext().getTHCState(), self_, output_, beta_, threshold_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSoftPlus_updateOutput(globalContext().getTHCState(), self_, output_, beta_, threshold_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSoftPlus_updateOutput(globalContext().getTHCState(), self_, output_, beta_, threshold_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_softplus_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_softplus_forward(const Tensor & self, Scalar beta, Scalar threshold) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleSoftPlus_updateOutput(globalContext().getTHCState(), self_, output_, beta_, threshold_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaSoftPlus_updateOutput(globalContext().getTHCState(), self_, output_, beta_, threshold_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfSoftPlus_updateOutput(globalContext().getTHCState(), self_, output_, beta_, threshold_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_softplus_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_softplus_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar beta, Scalar threshold, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSoftPlus_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_, beta_, threshold_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSoftPlus_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_, beta_, threshold_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSoftPlus_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_, beta_, threshold_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_softplus_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_softplus_backward(const Tensor & grad_output, const Tensor & self, Scalar beta, Scalar threshold, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleSoftPlus_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_, beta_, threshold_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaSoftPlus_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_, beta_, threshold_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfSoftPlus_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_, beta_, threshold_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_softplus_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_softshrink_forward_out(Tensor & output, const Tensor & self, Scalar lambd) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto lambd_ = lambd.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSoftShrink_updateOutput(globalContext().getTHCState(), self_, output_, lambd_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto lambd_ = lambd.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSoftShrink_updateOutput(globalContext().getTHCState(), self_, output_, lambd_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto lambd_ = lambd.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSoftShrink_updateOutput(globalContext().getTHCState(), self_, output_, lambd_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_softshrink_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_softshrink_forward(const Tensor & self, Scalar lambd) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto lambd_ = lambd.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleSoftShrink_updateOutput(globalContext().getTHCState(), self_, output_, lambd_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto lambd_ = lambd.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaSoftShrink_updateOutput(globalContext().getTHCState(), self_, output_, lambd_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto lambd_ = lambd.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfSoftShrink_updateOutput(globalContext().getTHCState(), self_, output_, lambd_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_softshrink_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_softshrink_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar lambd) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto lambd_ = lambd.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSoftShrink_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, lambd_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto lambd_ = lambd.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSoftShrink_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, lambd_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto lambd_ = lambd.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSoftShrink_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, lambd_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_softshrink_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_softshrink_backward(const Tensor & grad_output, const Tensor & self, Scalar lambd) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto lambd_ = lambd.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleSoftShrink_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, lambd_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto lambd_ = lambd.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaSoftShrink_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, lambd_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto lambd_ = lambd.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfSoftShrink_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, lambd_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_softshrink_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_adaptive_avg_pool3d_forward_out(Tensor & output, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleVolumetricAdaptiveAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[2], output_size_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaVolumetricAdaptiveAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[2], output_size_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfVolumetricAdaptiveAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[2], output_size_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_adaptive_avg_pool3d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_adaptive_avg_pool3d_forward(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleVolumetricAdaptiveAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[2], output_size_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaVolumetricAdaptiveAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[2], output_size_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfVolumetricAdaptiveAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[2], output_size_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_adaptive_avg_pool3d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_adaptive_avg_pool3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",2, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleVolumetricAdaptiveAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",2, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaVolumetricAdaptiveAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",2, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfVolumetricAdaptiveAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_adaptive_avg_pool3d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_adaptive_avg_pool3d_backward(const Tensor & grad_output, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleVolumetricAdaptiveAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaVolumetricAdaptiveAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfVolumetricAdaptiveAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_adaptive_avg_pool3d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_avg_pool2d_forward_out(Tensor & output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<2>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSpatialAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], ceil_mode, count_include_pad);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<2>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSpatialAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], ceil_mode, count_include_pad);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<2>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSpatialAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], ceil_mode, count_include_pad);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_avg_pool2d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_avg_pool2d_forward(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<2>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleSpatialAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], ceil_mode, count_include_pad);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<2>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaSpatialAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], ceil_mode, count_include_pad);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<2>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfSpatialAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], ceil_mode, count_include_pad);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_avg_pool2d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_avg_pool2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<2>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSpatialAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], ceil_mode, count_include_pad);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<2>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSpatialAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], ceil_mode, count_include_pad);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<2>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSpatialAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], ceil_mode, count_include_pad);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_avg_pool2d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_avg_pool2d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<2>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleSpatialAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], ceil_mode, count_include_pad);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<2>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaSpatialAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], ceil_mode, count_include_pad);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<2>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfSpatialAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], ceil_mode, count_include_pad);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_avg_pool2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_avg_pool3d_forward_out(Tensor & output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<3>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 4);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleVolumetricAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], ceil_mode, count_include_pad);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<3>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 4);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaVolumetricAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], ceil_mode, count_include_pad);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<3>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 4);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfVolumetricAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], ceil_mode, count_include_pad);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_avg_pool3d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_avg_pool3d_forward(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<3>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 4);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleVolumetricAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], ceil_mode, count_include_pad);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<3>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 4);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaVolumetricAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], ceil_mode, count_include_pad);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<3>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 4);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfVolumetricAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], ceil_mode, count_include_pad);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_avg_pool3d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_avg_pool3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleVolumetricAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], ceil_mode, count_include_pad);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaVolumetricAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], ceil_mode, count_include_pad);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfVolumetricAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], ceil_mode, count_include_pad);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_avg_pool3d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_avg_pool3d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleVolumetricAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], ceil_mode, count_include_pad);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaVolumetricAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], ceil_mode, count_include_pad);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfVolumetricAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], ceil_mode, count_include_pad);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_avg_pool3d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::_thnn_max_pool2d_with_indices_forward_out(Tensor & output, Tensor & indices, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<2>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 5);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Double);
            auto indices_ = checked_tensor_unwrap(indices,"indices",6, false, Backend::CUDA, ScalarType::Long);
            THNN_CudaDoubleSpatialDilatedMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], ceil_mode);
            output_->maybe_zero_dim(false);
            return std::tuple<Tensor &, Tensor &>(output, indices);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<2>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 5);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",6, false, Backend::CUDA, ScalarType::Long);
            THNN_CudaSpatialDilatedMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], ceil_mode);
            output_->maybe_zero_dim(false);
            return std::tuple<Tensor &, Tensor &>(output, indices);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<2>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 5);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Half);
            auto indices_ = checked_tensor_unwrap(indices,"indices",6, false, Backend::CUDA, ScalarType::Long);
            THNN_CudaHalfSpatialDilatedMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], ceil_mode);
            output_->maybe_zero_dim(false);
            return std::tuple<Tensor &, Tensor &>(output, indices);
            break;
        }
        default:
            AT_ERROR("_thnn_max_pool2d_with_indices_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_thnn_max_pool2d_with_indices_forward(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<2>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 5);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            THNN_CudaDoubleSpatialDilatedMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], ceil_mode);
            output_->maybe_zero_dim(false);
            return std::tuple<Tensor, Tensor>(output, indices);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<2>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 5);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            THNN_CudaSpatialDilatedMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], ceil_mode);
            output_->maybe_zero_dim(false);
            return std::tuple<Tensor, Tensor>(output, indices);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<2>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 5);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            THNN_CudaHalfSpatialDilatedMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], ceil_mode);
            output_->maybe_zero_dim(false);
            return std::tuple<Tensor, Tensor>(output, indices);
            break;
        }
        default:
            AT_ERROR("_thnn_max_pool2d_with_indices_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_max_pool2d_with_indices_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<2>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 6);
            auto indices_ = checked_tensor_unwrap(indices,"indices",8, false, Backend::CUDA, ScalarType::Long);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",8, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSpatialDilatedMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], ceil_mode);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<2>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 6);
            auto indices_ = checked_tensor_unwrap(indices,"indices",8, false, Backend::CUDA, ScalarType::Long);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",8, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSpatialDilatedMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], ceil_mode);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<2>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 6);
            auto indices_ = checked_tensor_unwrap(indices,"indices",8, false, Backend::CUDA, ScalarType::Long);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",8, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSpatialDilatedMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], ceil_mode);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_max_pool2d_with_indices_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_max_pool2d_with_indices_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<2>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 6);
            auto indices_ = checked_tensor_unwrap(indices,"indices",8, false, Backend::CUDA, ScalarType::Long);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleSpatialDilatedMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], ceil_mode);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<2>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 6);
            auto indices_ = checked_tensor_unwrap(indices,"indices",8, false, Backend::CUDA, ScalarType::Long);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaSpatialDilatedMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], ceil_mode);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<2>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 6);
            auto indices_ = checked_tensor_unwrap(indices,"indices",8, false, Backend::CUDA, ScalarType::Long);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfSpatialDilatedMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], ceil_mode);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_max_pool2d_with_indices_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::_thnn_max_pool3d_with_indices_forward_out(Tensor & output, Tensor & indices, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<3>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 4);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 5);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Double);
            auto indices_ = checked_tensor_unwrap(indices,"indices",6, false, Backend::CUDA, ScalarType::Long);
            THNN_CudaDoubleVolumetricDilatedMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], ceil_mode);
            output_->maybe_zero_dim(false);
            return std::tuple<Tensor &, Tensor &>(output, indices);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<3>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 4);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 5);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",6, false, Backend::CUDA, ScalarType::Long);
            THNN_CudaVolumetricDilatedMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], ceil_mode);
            output_->maybe_zero_dim(false);
            return std::tuple<Tensor &, Tensor &>(output, indices);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<3>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 4);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 5);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Half);
            auto indices_ = checked_tensor_unwrap(indices,"indices",6, false, Backend::CUDA, ScalarType::Long);
            THNN_CudaHalfVolumetricDilatedMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], ceil_mode);
            output_->maybe_zero_dim(false);
            return std::tuple<Tensor &, Tensor &>(output, indices);
            break;
        }
        default:
            AT_ERROR("_thnn_max_pool3d_with_indices_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_thnn_max_pool3d_with_indices_forward(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<3>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 4);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 5);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            THNN_CudaDoubleVolumetricDilatedMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], ceil_mode);
            output_->maybe_zero_dim(false);
            return std::tuple<Tensor, Tensor>(output, indices);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<3>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 4);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 5);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            THNN_CudaVolumetricDilatedMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], ceil_mode);
            output_->maybe_zero_dim(false);
            return std::tuple<Tensor, Tensor>(output, indices);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<3>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 4);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 5);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CUDATensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            THNN_CudaHalfVolumetricDilatedMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], ceil_mode);
            output_->maybe_zero_dim(false);
            return std::tuple<Tensor, Tensor>(output, indices);
            break;
        }
        default:
            AT_ERROR("_thnn_max_pool3d_with_indices_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_max_pool3d_with_indices_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 6);
            auto indices_ = checked_tensor_unwrap(indices,"indices",8, false, Backend::CUDA, ScalarType::Long);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",8, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleVolumetricDilatedMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], ceil_mode);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 6);
            auto indices_ = checked_tensor_unwrap(indices,"indices",8, false, Backend::CUDA, ScalarType::Long);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",8, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaVolumetricDilatedMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], ceil_mode);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 6);
            auto indices_ = checked_tensor_unwrap(indices,"indices",8, false, Backend::CUDA, ScalarType::Long);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",8, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfVolumetricDilatedMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], ceil_mode);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_max_pool3d_with_indices_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_max_pool3d_with_indices_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 6);
            auto indices_ = checked_tensor_unwrap(indices,"indices",8, false, Backend::CUDA, ScalarType::Long);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleVolumetricDilatedMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], ceil_mode);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 6);
            auto indices_ = checked_tensor_unwrap(indices,"indices",8, false, Backend::CUDA, ScalarType::Long);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaVolumetricDilatedMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], ceil_mode);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 6);
            auto indices_ = checked_tensor_unwrap(indices,"indices",8, false, Backend::CUDA, ScalarType::Long);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfVolumetricDilatedMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], ceil_mode);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_max_pool3d_with_indices_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_max_unpool2d_forward_out(Tensor & output, const Tensor & self, const Tensor & indices, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 3);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSpatialMaxUnpooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[1], output_size_[0]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 3);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSpatialMaxUnpooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[1], output_size_[0]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 3);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSpatialMaxUnpooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[1], output_size_[0]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_max_unpool2d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_max_unpool2d_forward(const Tensor & self, const Tensor & indices, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 3);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleSpatialMaxUnpooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[1], output_size_[0]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 3);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaSpatialMaxUnpooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[1], output_size_[0]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 3);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfSpatialMaxUnpooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[1], output_size_[0]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_max_unpool2d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_max_unpool2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 4);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSpatialMaxUnpooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, output_size_[1], output_size_[0]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 4);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSpatialMaxUnpooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, output_size_[1], output_size_[0]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 4);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSpatialMaxUnpooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, output_size_[1], output_size_[0]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_max_unpool2d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_max_unpool2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 4);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleSpatialMaxUnpooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, output_size_[1], output_size_[0]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 4);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaSpatialMaxUnpooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, output_size_[1], output_size_[0]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 4);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfSpatialMaxUnpooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, output_size_[1], output_size_[0]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_max_unpool2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_max_unpool3d_forward_out(Tensor & output, const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleVolumetricMaxUnpooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[0], output_size_[2], output_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaVolumetricMaxUnpooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[0], output_size_[2], output_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfVolumetricMaxUnpooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[0], output_size_[2], output_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_max_unpool3d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_max_unpool3d_forward(const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleVolumetricMaxUnpooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[0], output_size_[2], output_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaVolumetricMaxUnpooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[0], output_size_[2], output_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfVolumetricMaxUnpooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[0], output_size_[2], output_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_max_unpool3d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_max_unpool3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",6, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleVolumetricMaxUnpooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, output_size_[0], output_size_[2], output_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",6, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaVolumetricMaxUnpooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, output_size_[0], output_size_[2], output_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",6, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfVolumetricMaxUnpooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, output_size_[0], output_size_[2], output_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_max_unpool3d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_max_unpool3d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleVolumetricMaxUnpooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, output_size_[0], output_size_[2], output_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaVolumetricMaxUnpooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, output_size_[0], output_size_[2], output_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CUDA, ScalarType::Long);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfVolumetricMaxUnpooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, output_size_[0], output_size_[2], output_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_max_unpool3d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_upsample_linear1d_forward_out(Tensor & output, const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleTemporalUpSamplingLinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], align_corners);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaTemporalUpSamplingLinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], align_corners);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfTemporalUpSamplingLinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], align_corners);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_linear1d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_upsample_linear1d_forward(const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleTemporalUpSamplingLinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], align_corners);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaTemporalUpSamplingLinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], align_corners);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfTemporalUpSamplingLinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], align_corners);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_linear1d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_upsample_linear1d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<3>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleTemporalUpSamplingLinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], output_size_[0], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<3>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaTemporalUpSamplingLinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], output_size_[0], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<3>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfTemporalUpSamplingLinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], output_size_[0], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_linear1d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_upsample_linear1d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<3>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleTemporalUpSamplingLinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], output_size_[0], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<3>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaTemporalUpSamplingLinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], output_size_[0], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<3>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfTemporalUpSamplingLinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], output_size_[0], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_linear1d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_upsample_bilinear2d_forward_out(Tensor & output, const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSpatialUpSamplingBilinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], align_corners);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSpatialUpSamplingBilinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], align_corners);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSpatialUpSamplingBilinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], align_corners);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_bilinear2d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_upsample_bilinear2d_forward(const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleSpatialUpSamplingBilinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], align_corners);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaSpatialUpSamplingBilinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], align_corners);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfSpatialUpSamplingBilinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], align_corners);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_bilinear2d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_upsample_bilinear2d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSpatialUpSamplingBilinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSpatialUpSamplingBilinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSpatialUpSamplingBilinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_bilinear2d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_upsample_bilinear2d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleSpatialUpSamplingBilinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaSpatialUpSamplingBilinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfSpatialUpSamplingBilinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_bilinear2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_upsample_bicubic2d_forward_out(Tensor & output, const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSpatialUpSamplingBicubic_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], align_corners);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSpatialUpSamplingBicubic_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], align_corners);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSpatialUpSamplingBicubic_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], align_corners);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_bicubic2d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_upsample_bicubic2d_forward(const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleSpatialUpSamplingBicubic_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], align_corners);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaSpatialUpSamplingBicubic_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], align_corners);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfSpatialUpSamplingBicubic_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], align_corners);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_bicubic2d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_upsample_bicubic2d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSpatialUpSamplingBicubic_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSpatialUpSamplingBicubic_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSpatialUpSamplingBicubic_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_bicubic2d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_upsample_bicubic2d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleSpatialUpSamplingBicubic_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaSpatialUpSamplingBicubic_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfSpatialUpSamplingBicubic_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_bicubic2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_upsample_trilinear3d_forward_out(Tensor & output, const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleVolumetricUpSamplingTrilinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], output_size_[2], align_corners);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaVolumetricUpSamplingTrilinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], output_size_[2], align_corners);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfVolumetricUpSamplingTrilinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], output_size_[2], align_corners);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_trilinear3d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_upsample_trilinear3d_forward(const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleVolumetricUpSamplingTrilinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], output_size_[2], align_corners);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaVolumetricUpSamplingTrilinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], output_size_[2], align_corners);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfVolumetricUpSamplingTrilinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], output_size_[2], align_corners);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_trilinear3d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_upsample_trilinear3d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<5>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleVolumetricUpSamplingTrilinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], input_size_[4], output_size_[0], output_size_[1], output_size_[2], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<5>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaVolumetricUpSamplingTrilinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], input_size_[4], output_size_[0], output_size_[1], output_size_[2], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<5>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfVolumetricUpSamplingTrilinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], input_size_[4], output_size_[0], output_size_[1], output_size_[2], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_trilinear3d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_upsample_trilinear3d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<5>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleVolumetricUpSamplingTrilinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], input_size_[4], output_size_[0], output_size_[1], output_size_[2], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<5>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaVolumetricUpSamplingTrilinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], input_size_[4], output_size_[0], output_size_[1], output_size_[2], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<5>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfVolumetricUpSamplingTrilinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], input_size_[4], output_size_[0], output_size_[1], output_size_[2], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_trilinear3d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_upsample_nearest1d_forward_out(Tensor & output, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleTemporalUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0]);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaTemporalUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0]);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfTemporalUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0]);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest1d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_upsample_nearest1d_forward(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleTemporalUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0]);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaTemporalUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0]);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfTemporalUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0]);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest1d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_upsample_nearest1d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<3>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleTemporalUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], output_size_[0]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<3>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaTemporalUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], output_size_[0]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<3>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfTemporalUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], output_size_[0]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest1d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_upsample_nearest1d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<3>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleTemporalUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], output_size_[0]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<3>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaTemporalUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], output_size_[0]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<3>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfTemporalUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], output_size_[0]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest1d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_upsample_nearest2d_forward_out(Tensor & output, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSpatialUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1]);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSpatialUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1]);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSpatialUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1]);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest2d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_upsample_nearest2d_forward(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleSpatialUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1]);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaSpatialUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1]);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfSpatialUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1]);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest2d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_upsample_nearest2d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSpatialUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSpatialUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSpatialUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest2d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_upsample_nearest2d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleSpatialUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaSpatialUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfSpatialUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_upsample_nearest3d_forward_out(Tensor & output, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleVolumetricUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], output_size_[2]);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaVolumetricUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], output_size_[2]);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfVolumetricUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], output_size_[2]);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest3d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_upsample_nearest3d_forward(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleVolumetricUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], output_size_[2]);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaVolumetricUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], output_size_[2]);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfVolumetricUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], output_size_[2]);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest3d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_upsample_nearest3d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<5>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleVolumetricUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], input_size_[4], output_size_[0], output_size_[1], output_size_[2]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<5>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaVolumetricUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], input_size_[4], output_size_[0], output_size_[1], output_size_[2]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<5>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfVolumetricUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], input_size_[4], output_size_[0], output_size_[1], output_size_[2]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest3d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_upsample_nearest3d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<5>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleVolumetricUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], input_size_[4], output_size_[0], output_size_[1], output_size_[2]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<5>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaVolumetricUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], input_size_[4], output_size_[0], output_size_[1], output_size_[2]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<5>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfVolumetricUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], input_size_[4], output_size_[0], output_size_[1], output_size_[2]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest3d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_sigmoid_forward_out(Tensor & output, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output,"output",1, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSigmoid_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",1, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSigmoid_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output,"output",1, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSigmoid_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_sigmoid_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_sigmoid_forward(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleSigmoid_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaSigmoid_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfSigmoid_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_sigmoid_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_sigmoid_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",2, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSigmoid_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",2, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSigmoid_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",2, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSigmoid_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_sigmoid_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_sigmoid_backward(const Tensor & grad_output, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleSigmoid_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaSigmoid_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfSigmoid_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_sigmoid_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_tanh_forward_out(Tensor & output, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output,"output",1, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleTanh_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",1, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaTanh_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output,"output",1, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfTanh_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_tanh_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_tanh_forward(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleTanh_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaTanh_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfTanh_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_tanh_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_tanh_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",2, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleTanh_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",2, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaTanh_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",2, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfTanh_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_tanh_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_tanh_backward(const Tensor & grad_output, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleTanh_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaTanh_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfTanh_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_tanh_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAType::_thnn_conv_transpose2d_forward_out(Tensor & output, Tensor & columns, Tensor & ones, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Double);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_padding_ = check_intlist<2>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 8);
            auto output_ = checked_tensor_unwrap(output,"output",8, false, Backend::CUDA, ScalarType::Double);
            auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CUDA, ScalarType::Double);
            auto ones_ = checked_tensor_unwrap(ones,"ones",8, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSpatialFullDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, columns, ones);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Float);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_padding_ = check_intlist<2>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 8);
            auto output_ = checked_tensor_unwrap(output,"output",8, false, Backend::CUDA, ScalarType::Float);
            auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CUDA, ScalarType::Float);
            auto ones_ = checked_tensor_unwrap(ones,"ones",8, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSpatialFullDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, columns, ones);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_padding_ = check_intlist<2>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 8);
            auto output_ = checked_tensor_unwrap(output,"output",8, false, Backend::CUDA, ScalarType::Half);
            auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CUDA, ScalarType::Half);
            auto ones_ = checked_tensor_unwrap(ones,"ones",8, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSpatialFullDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, columns, ones);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_transpose2d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::_thnn_conv_transpose2d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Double);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_padding_ = check_intlist<2>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 8);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto columns_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto columns = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(columns_));
            auto ones_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto ones = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(ones_));
            THNN_CudaDoubleSpatialFullDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, columns, ones);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Float);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_padding_ = check_intlist<2>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 8);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto columns_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto columns = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(columns_));
            auto ones_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto ones = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(ones_));
            THNN_CudaSpatialFullDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, columns, ones);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_padding_ = check_intlist<2>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 8);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto columns_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto columns = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(columns_));
            auto ones_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto ones = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(ones_));
            THNN_CudaHalfSpatialFullDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, columns, ones);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_transpose2d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAType::_thnn_conv_transpose2d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation, const Tensor & columns, const Tensor & ones) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_padding_ = check_intlist<2>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 8);
            auto columns_ = checked_tensor_unwrap(columns,"columns",9, false, Backend::CUDA, ScalarType::Double);
            auto ones_ = checked_tensor_unwrap(ones,"ones",10, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",10, true, Backend::CUDA, ScalarType::Double);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",10, true, Backend::CUDA, ScalarType::Double);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",10, true, Backend::CUDA, ScalarType::Double);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(1) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaDoubleSpatialFullDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaDoubleSpatialFullDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_padding_ = check_intlist<2>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 8);
            auto columns_ = checked_tensor_unwrap(columns,"columns",9, false, Backend::CUDA, ScalarType::Float);
            auto ones_ = checked_tensor_unwrap(ones,"ones",10, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",10, true, Backend::CUDA, ScalarType::Float);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",10, true, Backend::CUDA, ScalarType::Float);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",10, true, Backend::CUDA, ScalarType::Float);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(1) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaSpatialFullDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaSpatialFullDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_padding_ = check_intlist<2>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 8);
            auto columns_ = checked_tensor_unwrap(columns,"columns",9, false, Backend::CUDA, ScalarType::Half);
            auto ones_ = checked_tensor_unwrap(ones,"ones",10, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",10, true, Backend::CUDA, ScalarType::Half);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",10, true, Backend::CUDA, ScalarType::Half);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",10, true, Backend::CUDA, ScalarType::Half);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(1) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaHalfSpatialFullDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaHalfSpatialFullDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_transpose2d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::_thnn_conv_transpose2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation, const Tensor & columns, const Tensor & ones, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_padding_ = check_intlist<2>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 8);
            auto columns_ = checked_tensor_unwrap(columns,"columns",9, false, Backend::CUDA, ScalarType::Double);
            auto ones_ = checked_tensor_unwrap(ones,"ones",10, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(1) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaDoubleSpatialFullDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaDoubleSpatialFullDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_padding_ = check_intlist<2>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 8);
            auto columns_ = checked_tensor_unwrap(columns,"columns",9, false, Backend::CUDA, ScalarType::Float);
            auto ones_ = checked_tensor_unwrap(ones,"ones",10, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(1) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaSpatialFullDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaSpatialFullDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_padding_ = check_intlist<2>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 8);
            auto columns_ = checked_tensor_unwrap(columns,"columns",9, false, Backend::CUDA, ScalarType::Half);
            auto ones_ = checked_tensor_unwrap(ones,"ones",10, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(1) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaHalfSpatialFullDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaHalfSpatialFullDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_transpose2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAType::_thnn_conv_transpose3d_forward_out(Tensor & output, Tensor & finput, Tensor & fgrad_input, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Double);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto output_padding_ = check_intlist<3>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 8);
            auto output_ = checked_tensor_unwrap(output,"output",8, false, Backend::CUDA, ScalarType::Double);
            auto finput_ = checked_tensor_unwrap(finput,"finput",8, false, Backend::CUDA, ScalarType::Double);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",8, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleVolumetricFullDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, finput, fgrad_input);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Float);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto output_padding_ = check_intlist<3>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 8);
            auto output_ = checked_tensor_unwrap(output,"output",8, false, Backend::CUDA, ScalarType::Float);
            auto finput_ = checked_tensor_unwrap(finput,"finput",8, false, Backend::CUDA, ScalarType::Float);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",8, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaVolumetricFullDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, finput, fgrad_input);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto output_padding_ = check_intlist<3>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 8);
            auto output_ = checked_tensor_unwrap(output,"output",8, false, Backend::CUDA, ScalarType::Half);
            auto finput_ = checked_tensor_unwrap(finput,"finput",8, false, Backend::CUDA, ScalarType::Half);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",8, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfVolumetricFullDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, finput, fgrad_input);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_transpose3d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::_thnn_conv_transpose3d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Double);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto output_padding_ = check_intlist<3>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 8);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto finput_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto finput = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(finput_));
            auto fgrad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto fgrad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(fgrad_input_));
            THNN_CudaDoubleVolumetricFullDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, finput, fgrad_input);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Float);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto output_padding_ = check_intlist<3>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 8);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto finput_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto finput = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(finput_));
            auto fgrad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto fgrad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(fgrad_input_));
            THNN_CudaVolumetricFullDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, finput, fgrad_input);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto output_padding_ = check_intlist<3>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 8);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto finput_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto finput = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(finput_));
            auto fgrad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto fgrad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(fgrad_input_));
            THNN_CudaHalfVolumetricFullDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, finput, fgrad_input);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_transpose3d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAType::_thnn_conv_transpose3d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation, const Tensor & finput, const Tensor & fgrad_input) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto output_padding_ = check_intlist<3>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 8);
            auto finput_ = checked_tensor_unwrap(finput,"finput",9, false, Backend::CUDA, ScalarType::Double);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",10, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",10, true, Backend::CUDA, ScalarType::Double);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",10, true, Backend::CUDA, ScalarType::Double);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",10, true, Backend::CUDA, ScalarType::Double);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(1) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaDoubleVolumetricFullDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1]);
            if (grad_weight_ || grad_bias_) THNN_CudaDoubleVolumetricFullDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto output_padding_ = check_intlist<3>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 8);
            auto finput_ = checked_tensor_unwrap(finput,"finput",9, false, Backend::CUDA, ScalarType::Float);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",10, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",10, true, Backend::CUDA, ScalarType::Float);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",10, true, Backend::CUDA, ScalarType::Float);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",10, true, Backend::CUDA, ScalarType::Float);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(1) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaVolumetricFullDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1]);
            if (grad_weight_ || grad_bias_) THNN_CudaVolumetricFullDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto output_padding_ = check_intlist<3>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 8);
            auto finput_ = checked_tensor_unwrap(finput,"finput",9, false, Backend::CUDA, ScalarType::Half);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",10, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",10, true, Backend::CUDA, ScalarType::Half);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",10, true, Backend::CUDA, ScalarType::Half);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",10, true, Backend::CUDA, ScalarType::Half);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(1) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaHalfVolumetricFullDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1]);
            if (grad_weight_ || grad_bias_) THNN_CudaHalfVolumetricFullDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_transpose3d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::_thnn_conv_transpose3d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation, const Tensor & finput, const Tensor & fgrad_input, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto output_padding_ = check_intlist<3>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 8);
            auto finput_ = checked_tensor_unwrap(finput,"finput",9, false, Backend::CUDA, ScalarType::Double);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",10, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(1) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaDoubleVolumetricFullDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1]);
            if (grad_weight_ || grad_bias_) THNN_CudaDoubleVolumetricFullDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto output_padding_ = check_intlist<3>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 8);
            auto finput_ = checked_tensor_unwrap(finput,"finput",9, false, Backend::CUDA, ScalarType::Float);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",10, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(1) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaVolumetricFullDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1]);
            if (grad_weight_ || grad_bias_) THNN_CudaVolumetricFullDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto output_padding_ = check_intlist<3>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 8);
            auto finput_ = checked_tensor_unwrap(finput,"finput",9, false, Backend::CUDA, ScalarType::Half);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",10, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(1) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaHalfVolumetricFullDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1]);
            if (grad_weight_ || grad_bias_) THNN_CudaHalfVolumetricFullDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_transpose3d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAType::_thnn_conv2d_forward_out(Tensor & output, Tensor & finput, Tensor & fgrad_input, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Double);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Double);
            auto finput_ = checked_tensor_unwrap(finput,"finput",6, false, Backend::CUDA, ScalarType::Double);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",6, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSpatialConvolutionMM_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, finput, fgrad_input);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Float);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Float);
            auto finput_ = checked_tensor_unwrap(finput,"finput",6, false, Backend::CUDA, ScalarType::Float);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",6, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSpatialConvolutionMM_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, finput, fgrad_input);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Half);
            auto finput_ = checked_tensor_unwrap(finput,"finput",6, false, Backend::CUDA, ScalarType::Half);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",6, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSpatialConvolutionMM_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, finput, fgrad_input);
            break;
        }
        default:
            AT_ERROR("_thnn_conv2d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::_thnn_conv2d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Double);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto finput_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto finput = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(finput_));
            auto fgrad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto fgrad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(fgrad_input_));
            THNN_CudaDoubleSpatialConvolutionMM_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, finput, fgrad_input);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Float);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto finput_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto finput = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(finput_));
            auto fgrad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto fgrad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(fgrad_input_));
            THNN_CudaSpatialConvolutionMM_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, finput, fgrad_input);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto finput_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto finput = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(finput_));
            auto fgrad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto fgrad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(fgrad_input_));
            THNN_CudaHalfSpatialConvolutionMM_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, finput, fgrad_input);
            break;
        }
        default:
            AT_ERROR("_thnn_conv2d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAType::_thnn_conv2d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, const Tensor & finput, const Tensor & fgrad_input) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto finput_ = checked_tensor_unwrap(finput,"finput",7, false, Backend::CUDA, ScalarType::Double);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",8, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",8, true, Backend::CUDA, ScalarType::Double);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",8, true, Backend::CUDA, ScalarType::Double);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",8, true, Backend::CUDA, ScalarType::Double);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaDoubleSpatialConvolutionMM_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaDoubleSpatialConvolutionMM_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto finput_ = checked_tensor_unwrap(finput,"finput",7, false, Backend::CUDA, ScalarType::Float);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",8, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",8, true, Backend::CUDA, ScalarType::Float);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",8, true, Backend::CUDA, ScalarType::Float);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",8, true, Backend::CUDA, ScalarType::Float);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaSpatialConvolutionMM_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaSpatialConvolutionMM_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto finput_ = checked_tensor_unwrap(finput,"finput",7, false, Backend::CUDA, ScalarType::Half);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",8, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",8, true, Backend::CUDA, ScalarType::Half);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",8, true, Backend::CUDA, ScalarType::Half);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",8, true, Backend::CUDA, ScalarType::Half);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaHalfSpatialConvolutionMM_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaHalfSpatialConvolutionMM_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv2d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::_thnn_conv2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, const Tensor & finput, const Tensor & fgrad_input, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto finput_ = checked_tensor_unwrap(finput,"finput",7, false, Backend::CUDA, ScalarType::Double);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",8, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaDoubleSpatialConvolutionMM_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaDoubleSpatialConvolutionMM_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto finput_ = checked_tensor_unwrap(finput,"finput",7, false, Backend::CUDA, ScalarType::Float);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",8, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaSpatialConvolutionMM_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaSpatialConvolutionMM_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto finput_ = checked_tensor_unwrap(finput,"finput",7, false, Backend::CUDA, ScalarType::Half);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",8, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaHalfSpatialConvolutionMM_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaHalfSpatialConvolutionMM_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_conv_depthwise2d_forward_out(Tensor & output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Double);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto output_ = checked_tensor_unwrap(output,"output",7, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSpatialDepthwiseConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            output_->maybe_zero_dim(self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0));
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Float);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto output_ = checked_tensor_unwrap(output,"output",7, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSpatialDepthwiseConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            output_->maybe_zero_dim(self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0));
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto output_ = checked_tensor_unwrap(output,"output",7, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSpatialDepthwiseConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            output_->maybe_zero_dim(self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0));
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_conv_depthwise2d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_conv_depthwise2d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Double);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleSpatialDepthwiseConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            output_->maybe_zero_dim(self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0));
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Float);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaSpatialDepthwiseConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            output_->maybe_zero_dim(self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0));
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfSpatialDepthwiseConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            output_->maybe_zero_dim(self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0));
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_conv_depthwise2d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::_thnn_conv_depthwise2d_backward_out(Tensor & grad_input, Tensor & grad_weight, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, true, Backend::CUDA, ScalarType::Double);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",7, true, Backend::CUDA, ScalarType::Double);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            if (grad_input_) THNN_CudaDoubleSpatialDepthwiseConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_weight_) THNN_CudaDoubleSpatialDepthwiseConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &>(grad_input, grad_weight);
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, true, Backend::CUDA, ScalarType::Float);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",7, true, Backend::CUDA, ScalarType::Float);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            if (grad_input_) THNN_CudaSpatialDepthwiseConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_weight_) THNN_CudaSpatialDepthwiseConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &>(grad_input, grad_weight);
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, true, Backend::CUDA, ScalarType::Half);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",7, true, Backend::CUDA, ScalarType::Half);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            if (grad_input_) THNN_CudaHalfSpatialDepthwiseConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_weight_) THNN_CudaHalfSpatialDepthwiseConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &>(grad_input, grad_weight);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_depthwise2d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_thnn_conv_depthwise2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, std::array<bool,2> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            if (grad_input_) THNN_CudaDoubleSpatialDepthwiseConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_weight_) THNN_CudaDoubleSpatialDepthwiseConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor>(grad_input, grad_weight);
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            if (grad_input_) THNN_CudaSpatialDepthwiseConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_weight_) THNN_CudaSpatialDepthwiseConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor>(grad_input, grad_weight);
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            if (grad_input_) THNN_CudaHalfSpatialDepthwiseConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_weight_) THNN_CudaHalfSpatialDepthwiseConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor>(grad_input, grad_weight);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_depthwise2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAType::_thnn_conv_dilated2d_forward_out(Tensor & output, Tensor & columns, Tensor & ones, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Double);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto output_ = checked_tensor_unwrap(output,"output",7, false, Backend::CUDA, ScalarType::Double);
            auto columns_ = checked_tensor_unwrap(columns,"columns",7, false, Backend::CUDA, ScalarType::Double);
            auto ones_ = checked_tensor_unwrap(ones,"ones",7, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSpatialDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, columns, ones);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Float);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto output_ = checked_tensor_unwrap(output,"output",7, false, Backend::CUDA, ScalarType::Float);
            auto columns_ = checked_tensor_unwrap(columns,"columns",7, false, Backend::CUDA, ScalarType::Float);
            auto ones_ = checked_tensor_unwrap(ones,"ones",7, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSpatialDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, columns, ones);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto output_ = checked_tensor_unwrap(output,"output",7, false, Backend::CUDA, ScalarType::Half);
            auto columns_ = checked_tensor_unwrap(columns,"columns",7, false, Backend::CUDA, ScalarType::Half);
            auto ones_ = checked_tensor_unwrap(ones,"ones",7, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSpatialDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, columns, ones);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_dilated2d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::_thnn_conv_dilated2d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Double);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto columns_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto columns = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(columns_));
            auto ones_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto ones = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(ones_));
            THNN_CudaDoubleSpatialDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, columns, ones);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Float);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto columns_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto columns = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(columns_));
            auto ones_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto ones = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(ones_));
            THNN_CudaSpatialDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, columns, ones);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto columns_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto columns = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(columns_));
            auto ones_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto ones = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(ones_));
            THNN_CudaHalfSpatialDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, columns, ones);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_dilated2d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAType::_thnn_conv_dilated2d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, const Tensor & columns, const Tensor & ones) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CUDA, ScalarType::Double);
            auto ones_ = checked_tensor_unwrap(ones,"ones",9, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",9, true, Backend::CUDA, ScalarType::Double);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",9, true, Backend::CUDA, ScalarType::Double);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",9, true, Backend::CUDA, ScalarType::Double);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaDoubleSpatialDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaDoubleSpatialDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CUDA, ScalarType::Float);
            auto ones_ = checked_tensor_unwrap(ones,"ones",9, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",9, true, Backend::CUDA, ScalarType::Float);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",9, true, Backend::CUDA, ScalarType::Float);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",9, true, Backend::CUDA, ScalarType::Float);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaSpatialDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaSpatialDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CUDA, ScalarType::Half);
            auto ones_ = checked_tensor_unwrap(ones,"ones",9, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",9, true, Backend::CUDA, ScalarType::Half);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",9, true, Backend::CUDA, ScalarType::Half);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",9, true, Backend::CUDA, ScalarType::Half);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaHalfSpatialDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaHalfSpatialDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_dilated2d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::_thnn_conv_dilated2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, const Tensor & columns, const Tensor & ones, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CUDA, ScalarType::Double);
            auto ones_ = checked_tensor_unwrap(ones,"ones",9, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaDoubleSpatialDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaDoubleSpatialDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CUDA, ScalarType::Float);
            auto ones_ = checked_tensor_unwrap(ones,"ones",9, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaSpatialDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaSpatialDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CUDA, ScalarType::Half);
            auto ones_ = checked_tensor_unwrap(ones,"ones",9, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaHalfSpatialDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaHalfSpatialDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_dilated2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAType::_thnn_conv_dilated3d_forward_out(Tensor & output, Tensor & columns, Tensor & ones, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Double);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 7);
            auto output_ = checked_tensor_unwrap(output,"output",7, false, Backend::CUDA, ScalarType::Double);
            auto columns_ = checked_tensor_unwrap(columns,"columns",7, false, Backend::CUDA, ScalarType::Double);
            auto ones_ = checked_tensor_unwrap(ones,"ones",7, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleVolumetricDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, columns, ones);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Float);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 7);
            auto output_ = checked_tensor_unwrap(output,"output",7, false, Backend::CUDA, ScalarType::Float);
            auto columns_ = checked_tensor_unwrap(columns,"columns",7, false, Backend::CUDA, ScalarType::Float);
            auto ones_ = checked_tensor_unwrap(ones,"ones",7, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaVolumetricDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, columns, ones);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 7);
            auto output_ = checked_tensor_unwrap(output,"output",7, false, Backend::CUDA, ScalarType::Half);
            auto columns_ = checked_tensor_unwrap(columns,"columns",7, false, Backend::CUDA, ScalarType::Half);
            auto ones_ = checked_tensor_unwrap(ones,"ones",7, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfVolumetricDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, columns, ones);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_dilated3d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::_thnn_conv_dilated3d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Double);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 7);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto columns_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto columns = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(columns_));
            auto ones_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto ones = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(ones_));
            THNN_CudaDoubleVolumetricDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, columns, ones);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Float);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 7);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto columns_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto columns = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(columns_));
            auto ones_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto ones = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(ones_));
            THNN_CudaVolumetricDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, columns, ones);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 7);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto columns_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto columns = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(columns_));
            auto ones_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto ones = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(ones_));
            THNN_CudaHalfVolumetricDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, columns, ones);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_dilated3d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAType::_thnn_conv_dilated3d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, const Tensor & columns, const Tensor & ones) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 7);
            auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CUDA, ScalarType::Double);
            auto ones_ = checked_tensor_unwrap(ones,"ones",9, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",9, true, Backend::CUDA, ScalarType::Double);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",9, true, Backend::CUDA, ScalarType::Double);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",9, true, Backend::CUDA, ScalarType::Double);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaDoubleVolumetricDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1]);
            if (grad_weight_ || grad_bias_) THNN_CudaDoubleVolumetricDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 7);
            auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CUDA, ScalarType::Float);
            auto ones_ = checked_tensor_unwrap(ones,"ones",9, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",9, true, Backend::CUDA, ScalarType::Float);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",9, true, Backend::CUDA, ScalarType::Float);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",9, true, Backend::CUDA, ScalarType::Float);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaVolumetricDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1]);
            if (grad_weight_ || grad_bias_) THNN_CudaVolumetricDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 7);
            auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CUDA, ScalarType::Half);
            auto ones_ = checked_tensor_unwrap(ones,"ones",9, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",9, true, Backend::CUDA, ScalarType::Half);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",9, true, Backend::CUDA, ScalarType::Half);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",9, true, Backend::CUDA, ScalarType::Half);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaHalfVolumetricDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1]);
            if (grad_weight_ || grad_bias_) THNN_CudaHalfVolumetricDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_dilated3d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::_thnn_conv_dilated3d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, const Tensor & columns, const Tensor & ones, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 7);
            auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CUDA, ScalarType::Double);
            auto ones_ = checked_tensor_unwrap(ones,"ones",9, false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaDoubleVolumetricDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1]);
            if (grad_weight_ || grad_bias_) THNN_CudaDoubleVolumetricDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 7);
            auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CUDA, ScalarType::Float);
            auto ones_ = checked_tensor_unwrap(ones,"ones",9, false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaVolumetricDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1]);
            if (grad_weight_ || grad_bias_) THNN_CudaVolumetricDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 7);
            auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CUDA, ScalarType::Half);
            auto ones_ = checked_tensor_unwrap(ones,"ones",9, false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaHalfVolumetricDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1]);
            if (grad_weight_ || grad_bias_) THNN_CudaHalfVolumetricDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_dilated3d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_col2im_forward_out(Tensor & output, const Tensor & self, IntArrayRef output_size, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 4);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto stride_ = check_intlist<2>(stride, "stride", 6);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleCol2Im_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 4);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto stride_ = check_intlist<2>(stride, "stride", 6);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaCol2Im_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 4);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto stride_ = check_intlist<2>(stride, "stride", 6);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfCol2Im_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_col2im_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_col2im_forward(const Tensor & self, IntArrayRef output_size, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 4);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto stride_ = check_intlist<2>(stride, "stride", 6);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleCol2Im_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 4);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto stride_ = check_intlist<2>(stride, "stride", 6);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaCol2Im_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 4);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto stride_ = check_intlist<2>(stride, "stride", 6);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfCol2Im_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_col2im_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_col2im_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 3);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleCol2Im_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 3);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaCol2Im_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 3);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfCol2Im_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_col2im_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_col2im_backward(const Tensor & grad_output, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 3);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleCol2Im_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 3);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaCol2Im_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 3);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfCol2Im_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_col2im_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_im2col_forward_out(Tensor & output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 3);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleIm2Col_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 3);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaIm2Col_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 3);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfIm2Col_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_im2col_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_im2col_forward(const Tensor & self, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 3);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleIm2Col_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 3);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaIm2Col_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 3);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfIm2Col_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_im2col_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_thnn_im2col_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef input_size, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto input_size_ = check_intlist<2>(input_size, "input_size", 2);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 4);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto stride_ = check_intlist<2>(stride, "stride", 6);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",6, false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleIm2Col_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto input_size_ = check_intlist<2>(input_size, "input_size", 2);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 4);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto stride_ = check_intlist<2>(stride, "stride", 6);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",6, false, Backend::CUDA, ScalarType::Float);
            THNN_CudaIm2Col_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto input_size_ = check_intlist<2>(input_size, "input_size", 2);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 4);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto stride_ = check_intlist<2>(stride, "stride", 6);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",6, false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfIm2Col_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_im2col_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_thnn_im2col_backward(const Tensor & grad_output, IntArrayRef input_size, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Double);
            auto input_size_ = check_intlist<2>(input_size, "input_size", 2);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 4);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto stride_ = check_intlist<2>(stride, "stride", 6);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleIm2Col_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Float);
            auto input_size_ = check_intlist<2>(input_size, "input_size", 2);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 4);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto stride_ = check_intlist<2>(stride, "stride", 6);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaIm2Col_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
            auto input_size_ = check_intlist<2>(input_size, "input_size", 2);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 4);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto stride_ = check_intlist<2>(stride, "stride", 6);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),CUDATensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfIm2Col_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_im2col_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_cudnn_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity) const {
    const OptionalDeviceGuard device_guard(device_of(log_probs));
    auto dispatch_scalar_type = infer_scalar_type(log_probs);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_cudnn_ctc_loss(/* actuals */ log_probs, targets, input_lengths, target_lengths, blank, deterministic, zero_infinity);
        break;
        default:
            AT_ERROR("_cudnn_ctc_loss not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_cudnn_rnn_flatten_weight(TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, bool bidirectional) const {
    const OptionalDeviceGuard device_guard(device_of(weight_arr));
    auto dispatch_scalar_type = infer_scalar_type(weight_arr);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_cudnn_rnn_flatten_weight(/* actuals */ weight_arr, weight_stride0, input_size, mode, hidden_size, num_layers, batch_first, bidirectional);
        break;
        default:
            AT_ERROR("_cudnn_rnn_flatten_weight not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CUDAType::_cudnn_rnn(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    auto dispatch_scalar_type = infer_scalar_type(input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_cudnn_rnn(/* actuals */ input, weight, weight_stride0, weight_buf, hx, cx, mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state);
        break;
        default:
            AT_ERROR("_cudnn_rnn not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor,std::vector<Tensor>> CUDAType::_cudnn_rnn_backward(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, const Tensor & output, const Tensor & grad_output, const Tensor & grad_hy, const Tensor & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state, const Tensor & reserve, std::array<bool,4> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    auto dispatch_scalar_type = infer_scalar_type(input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_cudnn_rnn_backward(/* actuals */ input, weight, weight_stride0, weight_buf, hx, cx, output, grad_output, grad_hy, grad_cy, mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state, reserve, output_mask);
        break;
        default:
            AT_ERROR("_cudnn_rnn_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    auto dispatch_scalar_type = typeMetaToScalarType(options.dtype());
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_cudnn_init_dropout_state(/* actuals */ dropout, train, dropout_seed, options);
        break;
        default:
            AT_ERROR("_cudnn_init_dropout_state not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_fused_dropout(const Tensor & self, double p, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::fused_dropout_cuda(/* actuals */ self, p, generator);
        break;
        default:
            AT_ERROR("_fused_dropout not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_masked_scale(const Tensor & self, const Tensor & mask, double scale) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::masked_scale_cuda(/* actuals */ self, mask, scale);
        break;
        default:
            AT_ERROR("_masked_scale not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::abs_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_abs__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("abs_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::abs_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_abs_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("abs_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::acos_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_acos__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("acos_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::acos_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_acos_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("acos_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::add(const Tensor & self, const Tensor & other, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::add(/* actuals */ self, other, alpha);
        break;
        default:
            AT_ERROR("add not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::add_(Tensor & self, const Tensor & other, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::add_(/* actuals */ self, other, alpha);
        break;
        default:
            AT_ERROR("add_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::add_out(/* actuals */ out, self, other, alpha);
        break;
        default:
            AT_ERROR("add_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::arange_out(Tensor & out, Scalar start, Scalar end, Scalar step) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    auto dispatch_scalar_type = infer_scalar_type(out);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::arange_cuda_out(/* actuals */ out, start, end, step);
        break;
        default:
            AT_ERROR("arange_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::as_strided(const Tensor & self, IntArrayRef size, IntArrayRef stride, c10::optional<int64_t> storage_offset) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool:
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::as_strided_tensorimpl(/* actuals */ self, size, stride, storage_offset);
        break;
        default:
            AT_ERROR("as_strided not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::asin_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_asin__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("asin_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::asin_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_asin_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("asin_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::atan_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_atan__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("atan_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::atan_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_atan_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("atan_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::baddbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::baddbmm_cuda(/* actuals */ self, batch1, batch2, beta, alpha);
        break;
        default:
            AT_ERROR("baddbmm not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::baddbmm_(Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::baddbmm__cuda(/* actuals */ self, batch1, batch2, beta, alpha);
        break;
        default:
            AT_ERROR("baddbmm_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::baddbmm_out(Tensor & out, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::baddbmm_out_cuda(/* actuals */ out, self, batch1, batch2, beta, alpha);
        break;
        default:
            AT_ERROR("baddbmm_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::bernoulli_(Tensor & self, const Tensor & p, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::bernoulli_tensor_cuda_(/* actuals */ self, p, generator);
        break;
        default:
            AT_ERROR("bernoulli_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::bernoulli_(Tensor & self, double p, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::bernoulli_scalar_cuda_(/* actuals */ self, p, generator);
        break;
        default:
            AT_ERROR("bernoulli_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::bincount(const Tensor & self, const Tensor & weights, int64_t minlength) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_bincount_cuda(/* actuals */ self, weights, minlength);
        break;
        default:
            AT_ERROR("bincount not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::bmm(const Tensor & self, const Tensor & mat2) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::bmm_cuda(/* actuals */ self, mat2);
        break;
        default:
            AT_ERROR("bmm not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::bmm_out(Tensor & out, const Tensor & self, const Tensor & mat2) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::bmm_out_cuda(/* actuals */ out, self, mat2);
        break;
        default:
            AT_ERROR("bmm_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::ceil_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_ceil__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("ceil_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::ceil_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_ceil_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("ceil_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::clamp_(Tensor & self, c10::optional<Scalar> min, c10::optional<Scalar> max) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_clamp__cuda(/* actuals */ self, min, max);
        break;
        default:
            AT_ERROR("clamp_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::clamp_out(Tensor & out, const Tensor & self, c10::optional<Scalar> min, c10::optional<Scalar> max) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_clamp_out_cuda(/* actuals */ out, self, min, max);
        break;
        default:
            AT_ERROR("clamp_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::clamp_max_(Tensor & self, Scalar max) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_clamp_max__cuda(/* actuals */ self, max);
        break;
        default:
            AT_ERROR("clamp_max_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::clamp_max_out(Tensor & out, const Tensor & self, Scalar max) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_clamp_max_out_cuda(/* actuals */ out, self, max);
        break;
        default:
            AT_ERROR("clamp_max_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::clamp_min_(Tensor & self, Scalar min) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_clamp_min__cuda(/* actuals */ self, min);
        break;
        default:
            AT_ERROR("clamp_min_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::clamp_min_out(Tensor & out, const Tensor & self, Scalar min) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_clamp_min_out_cuda(/* actuals */ out, self, min);
        break;
        default:
            AT_ERROR("clamp_min_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool:
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_s_copy__cuda(/* actuals */ self, src, non_blocking);
        break;
        default:
            AT_ERROR("s_copy_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool:
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_s_copy_from_cuda(/* actuals */ self, dst, non_blocking);
        break;
        default:
            AT_ERROR("_s_copy_from not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::cos_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_cos__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("cos_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::cos_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_cos_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("cos_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::cosh_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_cosh__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("cosh_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::cosh_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_cosh_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("cosh_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::cudnn_affine_grid_generator(const Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) const {
    const OptionalDeviceGuard device_guard(device_of(theta));
    auto dispatch_scalar_type = infer_scalar_type(theta);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::cudnn_affine_grid_generator_forward(/* actuals */ theta, N, C, H, W);
        break;
        default:
            AT_ERROR("cudnn_affine_grid_generator not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::cudnn_affine_grid_generator_backward(const Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) const {
    const OptionalDeviceGuard device_guard(device_of(grad));
    auto dispatch_scalar_type = infer_scalar_type(grad);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::cudnn_affine_grid_generator_backward(/* actuals */ grad, N, C, H, W);
        break;
        default:
            AT_ERROR("cudnn_affine_grid_generator_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::cudnn_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    auto dispatch_scalar_type = infer_scalar_type(input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::cudnn_batch_norm(/* actuals */ input, weight, bias, running_mean, running_var, training, exponential_average_factor, epsilon);
        break;
        default:
            AT_ERROR("cudnn_batch_norm not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::cudnn_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    auto dispatch_scalar_type = infer_scalar_type(input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::cudnn_batch_norm_backward(/* actuals */ input, grad_output, weight, running_mean, running_var, save_mean, save_var, epsilon);
        break;
        default:
            AT_ERROR("cudnn_batch_norm_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::cudnn_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::cudnn_convolution(/* actuals */ self, weight, bias, padding, stride, dilation, groups, benchmark, deterministic);
        break;
        default:
            AT_ERROR("cudnn_convolution not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::cudnn_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::cudnn_convolution_backward_input(/* actuals */ self_size, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic);
        break;
        default:
            AT_ERROR("cudnn_convolution_backward_input not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::cudnn_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::cudnn_convolution_backward(/* actuals */ self, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic, output_mask);
        break;
        default:
            AT_ERROR("cudnn_convolution_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::cudnn_convolution_backward_bias(const Tensor & grad_output) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::cudnn_convolution_backward_bias(/* actuals */ grad_output);
        break;
        default:
            AT_ERROR("cudnn_convolution_backward_bias not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::cudnn_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::cudnn_convolution_backward_weight(/* actuals */ weight_size, grad_output, self, padding, stride, dilation, groups, benchmark, deterministic);
        break;
        default:
            AT_ERROR("cudnn_convolution_backward_weight not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::cudnn_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::cudnn_convolution_transpose(/* actuals */ self, weight, bias, padding, output_padding, stride, dilation, groups, benchmark, deterministic);
        break;
        default:
            AT_ERROR("cudnn_convolution_transpose not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::cudnn_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::cudnn_convolution_transpose_backward(/* actuals */ self, grad_output, weight, padding, output_padding, stride, dilation, groups, benchmark, deterministic, output_mask);
        break;
        default:
            AT_ERROR("cudnn_convolution_transpose_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::cudnn_convolution_transpose_backward_bias(const Tensor & grad_output) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::cudnn_convolution_backward_bias(/* actuals */ grad_output);
        break;
        default:
            AT_ERROR("cudnn_convolution_transpose_backward_bias not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::cudnn_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::cudnn_convolution_transpose_backward_input(/* actuals */ grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic);
        break;
        default:
            AT_ERROR("cudnn_convolution_transpose_backward_input not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::cudnn_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::cudnn_convolution_transpose_backward_weight(/* actuals */ weight_size, grad_output, self, padding, stride, dilation, groups, benchmark, deterministic);
        break;
        default:
            AT_ERROR("cudnn_convolution_transpose_backward_weight not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::cudnn_grid_sampler(const Tensor & self, const Tensor & grid) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::cudnn_grid_sampler_forward(/* actuals */ self, grid);
        break;
        default:
            AT_ERROR("cudnn_grid_sampler not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::cudnn_grid_sampler_backward(const Tensor & self, const Tensor & grid, const Tensor & grad_output) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::cudnn_grid_sampler_backward(/* actuals */ self, grid, grad_output);
        break;
        default:
            AT_ERROR("cudnn_grid_sampler_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool zero_infinity) const {
    const OptionalDeviceGuard device_guard(device_of(log_probs));
    auto dispatch_scalar_type = infer_scalar_type(log_probs);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::ctc_loss_gpu(/* actuals */ log_probs, targets, input_lengths, target_lengths, blank, zero_infinity);
        break;
        default:
            AT_ERROR("_ctc_loss not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_ctc_loss_backward(const Tensor & grad, const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, const Tensor & neg_log_likelihood, const Tensor & log_alpha, int64_t blank, bool zero_infinity) const {
    const OptionalDeviceGuard device_guard(device_of(grad));
    auto dispatch_scalar_type = infer_scalar_type(grad);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::ctc_loss_backward_gpu(/* actuals */ grad, log_probs, targets, input_lengths, target_lengths, neg_log_likelihood, log_alpha, blank, zero_infinity);
        break;
        default:
            AT_ERROR("_ctc_loss_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::embedding_dense_backward(const Tensor & grad_output, const Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::embedding_dense_backward_cuda(/* actuals */ grad_output, indices, num_weights, padding_idx, scale_grad_by_freq);
        break;
        default:
            AT_ERROR("embedding_dense_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::embedding_renorm_(Tensor & self, const Tensor & indices, double max_norm, double norm_type) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::embedding_renorm_cuda_(/* actuals */ self, indices, max_norm, norm_type);
        break;
        default:
            AT_ERROR("embedding_renorm_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor,Tensor> CUDAType::_embedding_bag(const Tensor & weight, const Tensor & indices, const Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const Tensor & per_sample_weights) const {
    const OptionalDeviceGuard device_guard(device_of(weight));
    auto dispatch_scalar_type = infer_scalar_type(weight);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_embedding_bag_cuda(/* actuals */ weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights);
        break;
        default:
            AT_ERROR("_embedding_bag not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_embedding_bag_dense_backward(const Tensor & grad, const Tensor & indices, const Tensor & offsets, const Tensor & offset2bag, const Tensor & bag_size, const Tensor & maximum_indices, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, const Tensor & per_sample_weights) const {
    const OptionalDeviceGuard device_guard(device_of(grad));
    auto dispatch_scalar_type = infer_scalar_type(grad);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_embedding_bag_dense_backward_cuda(/* actuals */ grad, indices, offsets, offset2bag, bag_size, maximum_indices, num_weights, scale_grad_by_freq, mode, per_sample_weights);
        break;
        default:
            AT_ERROR("_embedding_bag_dense_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_embedding_bag_per_sample_weights_backward(const Tensor & grad, const Tensor & weight, const Tensor & indices, const Tensor & offsets, const Tensor & offset2bag, int64_t mode) const {
    const OptionalDeviceGuard device_guard(device_of(grad));
    auto dispatch_scalar_type = infer_scalar_type(grad);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_embedding_bag_per_sample_weights_backward_cuda(/* actuals */ grad, weight, indices, offsets, offset2bag, mode);
        break;
        default:
            AT_ERROR("_embedding_bag_per_sample_weights_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::empty(IntArrayRef size, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    auto dispatch_scalar_type = typeMetaToScalarType(options.dtype());
    switch (dispatch_scalar_type) {
        case ScalarType::Bool:
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::empty_cuda(/* actuals */ size, options);
        break;
        default:
            AT_ERROR("empty not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::resize_(Tensor & self, IntArrayRef size) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool:
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::resize_cuda_(/* actuals */ self, size);
        break;
        default:
            AT_ERROR("resize_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::empty_strided(IntArrayRef size, IntArrayRef stride, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    auto dispatch_scalar_type = typeMetaToScalarType(options.dtype());
    switch (dispatch_scalar_type) {
        case ScalarType::Bool:
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::empty_strided_cuda(/* actuals */ size, stride, options);
        break;
        default:
            AT_ERROR("empty_strided not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::erf_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_erf__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("erf_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::erf_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_erf_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("erf_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::erfc_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_erfc__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("erfc_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::erfc_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_erfc_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("erfc_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::exp_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_exp__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("exp_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::exp_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_exp_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("exp_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::expm1_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_expm1__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("expm1_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::expm1_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_expm1_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("expm1_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::eye_out(Tensor & out, int64_t n) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    auto dispatch_scalar_type = infer_scalar_type(out);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::eye_out_cuda(/* actuals */ out, n);
        break;
        default:
            AT_ERROR("eye_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::eye_out(Tensor & out, int64_t n, int64_t m) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    auto dispatch_scalar_type = infer_scalar_type(out);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::eye_out_cuda(/* actuals */ out, n, m);
        break;
        default:
            AT_ERROR("eye_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::floor_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_floor__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("floor_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::floor_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_floor_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("floor_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::frac_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_frac__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("frac_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::frac_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_frac_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("frac_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::grid_sampler_2d(const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    auto dispatch_scalar_type = infer_scalar_type(input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::grid_sampler_2d_cuda(/* actuals */ input, grid, interpolation_mode, padding_mode);
        break;
        default:
            AT_ERROR("grid_sampler_2d not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::grid_sampler_2d_backward(const Tensor & grad_output, const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::grid_sampler_2d_backward_cuda(/* actuals */ grad_output, input, grid, interpolation_mode, padding_mode);
        break;
        default:
            AT_ERROR("grid_sampler_2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::grid_sampler_3d(const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    auto dispatch_scalar_type = infer_scalar_type(input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::grid_sampler_3d_cuda(/* actuals */ input, grid, interpolation_mode, padding_mode);
        break;
        default:
            AT_ERROR("grid_sampler_3d not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::grid_sampler_3d_backward(const Tensor & grad_output, const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::grid_sampler_3d_backward_cuda(/* actuals */ grad_output, input, grid, interpolation_mode, padding_mode);
        break;
        default:
            AT_ERROR("grid_sampler_3d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_fft_with_size(const Tensor & self, int64_t signal_ndim, bool complex_input, bool complex_output, bool inverse, IntArrayRef checked_signal_sizes, bool normalized, bool onesided, IntArrayRef output_sizes) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_fft_cufft(/* actuals */ self, signal_ndim, complex_input, complex_output, inverse, checked_signal_sizes, normalized, onesided, output_sizes);
        break;
        default:
            AT_ERROR("_fft_with_size not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_inverse_helper(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_inverse_helper_cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("_inverse_helper not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::kl_div_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::kl_div_backward_cuda(/* actuals */ grad_output, self, target, reduction);
        break;
        default:
            AT_ERROR("kl_div_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::kthvalue_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t k, int64_t dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::kthvalue_out_cuda(/* actuals */ values, indices, self, k, dim, keepdim);
        break;
        default:
            AT_ERROR("kthvalue_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::linspace_out(Tensor & out, Scalar start, Scalar end, int64_t steps) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    auto dispatch_scalar_type = infer_scalar_type(out);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::linspace_cuda_out(/* actuals */ out, start, end, steps);
        break;
        default:
            AT_ERROR("linspace_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::log_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_log__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("log_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::log_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_log_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("log_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::log10_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_log10__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("log10_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::log10_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_log10_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("log10_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::log1p_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_log1p__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("log1p_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::log1p_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_log1p_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("log1p_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::log2_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_log2__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("log2_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::log2_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_log2_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("log2_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::logspace_out(Tensor & out, Scalar start, Scalar end, int64_t steps, double base) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    auto dispatch_scalar_type = infer_scalar_type(out);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::logspace_cuda_out(/* actuals */ out, start, end, steps, base);
        break;
        default:
            AT_ERROR("logspace_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_log_softmax(const Tensor & self, int64_t dim, bool half_to_float) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::log_softmax_cuda(/* actuals */ self, dim, half_to_float);
        break;
        default:
            AT_ERROR("_log_softmax not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_log_softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::log_softmax_backward_cuda(/* actuals */ grad_output, output, dim, self);
        break;
        default:
            AT_ERROR("_log_softmax_backward_data not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::miopen_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    auto dispatch_scalar_type = infer_scalar_type(input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::miopen_batch_norm(/* actuals */ input, weight, bias, running_mean, running_var, training, exponential_average_factor, epsilon);
        break;
        default:
            AT_ERROR("miopen_batch_norm not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::miopen_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    auto dispatch_scalar_type = infer_scalar_type(input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::miopen_batch_norm_backward(/* actuals */ input, grad_output, weight, running_mean, running_var, save_mean, save_var, epsilon);
        break;
        default:
            AT_ERROR("miopen_batch_norm_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::miopen_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::miopen_convolution(/* actuals */ self, weight, bias, padding, stride, dilation, groups, benchmark, deterministic);
        break;
        default:
            AT_ERROR("miopen_convolution not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::miopen_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::miopen_convolution_backward_input(/* actuals */ self_size, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic);
        break;
        default:
            AT_ERROR("miopen_convolution_backward_input not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::miopen_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::miopen_convolution_backward(/* actuals */ self, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic, output_mask);
        break;
        default:
            AT_ERROR("miopen_convolution_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::miopen_convolution_backward_bias(const Tensor & grad_output) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::miopen_convolution_backward_bias(/* actuals */ grad_output);
        break;
        default:
            AT_ERROR("miopen_convolution_backward_bias not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::miopen_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::miopen_convolution_backward_weight(/* actuals */ weight_size, grad_output, self, padding, stride, dilation, groups, benchmark, deterministic);
        break;
        default:
            AT_ERROR("miopen_convolution_backward_weight not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::miopen_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::miopen_convolution_transpose(/* actuals */ self, weight, bias, padding, output_padding, stride, dilation, groups, benchmark, deterministic);
        break;
        default:
            AT_ERROR("miopen_convolution_transpose not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::miopen_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::miopen_convolution_transpose_backward(/* actuals */ self, grad_output, weight, padding, output_padding, stride, dilation, groups, benchmark, deterministic, output_mask);
        break;
        default:
            AT_ERROR("miopen_convolution_transpose_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::miopen_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::miopen_convolution_transpose_backward_input(/* actuals */ grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic);
        break;
        default:
            AT_ERROR("miopen_convolution_transpose_backward_input not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::miopen_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::miopen_convolution_transpose_backward_weight(/* actuals */ weight_size, grad_output, self, padding, stride, dilation, groups, benchmark, deterministic);
        break;
        default:
            AT_ERROR("miopen_convolution_transpose_backward_weight not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::miopen_depthwise_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::miopen_depthwise_convolution(/* actuals */ self, weight, bias, padding, stride, dilation, groups, benchmark, deterministic);
        break;
        default:
            AT_ERROR("miopen_depthwise_convolution not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::miopen_depthwise_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::miopen_depthwise_convolution_backward_input(/* actuals */ self_size, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic);
        break;
        default:
            AT_ERROR("miopen_depthwise_convolution_backward_input not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::miopen_depthwise_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::miopen_depthwise_convolution_backward(/* actuals */ self, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic, output_mask);
        break;
        default:
            AT_ERROR("miopen_depthwise_convolution_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::miopen_depthwise_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::miopen_depthwise_convolution_backward_weight(/* actuals */ weight_size, grad_output, self, padding, stride, dilation, groups, benchmark, deterministic);
        break;
        default:
            AT_ERROR("miopen_depthwise_convolution_backward_weight not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::narrow_copy_dense(/* actuals */ self, dim, start, length);
        break;
        default:
            AT_ERROR("narrow_copy not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::native_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    auto dispatch_scalar_type = infer_scalar_type(input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::batch_norm_cuda(/* actuals */ input, weight, bias, running_mean, running_var, training, momentum, eps);
        break;
        default:
            AT_ERROR("native_batch_norm not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::batch_norm_stats(const Tensor & input, double eps) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    auto dispatch_scalar_type = infer_scalar_type(input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::batch_norm_stats_cuda(/* actuals */ input, eps);
        break;
        default:
            AT_ERROR("batch_norm_stats not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::batch_norm_elemt(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & mean, const Tensor & invstd, double eps) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    auto dispatch_scalar_type = infer_scalar_type(input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::batch_norm_elemt_cuda(/* actuals */ input, weight, bias, mean, invstd, eps);
        break;
        default:
            AT_ERROR("batch_norm_elemt not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::batch_norm_gather_stats(const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & running_mean, const Tensor & running_var, double momentum, double eps, int64_t count) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    auto dispatch_scalar_type = infer_scalar_type(input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::batch_norm_gather_stats_cuda(/* actuals */ input, mean, invstd, running_mean, running_var, momentum, eps, count);
        break;
        default:
            AT_ERROR("batch_norm_gather_stats not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::native_batch_norm_backward(const Tensor & grad_out, const Tensor & input, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_invstd, bool train, double eps, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(grad_out));
    auto dispatch_scalar_type = infer_scalar_type(grad_out);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::batch_norm_backward_cuda(/* actuals */ grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
        break;
        default:
            AT_ERROR("native_batch_norm_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor,Tensor> CUDAType::batch_norm_backward_reduce(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, bool input_g, bool weight_g, bool bias_g) const {
    const OptionalDeviceGuard device_guard(device_of(grad_out));
    auto dispatch_scalar_type = infer_scalar_type(grad_out);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::batch_norm_backward_reduce_cuda(/* actuals */ grad_out, input, mean, invstd, input_g, weight_g, bias_g);
        break;
        default:
            AT_ERROR("batch_norm_backward_reduce not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::batch_norm_backward_elemt(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & weight, const Tensor & mean_dy, const Tensor & mean_dy_xmu) const {
    const OptionalDeviceGuard device_guard(device_of(grad_out));
    auto dispatch_scalar_type = infer_scalar_type(grad_out);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::batch_norm_backward_elemt_cuda(/* actuals */ grad_out, input, mean, invstd, weight, mean_dy, mean_dy_xmu);
        break;
        default:
            AT_ERROR("batch_norm_backward_elemt not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::batch_norm_update_stats(const Tensor & input, const Tensor & running_mean, const Tensor & running_var, double momentum) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    auto dispatch_scalar_type = infer_scalar_type(input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::batch_norm_update_stats_cuda(/* actuals */ input, running_mean, running_var, momentum);
        break;
        default:
            AT_ERROR("batch_norm_update_stats not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::randperm_out(Tensor & out, int64_t n, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    auto dispatch_scalar_type = infer_scalar_type(out);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::randperm_out_cuda(/* actuals */ out, n, generator);
        break;
        default:
            AT_ERROR("randperm_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::range_out(Tensor & out, Scalar start, Scalar end, Scalar step) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    auto dispatch_scalar_type = infer_scalar_type(out);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::range_cuda_out(/* actuals */ out, start, end, step);
        break;
        default:
            AT_ERROR("range_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::reciprocal_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_reciprocal__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("reciprocal_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::reciprocal_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_reciprocal_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("reciprocal_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::neg_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_neg__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("neg_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::neg_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_neg_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("neg_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::repeat_interleave(const Tensor & repeats) const {
    const OptionalDeviceGuard device_guard(device_of(repeats));
    auto dispatch_scalar_type = infer_scalar_type(repeats);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::repeat_interleave_cuda(/* actuals */ repeats);
        break;
        default:
            AT_ERROR("repeat_interleave not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::round_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_round__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("round_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::round_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_round_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("round_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::relu(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::relu(/* actuals */ self);
        break;
        default:
            AT_ERROR("relu not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::relu_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::relu_(/* actuals */ self);
        break;
        default:
            AT_ERROR("relu_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::prelu(const Tensor & self, const Tensor & weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::prelu_cuda(/* actuals */ self, weight);
        break;
        default:
            AT_ERROR("prelu not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::prelu_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::prelu_backward_cuda(/* actuals */ grad_output, self, weight);
        break;
        default:
            AT_ERROR("prelu_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::hardshrink(const Tensor & self, Scalar lambd) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::hardshrink_cuda(/* actuals */ self, lambd);
        break;
        default:
            AT_ERROR("hardshrink not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::hardshrink_backward(const Tensor & grad_out, const Tensor & self, Scalar lambd) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::hardshrink_backward_cuda(/* actuals */ grad_out, self, lambd);
        break;
        default:
            AT_ERROR("hardshrink_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::rsqrt_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_rsqrt__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("rsqrt_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::rsqrt_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_rsqrt_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("rsqrt_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::sigmoid_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_sigmoid__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("sigmoid_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::sigmoid_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_sigmoid_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("sigmoid_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::sin_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_sin__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("sin_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::sin_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_sin_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("sin_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::sinh_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_sinh__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("sinh_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::sinh_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_sinh_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("sinh_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_softmax(const Tensor & self, int64_t dim, bool half_to_float) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::softmax_cuda(/* actuals */ self, dim, half_to_float);
        break;
        default:
            AT_ERROR("_softmax not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::softmax_backward_cuda(/* actuals */ grad_output, output, dim, self);
        break;
        default:
            AT_ERROR("_softmax_backward_data not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::_sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::add_out_dense_sparse_cuda(/* actuals */ out, self, other, alpha);
        break;
        default:
            AT_ERROR("_sparse_dense_add_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_sspaddmm_out_only_sparse_cuda(/* actuals */ out, self, mat1, mat2, beta, alpha);
        break;
        default:
            AT_ERROR("sspaddmm_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::sqrt_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_sqrt__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("sqrt_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::sqrt_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_sqrt_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("sqrt_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::tan_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_tan__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("tan_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::tan_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_tan_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("tan_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::tanh_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_tanh__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("tanh_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::tanh_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_tanh_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("tanh_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::flip(const Tensor & self, IntArrayRef dims) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::flip_cuda(/* actuals */ self, dims);
        break;
        default:
            AT_ERROR("flip not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::roll(const Tensor & self, IntArrayRef shifts, IntArrayRef dims) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::roll_cuda(/* actuals */ self, shifts, dims);
        break;
        default:
            AT_ERROR("roll not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::trunc_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_trunc__cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("trunc_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::trunc_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_trunc_out_cuda(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("trunc_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_unique(const Tensor & self, bool sorted, bool return_inverse) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_unique_cuda(/* actuals */ self, sorted, return_inverse);
        break;
        default:
            AT_ERROR("_unique not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::unique_dim(const Tensor & self, int64_t dim, bool sorted, bool return_inverse, bool return_counts) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::unique_dim_cuda(/* actuals */ self, dim, sorted, return_inverse, return_counts);
        break;
        default:
            AT_ERROR("unique_dim not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::unique_consecutive(const Tensor & self, bool return_inverse, bool return_counts, c10::optional<int64_t> dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::unique_consecutive_cuda(/* actuals */ self, return_inverse, return_counts, dim);
        break;
        default:
            AT_ERROR("unique_consecutive not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::unique_dim_consecutive(const Tensor & self, int64_t dim, bool return_inverse, bool return_counts) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::unique_dim_consecutive_cuda(/* actuals */ self, dim, return_inverse, return_counts);
        break;
        default:
            AT_ERROR("unique_dim_consecutive not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::_unique2(const Tensor & self, bool sorted, bool return_inverse, bool return_counts) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_unique2_cuda(/* actuals */ self, sorted, return_inverse, return_counts);
        break;
        default:
            AT_ERROR("_unique2 not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_s_where(const Tensor & condition, const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_s_where_cuda(/* actuals */ condition, self, other);
        break;
        default:
            AT_ERROR("_s_where not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_weight_norm_cuda_interface(const Tensor & v, const Tensor & g, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(v));
    auto dispatch_scalar_type = infer_scalar_type(v);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::weight_norm_cuda(/* actuals */ v, g, dim);
        break;
        default:
            AT_ERROR("_weight_norm_cuda_interface not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_weight_norm_cuda_interface_backward(const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(grad_w));
    auto dispatch_scalar_type = infer_scalar_type(grad_w);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::weight_norm_cuda_backward(/* actuals */ grad_w, saved_v, saved_g, saved_norms, dim);
        break;
        default:
            AT_ERROR("_weight_norm_cuda_interface_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_standard_gamma_grad(const Tensor & self, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_standard_gamma_grad_cuda(/* actuals */ self, output);
        break;
        default:
            AT_ERROR("_standard_gamma_grad not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_standard_gamma(const Tensor & self, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_s_gamma_cuda(/* actuals */ self, generator);
        break;
        default:
            AT_ERROR("_standard_gamma not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_sample_dirichlet(const Tensor & self, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_s_dirichlet_cuda(/* actuals */ self, generator);
        break;
        default:
            AT_ERROR("_sample_dirichlet not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::poisson(const Tensor & self, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_s_poisson_cuda(/* actuals */ self, generator);
        break;
        default:
            AT_ERROR("poisson not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::clone(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool:
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::clone(/* actuals */ self);
        break;
        default:
            AT_ERROR("clone not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::resize_as_(Tensor & self, const Tensor & the_template) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool:
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::resize_as_(/* actuals */ self, the_template);
        break;
        default:
            AT_ERROR("resize_as_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::pow_out(Tensor & out, const Tensor & self, Scalar exponent) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::pow_out(/* actuals */ out, self, exponent);
        break;
        default:
            AT_ERROR("pow_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::pow(const Tensor & self, Scalar exponent) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::pow(/* actuals */ self, exponent);
        break;
        default:
            AT_ERROR("pow not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::zero_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool:
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::zero_(/* actuals */ self);
        break;
        default:
            AT_ERROR("zero_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::s_addmm_out_sparse_dense_cuda(/* actuals */ out, self, mat1, mat2, beta, alpha);
        break;
        default:
            AT_ERROR("s_native_addmm_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::s_native_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::s_addmm_sparse_dense_cuda(/* actuals */ self, mat1, mat2, beta, alpha);
        break;
        default:
            AT_ERROR("s_native_addmm not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::s_native_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::s_addmm_sparse_dense_cuda_(/* actuals */ self, mat1, mat2, beta, alpha);
        break;
        default:
            AT_ERROR("s_native_addmm_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::sparse_mask(const Tensor & self, SparseTensorRef mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::sparse_mask_cuda(/* actuals */ self, mask);
        break;
        default:
            AT_ERROR("sparse_mask not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::to_sparse(const Tensor & self, int64_t sparse_dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::dense_to_sparse(/* actuals */ self, sparse_dim);
        break;
        default:
            AT_ERROR("to_sparse not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::to_sparse(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::dense_to_sparse(/* actuals */ self);
        break;
        default:
            AT_ERROR("to_sparse not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Scalar CUDAType::_local_scalar_dense(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool:
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_local_scalar_dense_cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("_local_scalar_dense not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::_thnn_fused_lstm_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & cx, const Tensor & input_bias, const Tensor & hidden_bias) const {
    const OptionalDeviceGuard device_guard(device_of(input_gates));
    auto dispatch_scalar_type = infer_scalar_type(input_gates);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_thnn_fused_lstm_cell_cuda(/* actuals */ input_gates, hidden_gates, cx, input_bias, hidden_bias);
        break;
        default:
            AT_ERROR("_thnn_fused_lstm_cell not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CUDAType::_thnn_fused_lstm_cell_backward(const Tensor & grad_hy, const Tensor & grad_cy, const Tensor & cx, const Tensor & cy, const Tensor & workspace, bool has_bias) const {
    const OptionalDeviceGuard device_guard(device_of(cx));
    auto dispatch_scalar_type = infer_scalar_type(cx);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_thnn_fused_lstm_cell_backward_cuda(/* actuals */ grad_hy, grad_cy, cx, cy, workspace, has_bias);
        break;
        default:
            AT_ERROR("_thnn_fused_lstm_cell_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_thnn_fused_gru_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & hx, const Tensor & input_bias, const Tensor & hidden_bias) const {
    const OptionalDeviceGuard device_guard(device_of(input_gates));
    auto dispatch_scalar_type = infer_scalar_type(input_gates);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_thnn_fused_gru_cell_cuda(/* actuals */ input_gates, hidden_gates, hx, input_bias, hidden_bias);
        break;
        default:
            AT_ERROR("_thnn_fused_gru_cell not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CUDAType::_thnn_fused_gru_cell_backward(const Tensor & grad_hy, const Tensor & workspace, bool has_bias) const {
    const OptionalDeviceGuard device_guard(device_of(grad_hy));
    auto dispatch_scalar_type = infer_scalar_type(grad_hy);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_thnn_fused_gru_cell_backward_cuda(/* actuals */ grad_hy, workspace, has_bias);
        break;
        default:
            AT_ERROR("_thnn_fused_gru_cell_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::tril_(Tensor & self, int64_t diagonal) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::tril_cuda_(/* actuals */ self, diagonal);
        break;
        default:
            AT_ERROR("tril_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::triu_(Tensor & self, int64_t diagonal) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::triu_cuda_(/* actuals */ self, diagonal);
        break;
        default:
            AT_ERROR("triu_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::lerp_(Tensor & self, const Tensor & end, Scalar weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::lerp_cuda_scalar_(/* actuals */ self, end, weight);
        break;
        default:
            AT_ERROR("lerp_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::lerp_(Tensor & self, const Tensor & end, const Tensor & weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::lerp_cuda_tensor_(/* actuals */ self, end, weight);
        break;
        default:
            AT_ERROR("lerp_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::uniform_(Tensor & self, double from, double to, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::uniform_cuda_(/* actuals */ self, from, to, generator);
        break;
        default:
            AT_ERROR("uniform_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::triu_out(Tensor & out, const Tensor & self, int64_t diagonal) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::triu_cuda_out(/* actuals */ out, self, diagonal);
        break;
        default:
            AT_ERROR("triu_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::tril_out(Tensor & out, const Tensor & self, int64_t diagonal) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::tril_cuda_out(/* actuals */ out, self, diagonal);
        break;
        default:
            AT_ERROR("tril_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::tril_indices(int64_t row, int64_t col, int64_t offset, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    auto dispatch_scalar_type = typeMetaToScalarType(options.dtype());
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::tril_indices_cuda(/* actuals */ row, col, offset, options);
        break;
        default:
            AT_ERROR("tril_indices not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::triu_indices(int64_t row, int64_t col, int64_t offset, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    auto dispatch_scalar_type = typeMetaToScalarType(options.dtype());
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::triu_indices_cuda(/* actuals */ row, col, offset, options);
        break;
        default:
            AT_ERROR("triu_indices not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_triangular_solve_helper(const Tensor & self, const Tensor & A, bool upper, bool transpose, bool unitriangular) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_triangular_solve_helper_cuda(/* actuals */ self, A, upper, transpose, unitriangular);
        break;
        default:
            AT_ERROR("_triangular_solve_helper not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_cholesky_helper(const Tensor & self, bool upper) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_cholesky_helper_cuda(/* actuals */ self, upper);
        break;
        default:
            AT_ERROR("_cholesky_helper not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_cholesky_solve_helper(const Tensor & self, const Tensor & A, bool upper) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_cholesky_solve_helper_cuda(/* actuals */ self, A, upper);
        break;
        default:
            AT_ERROR("_cholesky_solve_helper not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::_solve_helper(const Tensor & self, const Tensor & A) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_solve_helper_cuda(/* actuals */ self, A);
        break;
        default:
            AT_ERROR("_solve_helper not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> CUDAType::_lu_with_info(const Tensor & self, bool pivot, bool check_errors) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_lu_with_info_cuda(/* actuals */ self, pivot, check_errors);
        break;
        default:
            AT_ERROR("_lu_with_info not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::lerp_out(Tensor & out, const Tensor & self, const Tensor & end, Scalar weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::lerp_cuda_scalar_out(/* actuals */ out, self, end, weight);
        break;
        default:
            AT_ERROR("lerp_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::lerp_out(Tensor & out, const Tensor & self, const Tensor & end, const Tensor & weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::lerp_cuda_tensor_out(/* actuals */ out, self, end, weight);
        break;
        default:
            AT_ERROR("lerp_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::lerp(const Tensor & self, const Tensor & end, Scalar weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::lerp_cuda_scalar(/* actuals */ self, end, weight);
        break;
        default:
            AT_ERROR("lerp not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::lerp(const Tensor & self, const Tensor & end, const Tensor & weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::lerp_cuda_tensor(/* actuals */ self, end, weight);
        break;
        default:
            AT_ERROR("lerp not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::histc_out(Tensor & out, const Tensor & self, int64_t bins, Scalar min, Scalar max) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_histc_out_cuda(/* actuals */ out, self, bins, min, max);
        break;
        default:
            AT_ERROR("histc_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::histc(const Tensor & self, int64_t bins, Scalar min, Scalar max) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_histc_cuda(/* actuals */ self, bins, min, max);
        break;
        default:
            AT_ERROR("histc not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::median(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::median_cuda(/* actuals */ self);
        break;
        default:
            AT_ERROR("median not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::adaptive_avg_pool2d_out(Tensor & out, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_avg_pool2d_out_cuda(/* actuals */ out, self, output_size);
        break;
        default:
            AT_ERROR("adaptive_avg_pool2d_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_adaptive_avg_pool2d(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_avg_pool2d_cuda(/* actuals */ self, output_size);
        break;
        default:
            AT_ERROR("_adaptive_avg_pool2d not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::_adaptive_avg_pool2d_backward(const Tensor & grad_output, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_avg_pool2d_backward_cuda(/* actuals */ grad_output, self);
        break;
        default:
            AT_ERROR("_adaptive_avg_pool2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::adaptive_max_pool2d_out(Tensor & out, Tensor & indices, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_max_pool2d_out_cuda(/* actuals */ out, indices, self, output_size);
        break;
        default:
            AT_ERROR("adaptive_max_pool2d_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::adaptive_max_pool2d(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_max_pool2d_cuda(/* actuals */ self, output_size);
        break;
        default:
            AT_ERROR("adaptive_max_pool2d not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::adaptive_max_pool2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_max_pool2d_backward_out_cuda(/* actuals */ grad_input, grad_output, self, indices);
        break;
        default:
            AT_ERROR("adaptive_max_pool2d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::adaptive_max_pool2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_max_pool2d_backward_cuda(/* actuals */ grad_output, self, indices);
        break;
        default:
            AT_ERROR("adaptive_max_pool2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::adaptive_max_pool3d_out(Tensor & out, Tensor & indices, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_max_pool3d_out_cuda(/* actuals */ out, indices, self, output_size);
        break;
        default:
            AT_ERROR("adaptive_max_pool3d_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::adaptive_max_pool3d(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_max_pool3d_cuda(/* actuals */ self, output_size);
        break;
        default:
            AT_ERROR("adaptive_max_pool3d not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::adaptive_max_pool3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_max_pool3d_backward_out_cuda(/* actuals */ grad_input, grad_output, self, indices);
        break;
        default:
            AT_ERROR("adaptive_max_pool3d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::adaptive_max_pool3d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_max_pool3d_backward_cuda(/* actuals */ grad_output, self, indices);
        break;
        default:
            AT_ERROR("adaptive_max_pool3d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::avg_pool2d_out(Tensor & out, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::avg_pool2d_out(/* actuals */ out, self, kernel_size, stride, padding, ceil_mode, count_include_pad);
        break;
        default:
            AT_ERROR("avg_pool2d_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::avg_pool2d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::avg_pool2d(/* actuals */ self, kernel_size, stride, padding, ceil_mode, count_include_pad);
        break;
        default:
            AT_ERROR("avg_pool2d not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::fractional_max_pool2d_out(Tensor & output, Tensor & indices, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & random_samples) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::fractional_max_pool2d_out_cuda(/* actuals */ output, indices, self, kernel_size, output_size, random_samples);
        break;
        default:
            AT_ERROR("fractional_max_pool2d_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::fractional_max_pool2d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & random_samples) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::fractional_max_pool2d_cuda(/* actuals */ self, kernel_size, output_size, random_samples);
        break;
        default:
            AT_ERROR("fractional_max_pool2d not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::fractional_max_pool2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::fractional_max_pool2d_backward_out_cuda(/* actuals */ grad_input, grad_output, self, kernel_size, output_size, indices);
        break;
        default:
            AT_ERROR("fractional_max_pool2d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::fractional_max_pool2d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::fractional_max_pool2d_backward_cuda(/* actuals */ grad_output, self, kernel_size, output_size, indices);
        break;
        default:
            AT_ERROR("fractional_max_pool2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> CUDAType::fractional_max_pool3d_out(Tensor & output, Tensor & indices, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & random_samples) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::fractional_max_pool3d_out_cuda(/* actuals */ output, indices, self, kernel_size, output_size, random_samples);
        break;
        default:
            AT_ERROR("fractional_max_pool3d_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> CUDAType::fractional_max_pool3d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & random_samples) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::fractional_max_pool3d_cuda(/* actuals */ self, kernel_size, output_size, random_samples);
        break;
        default:
            AT_ERROR("fractional_max_pool3d not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::fractional_max_pool3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::fractional_max_pool3d_backward_out_cuda(/* actuals */ grad_input, grad_output, self, kernel_size, output_size, indices);
        break;
        default:
            AT_ERROR("fractional_max_pool3d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::fractional_max_pool3d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::fractional_max_pool3d_backward_cuda(/* actuals */ grad_output, self, kernel_size, output_size, indices);
        break;
        default:
            AT_ERROR("fractional_max_pool3d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::reflection_pad1d_out(Tensor & out, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::reflection_pad1d_out_cuda(/* actuals */ out, self, padding);
        break;
        default:
            AT_ERROR("reflection_pad1d_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::reflection_pad1d(const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::reflection_pad1d_cuda(/* actuals */ self, padding);
        break;
        default:
            AT_ERROR("reflection_pad1d not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::reflection_pad1d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::reflection_pad1d_backward_out_cuda(/* actuals */ grad_input, grad_output, self, padding);
        break;
        default:
            AT_ERROR("reflection_pad1d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::reflection_pad1d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::reflection_pad1d_backward_cuda(/* actuals */ grad_output, self, padding);
        break;
        default:
            AT_ERROR("reflection_pad1d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::reflection_pad2d_out(Tensor & out, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::reflection_pad2d_out_cuda(/* actuals */ out, self, padding);
        break;
        default:
            AT_ERROR("reflection_pad2d_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::reflection_pad2d(const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::reflection_pad2d_cuda(/* actuals */ self, padding);
        break;
        default:
            AT_ERROR("reflection_pad2d not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::reflection_pad2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::reflection_pad2d_backward_out_cuda(/* actuals */ grad_input, grad_output, self, padding);
        break;
        default:
            AT_ERROR("reflection_pad2d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::reflection_pad2d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::reflection_pad2d_backward_cuda(/* actuals */ grad_output, self, padding);
        break;
        default:
            AT_ERROR("reflection_pad2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::replication_pad1d_out(Tensor & out, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad1d_out_cuda(/* actuals */ out, self, padding);
        break;
        default:
            AT_ERROR("replication_pad1d_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::replication_pad1d(const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad1d_cuda(/* actuals */ self, padding);
        break;
        default:
            AT_ERROR("replication_pad1d not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::replication_pad1d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad1d_backward_out_cuda(/* actuals */ grad_input, grad_output, self, padding);
        break;
        default:
            AT_ERROR("replication_pad1d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::replication_pad1d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad1d_backward_cuda(/* actuals */ grad_output, self, padding);
        break;
        default:
            AT_ERROR("replication_pad1d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::replication_pad2d_out(Tensor & out, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad2d_out_cuda(/* actuals */ out, self, padding);
        break;
        default:
            AT_ERROR("replication_pad2d_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::replication_pad2d(const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad2d_cuda(/* actuals */ self, padding);
        break;
        default:
            AT_ERROR("replication_pad2d not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::replication_pad2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad2d_backward_out_cuda(/* actuals */ grad_input, grad_output, self, padding);
        break;
        default:
            AT_ERROR("replication_pad2d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::replication_pad2d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad2d_backward_cuda(/* actuals */ grad_output, self, padding);
        break;
        default:
            AT_ERROR("replication_pad2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::replication_pad3d_out(Tensor & out, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad3d_out_cuda(/* actuals */ out, self, padding);
        break;
        default:
            AT_ERROR("replication_pad3d_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::replication_pad3d(const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad3d_cuda(/* actuals */ self, padding);
        break;
        default:
            AT_ERROR("replication_pad3d not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::replication_pad3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad3d_backward_out_cuda(/* actuals */ grad_input, grad_output, self, padding);
        break;
        default:
            AT_ERROR("replication_pad3d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::replication_pad3d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad3d_backward_cuda(/* actuals */ grad_output, self, padding);
        break;
        default:
            AT_ERROR("replication_pad3d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::upsample_linear1d_out(Tensor & out, const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_linear1d_out_cuda(/* actuals */ out, self, output_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_linear1d_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::upsample_linear1d(const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_linear1d_cuda(/* actuals */ self, output_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_linear1d not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::upsample_linear1d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_linear1d_backward_out_cuda(/* actuals */ grad_input, grad_output, output_size, input_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_linear1d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::upsample_linear1d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_linear1d_backward_cuda(/* actuals */ grad_output, output_size, input_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_linear1d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::upsample_bilinear2d_out(Tensor & out, const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_bilinear2d_out_cuda(/* actuals */ out, self, output_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_bilinear2d_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::upsample_bilinear2d(const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_bilinear2d_cuda(/* actuals */ self, output_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_bilinear2d not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::upsample_bilinear2d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_bilinear2d_backward_out_cuda(/* actuals */ grad_input, grad_output, output_size, input_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_bilinear2d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::upsample_bilinear2d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_bilinear2d_backward_cuda(/* actuals */ grad_output, output_size, input_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_bilinear2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::upsample_bicubic2d_out(Tensor & out, const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_bicubic2d_out_cuda(/* actuals */ out, self, output_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_bicubic2d_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::upsample_bicubic2d(const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_bicubic2d_cuda(/* actuals */ self, output_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_bicubic2d not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::upsample_bicubic2d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_bicubic2d_backward_out_cuda(/* actuals */ grad_input, grad_output, output_size, input_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_bicubic2d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::upsample_bicubic2d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_bicubic2d_backward_cuda(/* actuals */ grad_output, output_size, input_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_bicubic2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::upsample_trilinear3d_out(Tensor & out, const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_trilinear3d_out_cuda(/* actuals */ out, self, output_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_trilinear3d_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::upsample_trilinear3d(const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_trilinear3d_cuda(/* actuals */ self, output_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_trilinear3d not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::upsample_trilinear3d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_trilinear3d_backward_out_cuda(/* actuals */ grad_input, grad_output, output_size, input_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_trilinear3d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::upsample_trilinear3d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_trilinear3d_backward_cuda(/* actuals */ grad_output, output_size, input_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_trilinear3d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::upsample_nearest1d_out(Tensor & out, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest1d_out_cuda(/* actuals */ out, self, output_size);
        break;
        default:
            AT_ERROR("upsample_nearest1d_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::upsample_nearest1d(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest1d_cuda(/* actuals */ self, output_size);
        break;
        default:
            AT_ERROR("upsample_nearest1d not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::upsample_nearest1d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest1d_backward_out_cuda(/* actuals */ grad_input, grad_output, output_size, input_size);
        break;
        default:
            AT_ERROR("upsample_nearest1d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::upsample_nearest1d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest1d_backward_cuda(/* actuals */ grad_output, output_size, input_size);
        break;
        default:
            AT_ERROR("upsample_nearest1d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::upsample_nearest2d_out(Tensor & out, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest2d_out_cuda(/* actuals */ out, self, output_size);
        break;
        default:
            AT_ERROR("upsample_nearest2d_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::upsample_nearest2d(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest2d_cuda(/* actuals */ self, output_size);
        break;
        default:
            AT_ERROR("upsample_nearest2d not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::upsample_nearest2d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest2d_backward_out_cuda(/* actuals */ grad_input, grad_output, output_size, input_size);
        break;
        default:
            AT_ERROR("upsample_nearest2d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::upsample_nearest2d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest2d_backward_cuda(/* actuals */ grad_output, output_size, input_size);
        break;
        default:
            AT_ERROR("upsample_nearest2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::upsample_nearest3d_out(Tensor & out, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest3d_out_cuda(/* actuals */ out, self, output_size);
        break;
        default:
            AT_ERROR("upsample_nearest3d_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::upsample_nearest3d(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest3d_cuda(/* actuals */ self, output_size);
        break;
        default:
            AT_ERROR("upsample_nearest3d not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & CUDAType::upsample_nearest3d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest3d_backward_out_cuda(/* actuals */ grad_input, grad_output, output_size, input_size);
        break;
        default:
            AT_ERROR("upsample_nearest3d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor CUDAType::upsample_nearest3d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest3d_backward_cuda(/* actuals */ grad_output, output_size, input_size);
        break;
        default:
            AT_ERROR("upsample_nearest3d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}

}
