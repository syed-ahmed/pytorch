// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#define __STDC_FORMAT_MACROS

#include <ATen/CPULongType.h>

// @generated by aten/src/ATen/gen.py

#include <TH/TH.h>
#include <TH/THTensor.hpp>
#include <THNN/THNN.h>
#undef THNN_
#include <c10/core/TensorImpl.h>
#include <ATen/CPUGenerator.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NativeFunctions.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <c10/util/Half.h>
#include <c10/core/TensorImpl.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>


namespace at {

CPULongType::CPULongType()
  : CPUTypeDefault(CPUTensorId(), /*is_variable=*/false, /*is_undefined=*/false) {}

ScalarType CPULongType::scalarType() const {
  return ScalarType::Long;
}

caffe2::TypeMeta CPULongType::typeMeta() const {
    return caffe2::TypeMeta::Make<int64_t>();
}

Backend CPULongType::backend() const {
  return Backend::CPU;
}

const char * CPULongType::toString() const {
  return "CPULongType";
}

TypeID CPULongType::ID() const {
  return TypeID::CPULong;
}

size_t CPULongType::elementSizeInBytes() const {
  return sizeof(int64_t);
}

/* example
Tensor * CPULongType::add(Tensor & a, Tensor & b) {
  std::cout << "add CPULongTensor\n";
  return &a;
}
*/

Tensor & CPULongType::_th_set_(Tensor & self, Storage source) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto source_ = checked_storage(source,"source",2, DeviceType::CPU, at::scalarTypeToDataType(ScalarType::Long));
    THLongTensor_setStorage(self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
    self_->maybe_zero_dim(false);
    return self;
}
Tensor & CPULongType::_th_set_(Tensor & self, Storage source, int64_t storage_offset, IntArrayRef size, IntArrayRef stride) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto source_ = checked_storage(source,"source",2, DeviceType::CPU, at::scalarTypeToDataType(ScalarType::Long));
    THLongTensor_setStorage(self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
    self_->maybe_zero_dim(size.size() == 0);
    return self;
}
Tensor & CPULongType::_th_set_(Tensor & self, const Tensor & source) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto source_ = checked_tensor_unwrap(source,"source",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_set(self_, source_);
    self_->maybe_zero_dim(source_->dim() == 0);
    return self;
}
Tensor & CPULongType::_th_set_(Tensor & self) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    THLongTensor_setStorage(self_, NULL, 0, {0}, {});
    self_->maybe_zero_dim(false);
    return self;
}
Tensor & CPULongType::_th_fill_(Tensor & self, Scalar value) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto value_ = value.toLong();
    THLongTensor_fill(self_, value_);
    return self;
}
Tensor & CPULongType::_th_fill_(Tensor & self, const Tensor & value) const {
    // DeviceGuard omitted
    if (value.dim() == 0) {
        return static_cast<const TypeExtendedInterface*>(this)->_th_fill_(self, value.item());
    }
    AT_ERROR("_th_fill_ only supports a 0-dimensional value tensor, but got tensor "
        "with ", value.dim(), " dimension(s).");
}
bool CPULongType::_th_is_set_to(const Tensor & self, const Tensor & tensor) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto tensor_ = checked_tensor_unwrap(tensor,"tensor",2, false, Backend::CPU, ScalarType::Long);
    return THLongTensor_isSetTo(self_, tensor_);
}
Tensor & CPULongType::s__th_masked_fill_(Tensor & self, const Tensor & mask, Scalar value) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CPU, ScalarType::Byte);
    auto value_ = value.toLong();
    THLongTensor_maskedFill(self_, mask_, value_);
    return self;
}
Tensor & CPULongType::s__th_masked_fill_(Tensor & self, const Tensor & mask, const Tensor & value) const {
    // DeviceGuard omitted
    if (value.dim() == 0) {
        return static_cast<const TypeExtendedInterface*>(this)->_th_masked_fill_(self, mask, value.item());
    }
    AT_ERROR("_th_masked_fill_ only supports a 0-dimensional value tensor, but got tensor "
        "with ", value.dim(), " dimension(s).");
}
Tensor & CPULongType::s__th_masked_scatter_(Tensor & self, const Tensor & mask, const Tensor & source) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CPU, ScalarType::Byte);
    auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CPU, ScalarType::Long);
    THLongTensor_maskedCopy(self_, mask_, source_);
    return self;
}
Tensor & CPULongType::s__th_masked_select_out(Tensor & result, const Tensor & self, const Tensor & mask) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CPU, ScalarType::Byte);
    THLongTensor_maskedSelect(result_, self_, mask_);
    result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_masked_select(const Tensor & self, const Tensor & mask) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CPU, ScalarType::Byte);
    THLongTensor_maskedSelect(result_, self_, mask_);
    result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_nonzero_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    THLongTensor_nonzero(result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_nonzero(const Tensor & self) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), scalarTypeToTypeMeta(ScalarType::Long), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    THLongTensor_nonzero(result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_clone(const Tensor & self) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THLongTensor_newClone(self_))->maybe_zero_dim(self_->dim() == 0)));
}
Tensor CPULongType::_th_view(const Tensor & self, IntArrayRef size) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THLongTensor_newView(self_, size))->maybe_zero_dim(size.size() == 0)));
}
Tensor & CPULongType::_th_resize_as_(Tensor & self, const Tensor & the_template) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto the_template_ = checked_tensor_unwrap(the_template,"the_template",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_resizeAs(self_, the_template_);
    self_->maybe_zero_dim(the_template_->dim() == 0);
    return self;
}
Tensor & CPULongType::_th_index_select_out(Tensor & result, const Tensor & self, int64_t dim, const Tensor & index) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
    THLongTensor_indexSelect(result_, self_, dim, index_);
    result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_index_select(const Tensor & self, int64_t dim, const Tensor & index) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
    THLongTensor_indexSelect(result_, self_, dim, index_);
    result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_index_copy_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
    auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CPU, ScalarType::Long);
    THLongTensor_indexCopy(self_, dim, index_, source_);
    return self;
}
Tensor & CPULongType::_th_take_out(Tensor & result, const Tensor & self, const Tensor & index) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_take(result_, self_, index_);
    result_->maybe_zero_dim(index_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_take(const Tensor & self, const Tensor & index) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_take(result_, self_, index_);
    result_->maybe_zero_dim(index_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_put_(Tensor & self, const Tensor & index, const Tensor & source, bool accumulate) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CPU, ScalarType::Long);
    auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CPU, ScalarType::Long);
    THLongTensor_put(self_, index_, source_, accumulate);
    return self;
}
Tensor & CPULongType::_th_index_add_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
    auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CPU, ScalarType::Long);
    THLongTensor_indexAdd(self_, dim, index_, source_);
    return self;
}
Tensor & CPULongType::_th_index_fill_(Tensor & self, int64_t dim, const Tensor & index, Scalar value) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
    auto value_ = value.toLong();
    THLongTensor_indexFill(self_, dim, index_, value_);
    return self;
}
Tensor & CPULongType::_th_index_fill_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & value) const {
    // DeviceGuard omitted
    if (value.dim() == 0) {
        return static_cast<const TypeExtendedInterface*>(this)->_th_index_fill_(self, dim, index, value.item());
    }
    AT_ERROR("_th_index_fill_ only supports a 0-dimensional value tensor, but got tensor "
        "with ", value.dim(), " dimension(s).");
}
Tensor & CPULongType::_th_unfold_out(Tensor & result, const Tensor & self, int64_t dimension, int64_t size, int64_t step) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dimension = maybe_wrap_dim(dimension, self_);
    THLongTensor_unfold(result_, self_, dimension, size, step);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_unfold(const Tensor & self, int64_t dimension, int64_t size, int64_t step) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dimension = maybe_wrap_dim(dimension, self_);
    THLongTensor_unfold(result_, self_, dimension, size, step);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_scatter_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) const {
    // DeviceGuard omitted
    if (src.dim() == 0) {
        return static_cast<const TypeExtendedInterface*>(this)->_th_scatter_(self, dim, index, src.item());
    }
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
    auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CPU, ScalarType::Long);
    THLongTensor_scatter(self_, dim, index_, src_);
    return self;
}
Tensor & CPULongType::_th_scatter_(Tensor & self, int64_t dim, const Tensor & index, Scalar value) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
    auto value_ = value.toLong();
    THLongTensor_scatterFill(self_, dim, index_, value_);
    return self;
}
Tensor & CPULongType::_th_scatter_add_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
    auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CPU, ScalarType::Long);
    THLongTensor_scatterAdd(self_, dim, index_, src_);
    return self;
}
Tensor & CPULongType::_th_gather_out(Tensor & result, const Tensor & self, int64_t dim, const Tensor & index) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    result.resize_(index.sizes());
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
    THLongTensor_gather(result_, self_, dim, index_);
    result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_gather(const Tensor & self, int64_t dim, const Tensor & index) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    result.resize_(index.sizes());
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
    THLongTensor_gather(result_, self_, dim, index_);
    result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
    return result;
}
bool CPULongType::_th_equal(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    return THLongTensor_equal(self_, other_);
}
Tensor & CPULongType::_th_and_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_bitand(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_and(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_bitand(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::s__th_and_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cbitand(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_and(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cbitand(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_iand_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_bitand(self_, self_, other_);
    return self;
}
Tensor & CPULongType::s__th_iand_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cbitand(self_, self_, other_);
    return self;
}
Tensor & CPULongType::_th_or_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_bitor(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_or(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_bitor(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::s__th_or_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cbitor(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_or(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cbitor(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_ior_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_bitor(self_, self_, other_);
    return self;
}
Tensor & CPULongType::s__th_ior_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cbitor(self_, self_, other_);
    return self;
}
Tensor & CPULongType::_th_xor_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_bitxor(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_xor(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_bitxor(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::s__th_xor_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cbitxor(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_xor(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cbitxor(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_ixor_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_bitxor(self_, self_, other_);
    return self;
}
Tensor & CPULongType::s__th_ixor_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cbitxor(self_, self_, other_);
    return self;
}
Tensor & CPULongType::_th_lshift_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_lshift(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_lshift(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_lshift(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::s__th_lshift_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_clshift(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_lshift(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_clshift(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_ilshift_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_lshift(self_, self_, other_);
    return self;
}
Tensor & CPULongType::s__th_ilshift_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
    THLongTensor_clshift(self_, self_, other_);
    return self;
}
Tensor & CPULongType::_th_rshift_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_rshift(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_rshift(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_rshift(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::s__th_rshift_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_crshift(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_rshift(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_crshift(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_irshift_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_rshift(self_, self_, other_);
    return self;
}
Tensor & CPULongType::s__th_irshift_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
    THLongTensor_crshift(self_, self_, other_);
    return self;
}
Tensor & CPULongType::_th_lt_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_ltValue(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_lt(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_ltValue(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::s__th_lt_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_ltTensor(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_lt(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_ltTensor(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_lt_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_ltValueT(self_, self_, other_);
    return self;
}
Tensor & CPULongType::s__th_lt_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
    THLongTensor_ltTensorT(self_, self_, other_);
    return self;
}
Tensor & CPULongType::_th_gt_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_gtValue(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_gt(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_gtValue(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::s__th_gt_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_gtTensor(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_gt(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_gtTensor(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_gt_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_gtValueT(self_, self_, other_);
    return self;
}
Tensor & CPULongType::s__th_gt_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
    THLongTensor_gtTensorT(self_, self_, other_);
    return self;
}
Tensor & CPULongType::_th_le_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_leValue(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_le(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_leValue(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::s__th_le_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_leTensor(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_le(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_leTensor(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_le_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_leValueT(self_, self_, other_);
    return self;
}
Tensor & CPULongType::s__th_le_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
    THLongTensor_leTensorT(self_, self_, other_);
    return self;
}
Tensor & CPULongType::_th_ge_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_geValue(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_ge(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_geValue(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::s__th_ge_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_geTensor(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_ge(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_geTensor(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_ge_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_geValueT(self_, self_, other_);
    return self;
}
Tensor & CPULongType::s__th_ge_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
    THLongTensor_geTensorT(self_, self_, other_);
    return self;
}
Tensor & CPULongType::_th_eq_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_eqValue(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_eq(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_eqValue(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::s__th_eq_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_eqTensor(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_eq(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_eqTensor(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_eq_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_eqValueT(self_, self_, other_);
    return self;
}
Tensor & CPULongType::s__th_eq_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
    THLongTensor_eqTensorT(self_, self_, other_);
    return self;
}
Tensor & CPULongType::_th_ne_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_neValue(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_ne(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_neValue(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::s__th_ne_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_neTensor(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_ne(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_neTensor(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_ne_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_neValueT(self_, self_, other_);
    return self;
}
Tensor & CPULongType::s__th_ne_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
    THLongTensor_neTensorT(self_, self_, other_);
    return self;
}
Tensor & CPULongType::s__th_min_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cmin(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_min(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cmin(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_min(const Tensor & self) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    return at::scalar_tensor(convert<int64_t>(THLongTensor_minall(self_)), options());
}
std::tuple<Tensor &,Tensor &> CPULongType::_th_min_out(Tensor & min, Tensor & min_indices, const Tensor & self, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto min_ = checked_tensor_unwrap(min,"min",0, false, Backend::CPU, ScalarType::Long);
    auto min_indices_ = checked_tensor_unwrap(min_indices,"min_indices",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    THLongTensor_min(min_, min_indices_, self_, dim, keepdim);
    bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
    min_->maybe_zero_dim(maybe_scalar);
    min_indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &>(min, min_indices);
}
std::tuple<Tensor,Tensor> CPULongType::_th_min(const Tensor & self, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
    auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), scalarTypeToTypeMeta(ScalarType::Long), allocator(), false).release();
    auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    THLongTensor_min(min_, min_indices_, self_, dim, keepdim);
    bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
    min_->maybe_zero_dim(maybe_scalar);
    min_indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor>(min, min_indices);
}
Tensor & CPULongType::s__th_max_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cmax(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_max(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cmax(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_max(const Tensor & self) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    return at::scalar_tensor(convert<int64_t>(THLongTensor_maxall(self_)), options());
}
std::tuple<Tensor &,Tensor &> CPULongType::_th_max_out(Tensor & max, Tensor & max_indices, const Tensor & self, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto max_ = checked_tensor_unwrap(max,"max",0, false, Backend::CPU, ScalarType::Long);
    auto max_indices_ = checked_tensor_unwrap(max_indices,"max_indices",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    THLongTensor_max(max_, max_indices_, self_, dim, keepdim);
    bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
    max_->maybe_zero_dim(maybe_scalar);
    max_indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &>(max, max_indices);
}
std::tuple<Tensor,Tensor> CPULongType::_th_max(const Tensor & self, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
    auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), scalarTypeToTypeMeta(ScalarType::Long), allocator(), false).release();
    auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    THLongTensor_max(max_, max_indices_, self_, dim, keepdim);
    bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
    max_->maybe_zero_dim(maybe_scalar);
    max_indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor>(max, max_indices);
}
std::tuple<Tensor &,Tensor &> CPULongType::_th_kthvalue_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t k, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CPU, ScalarType::Long);
    auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    THLongTensor_kthvalue(values_, indices_, self_, k, dim, keepdim);
    bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
    values_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &>(values, indices);
}
std::tuple<Tensor,Tensor> CPULongType::_th_kthvalue(const Tensor & self, int64_t k, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
    auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), scalarTypeToTypeMeta(ScalarType::Long), allocator(), false).release();
    auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    THLongTensor_kthvalue(values_, indices_, self_, k, dim, keepdim);
    bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
    values_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor>(values, indices);
}
std::tuple<Tensor &,Tensor &> CPULongType::_th_mode_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CPU, ScalarType::Long);
    auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    THLongTensor_mode(values_, indices_, self_, dim, keepdim);
    bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
    values_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &>(values, indices);
}
std::tuple<Tensor,Tensor> CPULongType::_th_mode(const Tensor & self, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
    auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), scalarTypeToTypeMeta(ScalarType::Long), allocator(), false).release();
    auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    THLongTensor_mode(values_, indices_, self_, dim, keepdim);
    bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
    values_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor>(values, indices);
}
Tensor CPULongType::_th_median(const Tensor & self) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    return at::scalar_tensor(convert<int64_t>(THLongTensor_medianall(self_)), options());
}
std::tuple<Tensor &,Tensor &> CPULongType::_th_median_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CPU, ScalarType::Long);
    auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    THLongTensor_median(values_, indices_, self_, dim, keepdim);
    bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
    values_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &>(values, indices);
}
std::tuple<Tensor,Tensor> CPULongType::_th_median(const Tensor & self, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
    auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), scalarTypeToTypeMeta(ScalarType::Long), allocator(), false).release();
    auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    THLongTensor_median(values_, indices_, self_, dim, keepdim);
    bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
    values_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor>(values, indices);
}
std::tuple<Tensor &,Tensor &> CPULongType::_th_sort_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool descending) const {
    // DeviceGuard omitted
    auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CPU, ScalarType::Long);
    auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    THLongTensor_sort(values_, indices_, self_, dim, descending);
    bool maybe_scalar = self_->dim() == 0;
    values_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &>(values, indices);
}
std::tuple<Tensor,Tensor> CPULongType::_th_sort(const Tensor & self, int64_t dim, bool descending) const {
    // DeviceGuard omitted
    auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
    auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), scalarTypeToTypeMeta(ScalarType::Long), allocator(), false).release();
    auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    THLongTensor_sort(values_, indices_, self_, dim, descending);
    bool maybe_scalar = self_->dim() == 0;
    values_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor>(values, indices);
}
std::tuple<Tensor &,Tensor &> CPULongType::_th_topk_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) const {
    // DeviceGuard omitted
    auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CPU, ScalarType::Long);
    auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    THLongTensor_topk(values_, indices_, self_, k, dim, largest, sorted);
    bool maybe_scalar = self_->dim() == 0;
    values_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &>(values, indices);
}
std::tuple<Tensor,Tensor> CPULongType::_th_topk(const Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) const {
    // DeviceGuard omitted
    auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
    auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), scalarTypeToTypeMeta(ScalarType::Long), allocator(), false).release();
    auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    THLongTensor_topk(values_, indices_, self_, k, dim, largest, sorted);
    bool maybe_scalar = self_->dim() == 0;
    values_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor>(values, indices);
}
Tensor & CPULongType::_th_abs_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    THLongTensor_abs(result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_abs(const Tensor & self) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    THLongTensor_abs(result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_neg_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    THLongTensor_neg(result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_neg(const Tensor & self) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    THLongTensor_neg(result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_neg_(Tensor & self) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    THLongTensor_neg(self_, self_);
    return self;
}
Tensor & CPULongType::_th_pow_out(Tensor & result, const Tensor & self, Scalar exponent) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto exponent_ = exponent.toLong();
    THLongTensor_pow(result_, self_, exponent_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_pow(const Tensor & self, Scalar exponent) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto exponent_ = exponent.toLong();
    THLongTensor_pow(result_, self_, exponent_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::s__th_pow_out(Tensor & result, const Tensor & self, const Tensor & exponent) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cpow(result_, self_, exponent_);
    result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_pow(const Tensor & self, const Tensor & exponent) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cpow(result_, self_, exponent_);
    result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_pow_out(Tensor & result, Scalar self, const Tensor & exponent) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = self.toLong();
    auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_tpow(result_, self_, exponent_);
    result_->maybe_zero_dim(exponent_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_pow(Scalar self, const Tensor & exponent) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = self.toLong();
    auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_tpow(result_, self_, exponent_);
    result_->maybe_zero_dim(exponent_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_pow_(Tensor & self, Scalar exponent) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto exponent_ = exponent.toLong();
    THLongTensor_pow(self_, self_, exponent_);
    return self;
}
Tensor & CPULongType::s__th_pow_(Tensor & self, const Tensor & exponent) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto exponent_ = checked_tensor_unwrap(exponent,"exponent",3, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cpow(self_, self_, exponent_);
    return self;
}
Tensor & CPULongType::_th_zero_(Tensor & self) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    THLongTensor_zero(self_);
    return self;
}
Tensor & CPULongType::_th_cumsum_out(Tensor & result, const Tensor & self, int64_t dim) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    THLongTensor_cumsum(result_, self_, dim);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_cumsum(const Tensor & self, int64_t dim) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    THLongTensor_cumsum(result_, self_, dim);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_cumprod_out(Tensor & result, const Tensor & self, int64_t dim) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    THLongTensor_cumprod(result_, self_, dim);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_cumprod(const Tensor & self, int64_t dim) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    dim = maybe_wrap_dim(dim, self_);
    THLongTensor_cumprod(result_, self_, dim);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_sign_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    THLongTensor_sign(result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_sign(const Tensor & self) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    THLongTensor_sign(result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_sign_(Tensor & self) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    THLongTensor_sign(self_, self_);
    return self;
}
Tensor CPULongType::_th_trace(const Tensor & self) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    return at::scalar_tensor(convert<int64_t>(THLongTensor_trace(self_)), options());
}
Tensor & CPULongType::_th_fmod_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_fmod(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_fmod(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_fmod(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::s__th_fmod_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cfmod(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_fmod(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cfmod(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_fmod_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_fmod(self_, self_, other_);
    return self;
}
Tensor & CPULongType::s__th_fmod_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cfmod(self_, self_, other_);
    return self;
}
Tensor & CPULongType::_th_remainder_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_remainder(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_remainder(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_remainder(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::s__th_remainder_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cremainder(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_remainder(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cremainder(result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_remainder_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = other.toLong();
    THLongTensor_remainder(self_, self_, other_);
    return self;
}
Tensor & CPULongType::s__th_remainder_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cremainder(self_, self_, other_);
    return self;
}
Tensor & CPULongType::_th_clamp_out(Tensor & result, const Tensor & self, Scalar min, Scalar max) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto min_ = min.toLong();
    auto max_ = max.toLong();
    THLongTensor_clamp(result_, self_, min_, max_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_clamp(const Tensor & self, Scalar min, Scalar max) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto min_ = min.toLong();
    auto max_ = max.toLong();
    THLongTensor_clamp(result_, self_, min_, max_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_clamp_min_out(Tensor & result, const Tensor & self, Scalar min) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto min_ = min.toLong();
    THLongTensor_cmaxValue(result_, self_, min_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_clamp_min(const Tensor & self, Scalar min) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto min_ = min.toLong();
    THLongTensor_cmaxValue(result_, self_, min_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_clamp_max_out(Tensor & result, const Tensor & self, Scalar max) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto max_ = max.toLong();
    THLongTensor_cminValue(result_, self_, max_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_clamp_max(const Tensor & self, Scalar max) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto max_ = max.toLong();
    THLongTensor_cminValue(result_, self_, max_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_dot(const Tensor & self, const Tensor & tensor) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto tensor_ = checked_tensor_unwrap(tensor,"tensor",2, false, Backend::CPU, ScalarType::Long);
    return at::scalar_tensor(convert<int64_t>(THLongTensor_dot(self_, tensor_)), options());
}
Tensor & CPULongType::_th_cross_out(Tensor & result, const Tensor & self, const Tensor & other, int64_t dim) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cross(result_, self_, other_, dim);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_cross(const Tensor & self, const Tensor & other, int64_t dim) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_cross(result_, self_, other_, dim);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_diag_out(Tensor & result, const Tensor & self, int64_t diagonal) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    if (self_->dim() == 0) {
      throw std::runtime_error("Input must be 1-d or 2-d");
    }
    THLongTensor_diag(result_, self_, diagonal);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_diag(const Tensor & self, int64_t diagonal) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    if (self_->dim() == 0) {
      throw std::runtime_error("Input must be 1-d or 2-d");
    }
    THLongTensor_diag(result_, self_, diagonal);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPULongType::s__th_addmm_out(Tensor & result, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto beta_ = beta.toLong();
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Long);
    auto alpha_ = alpha.toLong();
    auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CPU, ScalarType::Long);
    auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addmm(result_, beta_, self_, alpha_, mat1_, mat2_);
    result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto beta_ = beta.toLong();
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Long);
    auto alpha_ = alpha.toLong();
    auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CPU, ScalarType::Long);
    auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addmm(result_, beta_, self_, alpha_, mat1_, mat2_);
    result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto beta_ = beta.toLong();
    auto alpha_ = alpha.toLong();
    auto mat1_ = checked_tensor_unwrap(mat1,"mat1",5, false, Backend::CPU, ScalarType::Long);
    auto mat2_ = checked_tensor_unwrap(mat2,"mat2",6, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addmm(self_, beta_, self_, alpha_, mat1_, mat2_);
    return self;
}
Tensor & CPULongType::s__th_addmv_out(Tensor & result, const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto beta_ = beta.toLong();
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Long);
    auto alpha_ = alpha.toLong();
    auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CPU, ScalarType::Long);
    auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addmv(result_, beta_, self_, alpha_, mat_, vec_);
    result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_addmv(const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto beta_ = beta.toLong();
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Long);
    auto alpha_ = alpha.toLong();
    auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CPU, ScalarType::Long);
    auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addmv(result_, beta_, self_, alpha_, mat_, vec_);
    result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_addmv_(Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto beta_ = beta.toLong();
    auto alpha_ = alpha.toLong();
    auto mat_ = checked_tensor_unwrap(mat,"mat",5, false, Backend::CPU, ScalarType::Long);
    auto vec_ = checked_tensor_unwrap(vec,"vec",6, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addmv(self_, beta_, self_, alpha_, mat_, vec_);
    return self;
}
Tensor & CPULongType::s__th_addr_out(Tensor & result, const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto beta_ = beta.toLong();
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Long);
    auto alpha_ = alpha.toLong();
    auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CPU, ScalarType::Long);
    auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addr(result_, beta_, self_, alpha_, vec1_, vec2_);
    result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_addr(const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto beta_ = beta.toLong();
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Long);
    auto alpha_ = alpha.toLong();
    auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CPU, ScalarType::Long);
    auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addr(result_, beta_, self_, alpha_, vec1_, vec2_);
    result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_addr_(Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto beta_ = beta.toLong();
    auto alpha_ = alpha.toLong();
    auto vec1_ = checked_tensor_unwrap(vec1,"vec1",5, false, Backend::CPU, ScalarType::Long);
    auto vec2_ = checked_tensor_unwrap(vec2,"vec2",6, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addr(self_, beta_, self_, alpha_, vec1_, vec2_);
    return self;
}
Tensor & CPULongType::_th_ger_out(Tensor & result, const Tensor & self, const Tensor & vec2) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addr(result_, int64_t(0), result_, int64_t(1), self_, vec2_);
    result_->maybe_zero_dim(false);
    return result;
}
Tensor CPULongType::_th_ger(const Tensor & self, const Tensor & vec2) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addr(result_, int64_t(0), result_, int64_t(1), self_, vec2_);
    result_->maybe_zero_dim(false);
    return result;
}
Tensor & CPULongType::_th_mv_out(Tensor & result, const Tensor & self, const Tensor & vec) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    result.resize_({ self.size(0) });
    result.zero_();
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addmv(result_, int64_t(0), result_, int64_t(1), self_, vec_);
    result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_mv(const Tensor & self, const Tensor & vec) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    result.resize_({ self.size(0) });
    result.zero_();
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addmv(result_, int64_t(0), result_, int64_t(1), self_, vec_);
    result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_mm_out(Tensor & result, const Tensor & self, const Tensor & mat2) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    result.resize_({ self.size(0),mat2.size(1) });
    result.zero_();
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addmm(result_, int64_t(0), result_, int64_t(1), self_, mat2_);
    result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
    return result;
}
Tensor CPULongType::_th_mm(const Tensor & self, const Tensor & mat2) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    result.resize_({ self.size(0),mat2.size(1) });
    result.zero_();
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addmm(result_, int64_t(0), result_, int64_t(1), self_, mat2_);
    result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
    return result;
}
Tensor & CPULongType::s__th_addbmm_out(Tensor & result, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto beta_ = beta.toLong();
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Long);
    auto alpha_ = alpha.toLong();
    auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CPU, ScalarType::Long);
    auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addbmm(result_, beta_, self_, alpha_, batch1_, batch2_);
    result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_addbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto beta_ = beta.toLong();
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Long);
    auto alpha_ = alpha.toLong();
    auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CPU, ScalarType::Long);
    auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addbmm(result_, beta_, self_, alpha_, batch1_, batch2_);
    result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
    return result;
}
Tensor & CPULongType::_th_addbmm_(Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto beta_ = beta.toLong();
    auto alpha_ = alpha.toLong();
    auto batch1_ = checked_tensor_unwrap(batch1,"batch1",5, false, Backend::CPU, ScalarType::Long);
    auto batch2_ = checked_tensor_unwrap(batch2,"batch2",6, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addbmm(self_, beta_, self_, alpha_, batch1_, batch2_);
    return self;
}
Tensor & CPULongType::s__th_addcmul_out(Tensor & result, const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto value_ = value.toLong();
    auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CPU, ScalarType::Long);
    auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addcmul(result_, self_, value_, tensor1_, tensor2_);
    result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_addcmul(const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto value_ = value.toLong();
    auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CPU, ScalarType::Long);
    auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addcmul(result_, self_, value_, tensor1_, tensor2_);
    result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
    return result;
}
Tensor & CPULongType::s__th_addcmul_(Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto value_ = value.toLong();
    auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CPU, ScalarType::Long);
    auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addcmul(self_, self_, value_, tensor1_, tensor2_);
    return self;
}
Tensor & CPULongType::s__th_addcdiv_out(Tensor & result, const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto value_ = value.toLong();
    auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CPU, ScalarType::Long);
    auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addcdiv(result_, self_, value_, tensor1_, tensor2_);
    result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
    return result;
}
Tensor CPULongType::s__th_addcdiv(const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto value_ = value.toLong();
    auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CPU, ScalarType::Long);
    auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addcdiv(result_, self_, value_, tensor1_, tensor2_);
    result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
    return result;
}
Tensor & CPULongType::s__th_addcdiv_(Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto value_ = value.toLong();
    auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CPU, ScalarType::Long);
    auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CPU, ScalarType::Long);
    THLongTensor_addcdiv(self_, self_, value_, tensor1_, tensor2_);
    return self;
}
Tensor & CPULongType::_th_random_(Tensor & self, int64_t from, int64_t to, Generator * generator) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    THLongTensor_clampedRandom(self_, generator_->generator, from, to);
    return self;
}
Tensor & CPULongType::_th_random_(Tensor & self, int64_t to, Generator * generator) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    THLongTensor_cappedRandom(self_, generator_->generator, to);
    return self;
}
Tensor & CPULongType::_th_random_(Tensor & self, Generator * generator) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    THLongTensor_random(self_, generator_->generator);
    return self;
}
Tensor CPULongType::_th_alias(const Tensor & self) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
    return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THLongTensor_newWithTensor(self_))->maybe_zero_dim(self_->dim() == 0)));
}
Tensor & CPULongType::_th_cat_out(Tensor & self, TensorList tensors, int64_t dim) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",0, false, Backend::CPU, ScalarType::Long);
    auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Long);
    THLongTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
    return self;
}
Tensor CPULongType::_th_cat(TensorList tensors, int64_t dim) const {
    // DeviceGuard omitted
    auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<int64_t>(), allocator(), false).release();
    auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
    auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Long);
    THLongTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
    return self;
}
std::tuple<Tensor,Tensor> CPULongType::_cudnn_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity) const {
    AT_ERROR("_cudnn_ctc_loss not supported on CPULongType");
}
Tensor CPULongType::_cudnn_rnn_flatten_weight(TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, bool bidirectional) const {
    AT_ERROR("_cudnn_rnn_flatten_weight not supported on CPULongType");
}
std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPULongType::_cudnn_rnn(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state) const {
    AT_ERROR("_cudnn_rnn not supported on CPULongType");
}
std::tuple<Tensor,Tensor,Tensor,std::vector<Tensor>> CPULongType::_cudnn_rnn_backward(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, const Tensor & output, const Tensor & grad_output, const Tensor & grad_hy, const Tensor & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state, const Tensor & reserve, std::array<bool,4> output_mask) const {
    AT_ERROR("_cudnn_rnn_backward not supported on CPULongType");
}
Tensor CPULongType::_cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions & options) const {
    AT_ERROR("_cudnn_init_dropout_state not supported on CPULongType");
}
std::tuple<Tensor,Tensor> CPULongType::_fused_dropout(const Tensor & self, double p, Generator * generator) const {
    AT_ERROR("_fused_dropout not supported on CPULongType");
}
Tensor CPULongType::_masked_scale(const Tensor & self, const Tensor & mask, double scale) const {
    AT_ERROR("_masked_scale not supported on CPULongType");
}
Tensor & CPULongType::abs_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_abs__cpu(/* actuals */ self);
}
Tensor & CPULongType::abs_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_abs_out_cpu(/* actuals */ out, self);
}
Tensor & CPULongType::acos_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_acos__cpu(/* actuals */ self);
}
Tensor & CPULongType::acos_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_acos_out_cpu(/* actuals */ out, self);
}
Tensor & CPULongType::arange_out(Tensor & out, Scalar start, Scalar end, Scalar step) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::arange_cpu_out(/* actuals */ out, start, end, step);
}
Tensor & CPULongType::asin_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_asin__cpu(/* actuals */ self);
}
Tensor & CPULongType::asin_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_asin_out_cpu(/* actuals */ out, self);
}
Tensor & CPULongType::atan_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_atan__cpu(/* actuals */ self);
}
Tensor & CPULongType::atan_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_atan_out_cpu(/* actuals */ out, self);
}
Tensor CPULongType::baddbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::baddbmm_cpu(/* actuals */ self, batch1, batch2, beta, alpha);
}
Tensor & CPULongType::baddbmm_(Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::baddbmm__cpu(/* actuals */ self, batch1, batch2, beta, alpha);
}
Tensor & CPULongType::baddbmm_out(Tensor & out, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::baddbmm_out_cpu(/* actuals */ out, self, batch1, batch2, beta, alpha);
}
Tensor & CPULongType::bernoulli_(Tensor & self, const Tensor & p, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::bernoulli_tensor_cpu_(/* actuals */ self, p, generator);
}
Tensor & CPULongType::bernoulli_(Tensor & self, double p, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::bernoulli_scalar_cpu_(/* actuals */ self, p, generator);
}
Tensor CPULongType::bincount(const Tensor & self, const Tensor & weights, int64_t minlength) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_bincount_cpu(/* actuals */ self, weights, minlength);
}
Tensor CPULongType::bmm(const Tensor & self, const Tensor & mat2) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::bmm_cpu(/* actuals */ self, mat2);
}
Tensor & CPULongType::bmm_out(Tensor & out, const Tensor & self, const Tensor & mat2) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::bmm_out_cpu(/* actuals */ out, self, mat2);
}
Tensor & CPULongType::ceil_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_ceil__cpu(/* actuals */ self);
}
Tensor & CPULongType::ceil_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_ceil_out_cpu(/* actuals */ out, self);
}
Tensor & CPULongType::clamp_(Tensor & self, c10::optional<Scalar> min, c10::optional<Scalar> max) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_clamp__cpu(/* actuals */ self, min, max);
}
Tensor & CPULongType::clamp_out(Tensor & out, const Tensor & self, c10::optional<Scalar> min, c10::optional<Scalar> max) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_clamp_out_cpu(/* actuals */ out, self, min, max);
}
Tensor & CPULongType::clamp_max_(Tensor & self, Scalar max) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_clamp_max__cpu(/* actuals */ self, max);
}
Tensor & CPULongType::clamp_max_out(Tensor & out, const Tensor & self, Scalar max) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_clamp_max_out_cpu(/* actuals */ out, self, max);
}
Tensor & CPULongType::clamp_min_(Tensor & self, Scalar min) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_clamp_min__cpu(/* actuals */ self, min);
}
Tensor & CPULongType::clamp_min_out(Tensor & out, const Tensor & self, Scalar min) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_clamp_min_out_cpu(/* actuals */ out, self, min);
}
Tensor & CPULongType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_s_copy__cpu(/* actuals */ self, src, non_blocking);
}
Tensor CPULongType::_s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const {
    AT_ERROR("_s_copy_from not supported on CPULongType");
}
void CPULongType::_copy_same_type_(Tensor & self, const Tensor & src) const {
    const OptionalDeviceGuard device_guard(device_of(self));
 at::native::_copy_same_type__cpu(/* actuals */ self, src);
}
Tensor & CPULongType::cos_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_cos__cpu(/* actuals */ self);
}
Tensor & CPULongType::cos_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_cos_out_cpu(/* actuals */ out, self);
}
Tensor & CPULongType::cosh_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_cosh__cpu(/* actuals */ self);
}
Tensor & CPULongType::cosh_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_cosh_out_cpu(/* actuals */ out, self);
}
Tensor CPULongType::cudnn_affine_grid_generator(const Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) const {
    AT_ERROR("cudnn_affine_grid_generator not supported on CPULongType");
}
Tensor CPULongType::cudnn_affine_grid_generator_backward(const Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) const {
    AT_ERROR("cudnn_affine_grid_generator_backward not supported on CPULongType");
}
std::tuple<Tensor,Tensor,Tensor> CPULongType::cudnn_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
    AT_ERROR("cudnn_batch_norm not supported on CPULongType");
}
std::tuple<Tensor,Tensor,Tensor> CPULongType::cudnn_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
    AT_ERROR("cudnn_batch_norm_backward not supported on CPULongType");
}
Tensor CPULongType::cudnn_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("cudnn_convolution not supported on CPULongType");
}
Tensor CPULongType::cudnn_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("cudnn_convolution_backward_input not supported on CPULongType");
}
std::tuple<Tensor,Tensor,Tensor> CPULongType::cudnn_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
    AT_ERROR("cudnn_convolution_backward not supported on CPULongType");
}
Tensor CPULongType::cudnn_convolution_backward_bias(const Tensor & grad_output) const {
    AT_ERROR("cudnn_convolution_backward_bias not supported on CPULongType");
}
Tensor CPULongType::cudnn_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("cudnn_convolution_backward_weight not supported on CPULongType");
}
Tensor CPULongType::cudnn_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("cudnn_convolution_transpose not supported on CPULongType");
}
std::tuple<Tensor,Tensor,Tensor> CPULongType::cudnn_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
    AT_ERROR("cudnn_convolution_transpose_backward not supported on CPULongType");
}
Tensor CPULongType::cudnn_convolution_transpose_backward_bias(const Tensor & grad_output) const {
    AT_ERROR("cudnn_convolution_transpose_backward_bias not supported on CPULongType");
}
Tensor CPULongType::cudnn_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("cudnn_convolution_transpose_backward_input not supported on CPULongType");
}
Tensor CPULongType::cudnn_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("cudnn_convolution_transpose_backward_weight not supported on CPULongType");
}
Tensor CPULongType::cudnn_grid_sampler(const Tensor & self, const Tensor & grid) const {
    AT_ERROR("cudnn_grid_sampler not supported on CPULongType");
}
std::tuple<Tensor,Tensor> CPULongType::cudnn_grid_sampler_backward(const Tensor & self, const Tensor & grid, const Tensor & grad_output) const {
    AT_ERROR("cudnn_grid_sampler_backward not supported on CPULongType");
}
std::tuple<Tensor,Tensor> CPULongType::_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool zero_infinity) const {
    const OptionalDeviceGuard device_guard(device_of(log_probs));
    return at::native::ctc_loss_cpu(/* actuals */ log_probs, targets, input_lengths, target_lengths, blank, zero_infinity);
}
Tensor CPULongType::_ctc_loss_backward(const Tensor & grad, const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, const Tensor & neg_log_likelihood, const Tensor & log_alpha, int64_t blank, bool zero_infinity) const {
    const OptionalDeviceGuard device_guard(device_of(grad));
    return at::native::ctc_loss_backward_cpu(/* actuals */ grad, log_probs, targets, input_lengths, target_lengths, neg_log_likelihood, log_alpha, blank, zero_infinity);
}
Tensor CPULongType::embedding_dense_backward(const Tensor & grad, const Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) const {
    const OptionalDeviceGuard device_guard(device_of(grad));
    return at::native::embedding_dense_backward_cpu(/* actuals */ grad, indices, num_weights, padding_idx, scale_grad_by_freq);
}
Tensor & CPULongType::embedding_renorm_(Tensor & self, const Tensor & indices, double max_norm, double norm_type) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::embedding_renorm_cpu_(/* actuals */ self, indices, max_norm, norm_type);
}
std::tuple<Tensor,Tensor,Tensor,Tensor> CPULongType::_embedding_bag(const Tensor & weight, const Tensor & indices, const Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse) const {
    const OptionalDeviceGuard device_guard(device_of(weight));
    return at::native::_embedding_bag_cpu(/* actuals */ weight, indices, offsets, scale_grad_by_freq, mode, sparse);
}
Tensor CPULongType::_embedding_bag_dense_backward(const Tensor & grad, const Tensor & indices, const Tensor & offsets, const Tensor & offset2bag, const Tensor & bag_size, const Tensor & maximum_indices, int64_t num_weights, bool scale_grad_by_freq, int64_t mode) const {
    const OptionalDeviceGuard device_guard(device_of(grad));
    return at::native::_embedding_bag_dense_backward_cpu(/* actuals */ grad, indices, offsets, offset2bag, bag_size, maximum_indices, num_weights, scale_grad_by_freq, mode);
}
Tensor CPULongType::empty(IntArrayRef size, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::empty_cpu(/* actuals */ size, options);
}
Tensor & CPULongType::resize_(Tensor & self, IntArrayRef size) const {
    // DeviceGuard omitted
    return at::native::resize_cpu_(/* actuals */ self, size);
}
Tensor CPULongType::empty_strided(IntArrayRef size, IntArrayRef stride, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::empty_strided_cpu(/* actuals */ size, stride, options);
}
Tensor & CPULongType::erf_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_erf__cpu(/* actuals */ self);
}
Tensor & CPULongType::erf_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_erf_out_cpu(/* actuals */ out, self);
}
Tensor & CPULongType::erfc_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_erfc__cpu(/* actuals */ self);
}
Tensor & CPULongType::erfc_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_erfc_out_cpu(/* actuals */ out, self);
}
Tensor & CPULongType::exp_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_exp__cpu(/* actuals */ self);
}
Tensor & CPULongType::exp_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_exp_out_cpu(/* actuals */ out, self);
}
Tensor & CPULongType::expm1_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_expm1__cpu(/* actuals */ self);
}
Tensor & CPULongType::expm1_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_expm1_out_cpu(/* actuals */ out, self);
}
Tensor & CPULongType::eye_out(Tensor & out, int64_t n) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::eye_out_cpu(/* actuals */ out, n);
}
Tensor & CPULongType::eye_out(Tensor & out, int64_t n, int64_t m) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::eye_out_cpu(/* actuals */ out, n, m);
}
Tensor & CPULongType::floor_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_floor__cpu(/* actuals */ self);
}
Tensor & CPULongType::floor_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_floor_out_cpu(/* actuals */ out, self);
}
Tensor CPULongType::grid_sampler_2d(const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::grid_sampler_2d_cpu(/* actuals */ input, grid, interpolation_mode, padding_mode);
}
std::tuple<Tensor,Tensor> CPULongType::grid_sampler_2d_backward(const Tensor & grad_output, const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    return at::native::grid_sampler_2d_backward_cpu(/* actuals */ grad_output, input, grid, interpolation_mode, padding_mode);
}
Tensor CPULongType::grid_sampler_3d(const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::grid_sampler_3d_cpu(/* actuals */ input, grid, interpolation_mode, padding_mode);
}
std::tuple<Tensor,Tensor> CPULongType::grid_sampler_3d_backward(const Tensor & grad_output, const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    return at::native::grid_sampler_3d_backward_cpu(/* actuals */ grad_output, input, grid, interpolation_mode, padding_mode);
}
std::tuple<Tensor,Tensor> CPULongType::_gesv_helper(const Tensor & self, const Tensor & A) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_gesv_helper_cpu(/* actuals */ self, A);
}
Tensor CPULongType::_fft_with_size(const Tensor & self, int64_t signal_ndim, bool complex_input, bool complex_output, bool inverse, IntArrayRef checked_signal_sizes, bool normalized, bool onesided, IntArrayRef output_sizes) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_fft_mkl(/* actuals */ self, signal_ndim, complex_input, complex_output, inverse, checked_signal_sizes, normalized, onesided, output_sizes);
}
Tensor CPULongType::_inverse_helper(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_inverse_helper_cpu(/* actuals */ self);
}
Tensor CPULongType::kl_div_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::kl_div_backward_cpu(/* actuals */ grad_output, self, target, reduction);
}
Tensor & CPULongType::linspace_out(Tensor & out, Scalar start, Scalar end, int64_t steps) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::linspace_cpu_out(/* actuals */ out, start, end, steps);
}
Tensor & CPULongType::log_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_log__cpu(/* actuals */ self);
}
Tensor & CPULongType::log_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_log_out_cpu(/* actuals */ out, self);
}
Tensor & CPULongType::log10_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_log10__cpu(/* actuals */ self);
}
Tensor & CPULongType::log10_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_log10_out_cpu(/* actuals */ out, self);
}
Tensor & CPULongType::log1p_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_log1p__cpu(/* actuals */ self);
}
Tensor & CPULongType::log1p_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_log1p_out_cpu(/* actuals */ out, self);
}
Tensor & CPULongType::log2_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_log2__cpu(/* actuals */ self);
}
Tensor & CPULongType::log2_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_log2_out_cpu(/* actuals */ out, self);
}
Tensor & CPULongType::logspace_out(Tensor & out, Scalar start, Scalar end, int64_t steps) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::logspace_cpu_out(/* actuals */ out, start, end, steps);
}
Tensor CPULongType::_log_softmax(const Tensor & self, int64_t dim, bool half_to_float) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::log_softmax_cpu(/* actuals */ self, dim, half_to_float);
}
Tensor CPULongType::_log_softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::log_softmax_backward_cpu(/* actuals */ grad_output, output, dim, self);
}
std::tuple<Tensor,Tensor,Tensor> CPULongType::miopen_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
    AT_ERROR("miopen_batch_norm not supported on CPULongType");
}
std::tuple<Tensor,Tensor,Tensor> CPULongType::miopen_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
    AT_ERROR("miopen_batch_norm_backward not supported on CPULongType");
}
Tensor CPULongType::miopen_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("miopen_convolution not supported on CPULongType");
}
Tensor CPULongType::miopen_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("miopen_convolution_backward_input not supported on CPULongType");
}
std::tuple<Tensor,Tensor,Tensor> CPULongType::miopen_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
    AT_ERROR("miopen_convolution_backward not supported on CPULongType");
}
Tensor CPULongType::miopen_convolution_backward_bias(const Tensor & grad_output) const {
    AT_ERROR("miopen_convolution_backward_bias not supported on CPULongType");
}
Tensor CPULongType::miopen_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("miopen_convolution_backward_weight not supported on CPULongType");
}
Tensor CPULongType::miopen_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("miopen_convolution_transpose not supported on CPULongType");
}
std::tuple<Tensor,Tensor,Tensor> CPULongType::miopen_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
    AT_ERROR("miopen_convolution_transpose_backward not supported on CPULongType");
}
Tensor CPULongType::miopen_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("miopen_convolution_transpose_backward_input not supported on CPULongType");
}
Tensor CPULongType::miopen_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("miopen_convolution_transpose_backward_weight not supported on CPULongType");
}
Tensor CPULongType::narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::narrow_copy_dense(/* actuals */ self, dim, start, length);
}
std::tuple<Tensor,Tensor,Tensor> CPULongType::native_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::batch_norm_cpu(/* actuals */ input, weight, bias, running_mean, running_var, training, momentum, eps);
}
std::tuple<Tensor,Tensor,Tensor> CPULongType::native_batch_norm_backward(const Tensor & grad_out, const Tensor & input, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_invstd, bool train, double eps, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(grad_out));
    return at::native::batch_norm_backward_cpu(/* actuals */ grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
}
std::tuple<Tensor,Tensor> CPULongType::batch_norm_update_stats(const Tensor & input, const Tensor & running_mean, const Tensor & running_var, double momentum) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::batch_norm_update_stats_cpu(/* actuals */ input, running_mean, running_var, momentum);
}
Tensor & CPULongType::randperm_out(Tensor & out, int64_t n, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::randperm_out_cpu(/* actuals */ out, n, generator);
}
Tensor & CPULongType::range_out(Tensor & out, Scalar start, Scalar end, Scalar step) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::range_cpu_out(/* actuals */ out, start, end, step);
}
Tensor & CPULongType::round_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_round__cpu(/* actuals */ self);
}
Tensor & CPULongType::round_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_round_out_cpu(/* actuals */ out, self);
}
Tensor CPULongType::prelu(const Tensor & self, const Tensor & weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::prelu_cpu(/* actuals */ self, weight);
}
std::tuple<Tensor,Tensor> CPULongType::prelu_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::prelu_backward_cpu(/* actuals */ grad_output, self, weight);
}
Tensor CPULongType::hardshrink(const Tensor & self, Scalar lambd) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::hardshrink_cpu(/* actuals */ self, lambd);
}
Tensor CPULongType::hardshrink_backward(const Tensor & grad_out, const Tensor & self, Scalar lambd) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::hardshrink_backward_cpu(/* actuals */ grad_out, self, lambd);
}
Tensor & CPULongType::rsqrt_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_rsqrt__cpu(/* actuals */ self);
}
Tensor & CPULongType::rsqrt_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_rsqrt_out_cpu(/* actuals */ out, self);
}
Tensor & CPULongType::sigmoid_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sigmoid__cpu(/* actuals */ self);
}
Tensor & CPULongType::sigmoid_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sigmoid_out_cpu(/* actuals */ out, self);
}
Tensor & CPULongType::sin_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sin__cpu(/* actuals */ self);
}
Tensor & CPULongType::sin_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sin_out_cpu(/* actuals */ out, self);
}
Tensor & CPULongType::sinh_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sinh__cpu(/* actuals */ self);
}
Tensor & CPULongType::sinh_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sinh_out_cpu(/* actuals */ out, self);
}
Tensor CPULongType::_softmax(const Tensor & self, int64_t dim, bool half_to_float) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::softmax_cpu(/* actuals */ self, dim, half_to_float);
}
Tensor CPULongType::_softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::softmax_backward_cpu(/* actuals */ grad_output, output, dim, self);
}
Tensor & CPULongType::_sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const {
    AT_ERROR("_sparse_add_out not supported on CPULongType");
}
Tensor & CPULongType::_sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::add_out_dense_sparse_cpu(/* actuals */ out, self, other, alpha);
}
Tensor & CPULongType::_sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
    AT_ERROR("_sparse_div_zerodim_out not supported on CPULongType");
}
Tensor & CPULongType::_sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
    AT_ERROR("_sparse_div_scalar_out not supported on CPULongType");
}
Tensor & CPULongType::_sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const {
    AT_ERROR("_sparse_mul_out not supported on CPULongType");
}
Tensor & CPULongType::_sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
    AT_ERROR("_sparse_mul_zerodim_out not supported on CPULongType");
}
Tensor & CPULongType::_sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
    AT_ERROR("_sparse_mul_scalar_out not supported on CPULongType");
}
Tensor & CPULongType::sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sspaddmm_out_only_sparse(/* actuals */ out, self, mat1, mat2, beta, alpha);
}
Tensor & CPULongType::sqrt_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sqrt__cpu(/* actuals */ self);
}
Tensor & CPULongType::sqrt_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sqrt_out_cpu(/* actuals */ out, self);
}
Tensor & CPULongType::tan_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_tan__cpu(/* actuals */ self);
}
Tensor & CPULongType::tan_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_tan_out_cpu(/* actuals */ out, self);
}
Tensor & CPULongType::tanh_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_tanh__cpu(/* actuals */ self);
}
Tensor & CPULongType::tanh_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_tanh_out_cpu(/* actuals */ out, self);
}
Tensor CPULongType::flip(const Tensor & self, IntArrayRef dims) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::flip_cpu(/* actuals */ self, dims);
}
Tensor CPULongType::roll(const Tensor & self, IntArrayRef shifts, IntArrayRef dims) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::roll_cpu(/* actuals */ self, shifts, dims);
}
Tensor & CPULongType::trunc_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_trunc__cpu(/* actuals */ self);
}
Tensor & CPULongType::trunc_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_trunc_out_cpu(/* actuals */ out, self);
}
std::tuple<Tensor,Tensor> CPULongType::_unique(const Tensor & self, bool sorted, bool return_inverse) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_unique_cpu(/* actuals */ self, sorted, return_inverse);
}
std::tuple<Tensor,Tensor> CPULongType::_unique_dim(const Tensor & self, int64_t dim, bool sorted, bool return_inverse) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_unique_dim_cpu(/* actuals */ self, dim, sorted, return_inverse);
}
Tensor CPULongType::_s_where(const Tensor & condition, const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_s_where_cpu(/* actuals */ condition, self, other);
}
std::tuple<Tensor,Tensor> CPULongType::_weight_norm_cuda_interface(const Tensor & v, const Tensor & g, int64_t dim) const {
    AT_ERROR("_weight_norm_cuda_interface not supported on CPULongType");
}
std::tuple<Tensor,Tensor> CPULongType::_weight_norm_cuda_interface_backward(const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim) const {
    AT_ERROR("_weight_norm_cuda_interface_backward not supported on CPULongType");
}
Tensor CPULongType::_standard_gamma_grad(const Tensor & self, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_standard_gamma_grad_cpu(/* actuals */ self, output);
}
Tensor CPULongType::_standard_gamma(const Tensor & self, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_s_gamma_cpu(/* actuals */ self, generator);
}
Tensor CPULongType::poisson(const Tensor & self, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_s_poisson_cpu(/* actuals */ self, generator);
}
Tensor CPULongType::native_norm(const Tensor & self, Scalar p) const {
    AT_ERROR("native_norm not supported on CPULongType");
}
Tensor CPULongType::_sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const {
    AT_ERROR("_sparse_sum_backward not supported on CPULongType");
}
Tensor CPULongType::native_clone(const Tensor & self) const {
    AT_ERROR("native_clone not supported on CPULongType");
}
Tensor & CPULongType::native_resize_as_(Tensor & self, const Tensor & the_template) const {
    AT_ERROR("native_resize_as_ not supported on CPULongType");
}
Tensor & CPULongType::native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const {
    AT_ERROR("native_pow_out not supported on CPULongType");
}
Tensor CPULongType::native_pow(const Tensor & self, Scalar exponent) const {
    AT_ERROR("native_pow not supported on CPULongType");
}
Tensor & CPULongType::native_zero_(Tensor & self) const {
    AT_ERROR("native_zero_ not supported on CPULongType");
}
Tensor & CPULongType::s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::s_addmm_out_sparse_dense_cpu(/* actuals */ out, self, mat1, mat2, beta, alpha);
}
Tensor CPULongType::s_native_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::s_addmm_sparse_dense_cpu(/* actuals */ self, mat1, mat2, beta, alpha);
}
Tensor & CPULongType::s_native_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::s_addmm_sparse_dense_cpu_(/* actuals */ self, mat1, mat2, beta, alpha);
}
Tensor CPULongType::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const {
    AT_ERROR("_sparse_coo_tensor_with_dims not supported on CPULongType");
}
Tensor CPULongType::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const {
    AT_ERROR("_sparse_coo_tensor_with_dims_and_tensors not supported on CPULongType");
}
Tensor & CPULongType::sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
    AT_ERROR("sparse_resize_ not supported on CPULongType");
}
Tensor & CPULongType::sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
    AT_ERROR("sparse_resize_and_clear_ not supported on CPULongType");
}
Tensor CPULongType::sparse_mask(const Tensor & self, SparseTensorRef mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sparse_mask_cpu(/* actuals */ self, mask);
}
Tensor CPULongType::to_dense(const Tensor & self) const {
    AT_ERROR("to_dense not supported on CPULongType");
}
int64_t CPULongType::sparse_dim(const Tensor & self) const {
    AT_ERROR("sparse_dim not supported on CPULongType");
}
int64_t CPULongType::dense_dim(const Tensor & self) const {
    AT_ERROR("dense_dim not supported on CPULongType");
}
int64_t CPULongType::_nnz(const Tensor & self) const {
    AT_ERROR("_nnz not supported on CPULongType");
}
Tensor CPULongType::coalesce(const Tensor & self) const {
    AT_ERROR("coalesce not supported on CPULongType");
}
bool CPULongType::is_coalesced(const Tensor & self) const {
    AT_ERROR("is_coalesced not supported on CPULongType");
}
Tensor CPULongType::_indices(const Tensor & self) const {
    AT_ERROR("_indices not supported on CPULongType");
}
Tensor CPULongType::_values(const Tensor & self) const {
    AT_ERROR("_values not supported on CPULongType");
}
Tensor & CPULongType::_coalesced_(Tensor & self, bool coalesced) const {
    AT_ERROR("_coalesced_ not supported on CPULongType");
}
Tensor CPULongType::indices(const Tensor & self) const {
    AT_ERROR("indices not supported on CPULongType");
}
Tensor CPULongType::values(const Tensor & self) const {
    AT_ERROR("values not supported on CPULongType");
}
Tensor & CPULongType::hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const {
    AT_ERROR("hspmm_out not supported on CPULongType");
}
Tensor CPULongType::hspmm(const Tensor & mat1, const Tensor & mat2) const {
    AT_ERROR("hspmm not supported on CPULongType");
}
Tensor & CPULongType::copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const {
    AT_ERROR("copy_sparse_to_sparse_ not supported on CPULongType");
}
Tensor CPULongType::to_sparse(const Tensor & self, int64_t sparse_dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::dense_to_sparse(/* actuals */ self, sparse_dim);
}
Tensor CPULongType::to_sparse(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::dense_to_sparse(/* actuals */ self);
}
Scalar CPULongType::_local_scalar_dense(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_local_scalar_dense_cpu(/* actuals */ self);
}
std::tuple<Tensor,Tensor,Tensor> CPULongType::_thnn_fused_lstm_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & cx, const Tensor & input_bias, const Tensor & hidden_bias) const {
    AT_ERROR("_thnn_fused_lstm_cell not supported on CPULongType");
}
std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPULongType::_thnn_fused_lstm_cell_backward(const Tensor & grad_hy, const Tensor & grad_cy, const Tensor & cx, const Tensor & cy, const Tensor & workspace, bool has_bias) const {
    AT_ERROR("_thnn_fused_lstm_cell_backward not supported on CPULongType");
}
std::tuple<Tensor,Tensor> CPULongType::_thnn_fused_gru_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & hx, const Tensor & input_bias, const Tensor & hidden_bias) const {
    AT_ERROR("_thnn_fused_gru_cell not supported on CPULongType");
}
std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPULongType::_thnn_fused_gru_cell_backward(const Tensor & grad_hy, const Tensor & workspace, bool has_bias) const {
    AT_ERROR("_thnn_fused_gru_cell_backward not supported on CPULongType");
}
Tensor & CPULongType::tril_(Tensor & self, int64_t diagonal) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::tril_cpu_(/* actuals */ self, diagonal);
}
Tensor & CPULongType::triu_(Tensor & self, int64_t diagonal) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::triu_cpu_(/* actuals */ self, diagonal);
}
Tensor & CPULongType::triu_out(Tensor & out, const Tensor & self, int64_t diagonal) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::triu_cpu_out(/* actuals */ out, self, diagonal);
}
Tensor & CPULongType::tril_out(Tensor & out, const Tensor & self, int64_t diagonal) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::tril_cpu_out(/* actuals */ out, self, diagonal);
}
Tensor CPULongType::tril_indices(int64_t row, int64_t col, int64_t offset, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::tril_indices_cpu(/* actuals */ row, col, offset, options);
}
Tensor CPULongType::triu_indices(int64_t row, int64_t col, int64_t offset, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::triu_indices_cpu(/* actuals */ row, col, offset, options);
}
Tensor CPULongType::_cholesky_helper(const Tensor & self, bool upper) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_cholesky_helper_cpu(/* actuals */ self, upper);
}
Tensor CPULongType::_cholesky_solve_helper(const Tensor & self, const Tensor & A, bool upper) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_cholesky_solve_helper_cpu(/* actuals */ self, A, upper);
}
Tensor & CPULongType::histc_out(Tensor & out, const Tensor & self, int64_t bins, Scalar min, Scalar max) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_histc_out_cpu(/* actuals */ out, self, bins, min, max);
}
Tensor CPULongType::histc(const Tensor & self, int64_t bins, Scalar min, Scalar max) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_histc_cpu(/* actuals */ self, bins, min, max);
}
Tensor & CPULongType::adaptive_avg_pool2d_out(Tensor & output, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::adaptive_avg_pool2d_out_cpu(/* actuals */ output, self, output_size);
}
Tensor CPULongType::_adaptive_avg_pool2d(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::adaptive_avg_pool2d_cpu(/* actuals */ self, output_size);
}
Tensor CPULongType::_adaptive_avg_pool2d_backward(const Tensor & grad_output, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::adaptive_avg_pool2d_backward_cpu(/* actuals */ grad_output, self);
}
std::tuple<Tensor &,Tensor &> CPULongType::fractional_max_pool2d_out(Tensor & output, Tensor & indices, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & random_samples) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::fractional_max_pool2d_out_cpu(/* actuals */ output, indices, self, kernel_size, output_size, random_samples);
}
std::tuple<Tensor,Tensor> CPULongType::fractional_max_pool2d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & random_samples) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::fractional_max_pool2d_cpu(/* actuals */ self, kernel_size, output_size, random_samples);
}
Tensor & CPULongType::fractional_max_pool2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::fractional_max_pool2d_backward_out_cpu(/* actuals */ grad_input, grad_output, self, kernel_size, output_size, indices);
}
Tensor CPULongType::fractional_max_pool2d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::fractional_max_pool2d_backward_cpu(/* actuals */ grad_output, self, kernel_size, output_size, indices);
}
std::tuple<Tensor &,Tensor &> CPULongType::fractional_max_pool3d_out(Tensor & output, Tensor & indices, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & random_samples) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::fractional_max_pool3d_out_cpu(/* actuals */ output, indices, self, kernel_size, output_size, random_samples);
}
std::tuple<Tensor,Tensor> CPULongType::fractional_max_pool3d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & random_samples) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::fractional_max_pool3d_cpu(/* actuals */ self, kernel_size, output_size, random_samples);
}
Tensor & CPULongType::fractional_max_pool3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::fractional_max_pool3d_backward_out_cpu(/* actuals */ grad_input, grad_output, self, kernel_size, output_size, indices);
}
Tensor CPULongType::fractional_max_pool3d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::fractional_max_pool3d_backward_cpu(/* actuals */ grad_output, self, kernel_size, output_size, indices);
}
Tensor & CPULongType::reflection_pad1d_out(Tensor & output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::reflection_pad1d_out_cpu(/* actuals */ output, self, padding);
}
Tensor CPULongType::reflection_pad1d(const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::reflection_pad1d_cpu(/* actuals */ self, padding);
}
Tensor & CPULongType::reflection_pad1d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::reflection_pad1d_backward_out_cpu(/* actuals */ grad_input, grad_output, self, padding);
}
Tensor CPULongType::reflection_pad1d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::reflection_pad1d_backward_cpu(/* actuals */ grad_output, self, padding);
}
Tensor & CPULongType::reflection_pad2d_out(Tensor & output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::reflection_pad2d_out_cpu(/* actuals */ output, self, padding);
}
Tensor CPULongType::reflection_pad2d(const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::reflection_pad2d_cpu(/* actuals */ self, padding);
}
Tensor & CPULongType::reflection_pad2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::reflection_pad2d_backward_out_cpu(/* actuals */ grad_input, grad_output, self, padding);
}
Tensor CPULongType::reflection_pad2d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::reflection_pad2d_backward_cpu(/* actuals */ grad_output, self, padding);
}
Tensor & CPULongType::replication_pad1d_out(Tensor & output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::replication_pad1d_out_cpu(/* actuals */ output, self, padding);
}
Tensor CPULongType::replication_pad1d(const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::replication_pad1d_cpu(/* actuals */ self, padding);
}
Tensor & CPULongType::replication_pad1d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::replication_pad1d_backward_out_cpu(/* actuals */ grad_input, grad_output, self, padding);
}
Tensor CPULongType::replication_pad1d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::replication_pad1d_backward_cpu(/* actuals */ grad_output, self, padding);
}
Tensor & CPULongType::replication_pad2d_out(Tensor & output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::replication_pad2d_out_cpu(/* actuals */ output, self, padding);
}
Tensor CPULongType::replication_pad2d(const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::replication_pad2d_cpu(/* actuals */ self, padding);
}
Tensor & CPULongType::replication_pad2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::replication_pad2d_backward_out_cpu(/* actuals */ grad_input, grad_output, self, padding);
}
Tensor CPULongType::replication_pad2d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::replication_pad2d_backward_cpu(/* actuals */ grad_output, self, padding);
}
Tensor & CPULongType::replication_pad3d_out(Tensor & output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::replication_pad3d_out_cpu(/* actuals */ output, self, padding);
}
Tensor CPULongType::replication_pad3d(const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::replication_pad3d_cpu(/* actuals */ self, padding);
}
Tensor & CPULongType::replication_pad3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::replication_pad3d_backward_out_cpu(/* actuals */ grad_input, grad_output, self, padding);
}
Tensor CPULongType::replication_pad3d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::replication_pad3d_backward_cpu(/* actuals */ grad_output, self, padding);
}

}
