// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#define __STDC_FORMAT_MACROS

#include <ATen/QuantizedCPUType.h>

// @generated by aten/src/ATen/gen.py

#include <TH/TH.h>
#include <TH/THTensor.hpp>
#include <THNN/THNN.h>
#undef THNN_
#include <c10/core/TensorImpl.h>
#include <ATen/CPUGenerator.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NativeFunctions.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/Half.h>
#include <c10/core/TensorImpl.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>


namespace at {

QuantizedCPUType::QuantizedCPUType()
  : CPUTypeDefault(QuantizedCPUTensorId(), /*is_variable=*/false, /*is_undefined=*/false) {}

Backend QuantizedCPUType::backend() const {
  return Backend::QuantizedCPU;
}

const char * QuantizedCPUType::toString() const {
  return "QuantizedCPUType";
}

TypeID QuantizedCPUType::ID() const {
  return TypeID::QuantizedCPU;
}

/* example
Tensor * QuantizedCPUType::add(Tensor & a, Tensor & b) {
  std::cout << "add Tensor with backend QuantizedCPU\n";
  return &a;
}
*/

Tensor QuantizedCPUType::as_strided(const Tensor & self, IntArrayRef size, IntArrayRef stride, c10::optional<int64_t> storage_offset) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::QInt8:
            return at::native::as_strided_qtensorimpl(/* actuals */ self, size, stride, storage_offset);
        break;
        default:
            AT_ERROR("as_strided not supported on QuantizedCPUType for ", dispatch_scalar_type);
    }
}
Tensor & QuantizedCPUType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::QInt8:
            return at::native::_s_copy__quantized(/* actuals */ self, src, non_blocking);
        break;
        default:
            AT_ERROR("s_copy_ not supported on QuantizedCPUType for ", dispatch_scalar_type);
    }
}
Tensor QuantizedCPUType::_empty_affine_quantized(IntArrayRef size, const TensorOptions & options, double scale, int64_t zero_point) const {
    const DeviceGuard device_guard(options.device());
    auto dispatch_scalar_type = typeMetaToScalarType(options.dtype());
    switch (dispatch_scalar_type) {
        case ScalarType::QInt8:
            return at::native::empty_affine_quantized_cpu(/* actuals */ size, options, scale, zero_point);
        break;
        default:
            AT_ERROR("_empty_affine_quantized not supported on QuantizedCPUType for ", dispatch_scalar_type);
    }
}
Tensor QuantizedCPUType::dequantize(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::QInt8:
            return at::native::dequantize_quant(/* actuals */ self);
        break;
        default:
            AT_ERROR("dequantize not supported on QuantizedCPUType for ", dispatch_scalar_type);
    }
}
Scalar QuantizedCPUType::q_scale(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::QInt8:
            return at::native::q_scale_quant(/* actuals */ self);
        break;
        default:
            AT_ERROR("q_scale not supported on QuantizedCPUType for ", dispatch_scalar_type);
    }
}
Scalar QuantizedCPUType::q_zero_point(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::QInt8:
            return at::native::q_zero_point_quant(/* actuals */ self);
        break;
        default:
            AT_ERROR("q_zero_point not supported on QuantizedCPUType for ", dispatch_scalar_type);
    }
}
Tensor QuantizedCPUType::int_repr(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::QInt8:
            return at::native::int_repr_quant(/* actuals */ self);
        break;
        default:
            AT_ERROR("int_repr not supported on QuantizedCPUType for ", dispatch_scalar_type);
    }
}

}
