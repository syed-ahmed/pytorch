// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#define __STDC_FORMAT_MACROS

#include <ATen/CPUHalfType.h>

// @generated by aten/src/ATen/gen.py

#include <TH/TH.h>
#include <TH/THTensor.hpp>
#include <THNN/THNN.h>
#undef THNN_
#include <c10/core/TensorImpl.h>
#include <ATen/CPUGenerator.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NativeFunctions.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <c10/util/Half.h>
#include <c10/core/TensorImpl.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>


namespace at {

CPUHalfType::CPUHalfType()
  : CPUTypeDefault(CPUTensorId(), /*is_variable=*/false, /*is_undefined=*/false) {}

ScalarType CPUHalfType::scalarType() const {
  return ScalarType::Half;
}

caffe2::TypeMeta CPUHalfType::typeMeta() const {
    return caffe2::TypeMeta::Make<Half>();
}

Backend CPUHalfType::backend() const {
  return Backend::CPU;
}

const char * CPUHalfType::toString() const {
  return "CPUHalfType";
}

TypeID CPUHalfType::ID() const {
  return TypeID::CPUHalf;
}

size_t CPUHalfType::elementSizeInBytes() const {
  return sizeof(Half);
}

/* example
Tensor * CPUHalfType::add(Tensor & a, Tensor & b) {
  std::cout << "add CPUHalfTensor\n";
  return &a;
}
*/

Tensor & CPUHalfType::_th_set_(Tensor & self, Storage source) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Half);
    auto source_ = checked_storage(source,"source",2, DeviceType::CPU, at::scalarTypeToDataType(ScalarType::Half));
    THHalfTensor_setStorage(self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
    self_->maybe_zero_dim(false);
    return self;
}
Tensor & CPUHalfType::_th_set_(Tensor & self, Storage source, int64_t storage_offset, IntArrayRef size, IntArrayRef stride) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Half);
    auto source_ = checked_storage(source,"source",2, DeviceType::CPU, at::scalarTypeToDataType(ScalarType::Half));
    THHalfTensor_setStorage(self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
    self_->maybe_zero_dim(size.size() == 0);
    return self;
}
Tensor & CPUHalfType::_th_set_(Tensor & self, const Tensor & source) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Half);
    auto source_ = checked_tensor_unwrap(source,"source",2, false, Backend::CPU, ScalarType::Half);
    THHalfTensor_set(self_, source_);
    self_->maybe_zero_dim(source_->dim() == 0);
    return self;
}
Tensor & CPUHalfType::_th_set_(Tensor & self) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Half);
    THHalfTensor_setStorage(self_, NULL, 0, {0}, {});
    self_->maybe_zero_dim(false);
    return self;
}
Tensor & CPUHalfType::_th_fill_(Tensor & self, Scalar value) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Half);
    auto value_ = value.toHalf();
    THHalfTensor_fill(self_, value_);
    return self;
}
Tensor & CPUHalfType::_th_fill_(Tensor & self, const Tensor & value) const {
    // DeviceGuard omitted
    if (value.dim() == 0) {
        return static_cast<const TypeExtendedInterface*>(this)->_th_fill_(self, value.item());
    }
    AT_ERROR("_th_fill_ only supports a 0-dimensional value tensor, but got tensor "
        "with ", value.dim(), " dimension(s).");
}
bool CPUHalfType::_th_is_set_to(const Tensor & self, const Tensor & tensor) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Half);
    auto tensor_ = checked_tensor_unwrap(tensor,"tensor",2, false, Backend::CPU, ScalarType::Half);
    return THHalfTensor_isSetTo(self_, tensor_);
}
Tensor CPUHalfType::_th_clone(const Tensor & self) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Half);
    return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THHalfTensor_newClone(self_))->maybe_zero_dim(self_->dim() == 0)));
}
Tensor & CPUHalfType::_th_unfold_out(Tensor & result, const Tensor & self, int64_t dimension, int64_t size, int64_t step) const {
    // DeviceGuard omitted
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Half);
    dimension = maybe_wrap_dim(dimension, self_);
    THHalfTensor_unfold(result_, self_, dimension, size, step);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CPUHalfType::_th_unfold(const Tensor & self, int64_t dimension, int64_t size, int64_t step) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Half);
    dimension = maybe_wrap_dim(dimension, self_);
    THHalfTensor_unfold(result_, self_, dimension, size, step);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CPUHalfType::_th_zero_(Tensor & self) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Half);
    THHalfTensor_zero(self_);
    return self;
}
Tensor CPUHalfType::_th_alias(const Tensor & self) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Half);
    return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THHalfTensor_newWithTensor(self_))->maybe_zero_dim(self_->dim() == 0)));
}
Tensor & CPUHalfType::_th_cat_out(Tensor & self, TensorList tensors, int64_t dim) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",0, false, Backend::CPU, ScalarType::Half);
    auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Half);
    THHalfTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
    return self;
}
Tensor CPUHalfType::_th_cat(TensorList tensors, int64_t dim) const {
    // DeviceGuard omitted
    auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CPUTensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
    auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Half);
    THHalfTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
    return self;
}
Tensor & CPUHalfType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_s_copy__cpu(/* actuals */ self, src, non_blocking);
}
Tensor CPUHalfType::_s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const {
    AT_ERROR("_s_copy_from not supported on CPUHalfType");
}
void CPUHalfType::_copy_same_type_(Tensor & self, const Tensor & src) const {
    const OptionalDeviceGuard device_guard(device_of(self));
 at::native::_copy_same_type__cpu(/* actuals */ self, src);
}
Tensor CPUHalfType::empty(IntArrayRef size, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::empty_cpu(/* actuals */ size, options);
}
Tensor & CPUHalfType::resize_(Tensor & self, IntArrayRef size) const {
    // DeviceGuard omitted
    return at::native::resize_cpu_(/* actuals */ self, size);
}
Scalar CPUHalfType::_local_scalar_dense(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_local_scalar_dense_cpu(/* actuals */ self);
}

}
